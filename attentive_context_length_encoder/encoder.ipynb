{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "\n",
    "import torchtext\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import math\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import random\n",
    "import functools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('seaborn')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To ensure that the code is reproducible, set random seeds\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pwngc_path = \"../data/test_transformer/pwngc4torchtext.csv\"\n",
    "pwngc_df = pd.read_csv(pwngc_path, delimiter=\"\\t\", header=None)\n",
    "pwngc_df.columns = [\"token\", \"stemm\", \"pos\", \"annotation\", \"synset\", \"tag\" ]\n",
    "pwngc_df.head(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "len(np.where(pwngc_df[\"tag\"] != 'O')[0])\n",
    "# 532.821 annotated tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import ast\n",
    "tag2 = pwngc_df['tag'][2]\n",
    "print(tag2)\n",
    "print(type(tag2))\n",
    "def removeBra(string_list):\n",
    "    if string_list[0] == \"[\" and string_list[-1] == \"]\":\n",
    "        return string_list[1:-1]\n",
    "    else:\n",
    "        return string_list\n",
    "\n",
    "tag2list = torch.tensor(list(map(float, removeBra(tag2).split(' '))), dtype=torch.float32)\n",
    "# tag2list = ast.literal_eval(tag2)\n",
    "print(tag2list)\n",
    "print(type(tag2list))\n",
    "# type(eval(\"tensor({}, device='{}')\".format(tag2, \"cpu\")))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# split the dataset into training, validation and testing\n",
    "train_path = \"train.csv\"\n",
    "validate_path = \"validate.csv\"\n",
    "test_path = \"test.csv\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#   train_examples = read_data(\"../data/test_transformer/train.csv\", self.fields) #'data/eng.train.iob', self.fields)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TEXT = data.Field(use_vocab=True,\n",
    "                  lower=True)\n",
    "\n",
    "LABEL = data.Field(is_target=True,\n",
    "                   use_vocab=False,\n",
    "                   unk_token=None,\n",
    "                   preprocessing=data.Pipeline(lambda x: torch.tensor(list(map(float, removeBra(x).split(' '))), dtype=torch.double)),\n",
    "                   dtype=data.Pipeline(lambda x: torch.tensor(x, dtype=torch.double)))\n",
    "\n",
    "train, valid, test = datasets.SequenceTaggingDataset.splits(path='../data/test_transformer/',\n",
    "                                   train = train_path,\n",
    "                                   validation = validate_path,\n",
    "                                   test = test_path,\n",
    "                                   fields=[(\"text\",TEXT),(\"lemmatized_text\",TEXT), (None, None), (None,None), (None, None), (\"label\",LABEL)]) #,\n",
    "\n",
    "type(train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for t, lt, l in zip(train.text, train.lemmatized_text, train.label):\n",
    "    print(type(l), type(lt), type(l))\n",
    "    print(t, lt, l)\n",
    "    print(len(t))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for ex in train:\n",
    "    print(len(ex.text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the pre-trained embeddings that come with the torchtext library.\n",
    "use_pretrained = True\n",
    "if use_pretrained:\n",
    "    print('We are using pre-trained word embeddings.')\n",
    "    TEXT.build_vocab(train, vectors=\"glove.840B.300d\")\n",
    "else:\n",
    "    print('We are training word embeddings from scratch.')\n",
    "    TEXT.build_vocab(train, max_size=5000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for key, val in zip(TEXT.vocab.stoi, TEXT.vocab.vectors):\n",
    "    # print(key, val)\n",
    "\n",
    "print(list(train))\n",
    "# print(TEXT.vocab.stoi)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# b= data.example.Example.fromlist([train.text, train.label], fields=[(\"text\",TEXT),(\"lemmatized_text\",TEXT), (None, None), (None,None), (None, None), (\"label\",LABEL)])\n",
    "# TEXT.build_vocab(b, vectors=\"glove.840B.300d\")\n",
    "# print(b.vocab)\n",
    "# for tt in b:\n",
    "#     print(tt.vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    # sort=False, # to skip sorting validation and testing data\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    repeat=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(train_iterator.data())\n",
    "\n",
    "train_batch = train_iterator.data()\n",
    "print(TEXT.vocab.freqs.most_common(10))\n",
    "print(\"-\"*40)\n",
    "print(TEXT.vocab.itos[:10])\n",
    "print(TEXT.vocab.stoi)\n",
    "print(TEXT.vocab.vectors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for bat in train_iterator:\n",
    "    print(bat)\n",
    "# for batch in train_iterator:\n",
    "#     print(type(batch))#, type(batch.text), type(batch.label))\n",
    "    # print(batch.text[0], type(batch.text[0]))\n",
    "    # print(batch.text, batch.lemmatized_text, batch.label)\n",
    "\n",
    "\n",
    "\n",
    "# for batch in train_iterator.data():\n",
    "#     print(type(batch), type(batch.text), type(batch.label))\n",
    "#     print(batch.text[0], type(batch.text[0]))\n",
    "#     print(batch.text, batch.lemmatized_text, batch.label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list(train_iterator.data())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_batches = list(train_iterator.data())\n",
    "valid_batches = list(valid_iterator.data())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TransformerEncoderModel(nn.Module):\n",
    "\n",
    "    def __init__(self, text_field, label_field, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        # Multi-head attention mechanism is included in TransformerEncoderLayer\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        # self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "        # -------------------------------------\n",
    "\n",
    "        voc_size = len(text_field.vocab)\n",
    "        print(\"voc_size: \", voc_size )\n",
    "\n",
    "        # Embedding layer. If we're using pre-trained embeddings, copy them\n",
    "        # into our embedding module.\n",
    "        self.embedding = nn.Embedding(voc_size, 300)\n",
    "        print(\"Embedding\", self.embedding)\n",
    "        if text_field.vocab.vectors is not None:\n",
    "            self.embedding.weight = torch.nn.Parameter(TEXT.vocab.vectors)\n",
    "\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # self.decoder.bias.data.zero_()\n",
    "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = torch.tensor(src, device=\"cpu\")\n",
    "        src = self.embedding(src)\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src) #, src_mask)\n",
    "        # output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "# def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "#     \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "#     return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_batches = train_iterator.data()\n",
    "# for batch in train_batches:\n",
    "#     print(batch)\n",
    "#     # batch.text is of shape: (input_data_length, batch_size)\n",
    "#     print(batch.text)\n",
    "#     # print(batch.text.vocab)\n",
    "#     # print(batch.text.vocab.vectors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Tagger:\n",
    "\n",
    "    def __init__(self, lower):\n",
    "        self.TEXT = data.Field(use_vocab=True,\n",
    "                  lower=True)\n",
    "\n",
    "        self.LABEL = data.Field(is_target=True,\n",
    "                           use_vocab=False,\n",
    "                           unk_token=None,\n",
    "                           preprocessing=data.Pipeline(\n",
    "                               lambda x: torch.tensor(list(map(float, removeBra(x).split(' '))),\n",
    "                                                      dtype=torch.double)),\n",
    "                           dtype=torch.DoubleTensor)\n",
    "                                #data.Pipeline(lambda x: torch.tensor(x, dtype=torch.double)))\n",
    "\n",
    "        self.fields = [(\"text\",self.TEXT),(\"lemmatized_text\",self.TEXT), (None, None), (None,None), (None, None), (\"label\",LABEL)]\n",
    "\n",
    "\n",
    "        # self.TEXT = data.Field(init_token='<bos>', eos_token='<eos>', sequential=True, lower=lower)\n",
    "        # I changed sequential = True to false, because my data is not sequential\n",
    "        # self.LABEL = data.Field(init_token='<bos>', eos_token='<eos>', sequential=True, unk_token=None)\n",
    "        # self.LABEL = data.Field(is_target=True, sequential=False, unk_token=None, dtype=list)\n",
    "        # data.Field(init_token='<bos>', eos_token='<eos>',\n",
    "        #     sequential=False, use_vocab=False)\n",
    "        # self.fields = [('text', self.TEXT), ('label', self.LABEL)]\n",
    "        # self.device = 'cuda'\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "    def tag(self, sentences):\n",
    "        # This method applies the trained model to a list of sentences.\n",
    "\n",
    "        # First, create a torchtext Dataset containing the sentences to tag.\n",
    "        examples = []\n",
    "        for sen in sentences:\n",
    "            labels = ['?']*len(sen) # placeholder\n",
    "            examples.append(data.Example.fromlist([sen, labels], self.fields))\n",
    "        dataset = data.Dataset(examples, self.fields)\n",
    "\n",
    "        iterator = data.Iterator(\n",
    "            dataset,\n",
    "            device=self.device,\n",
    "            batch_size= 5, #64,\n",
    "            repeat=False,\n",
    "            train=False,\n",
    "            sort=False)\n",
    "\n",
    "        # Apply the trained model to all batches.\n",
    "        out = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in iterator:\n",
    "                # Call the model's predict method. This returns a list of NumPy matrix\n",
    "                # containing the integer-encoded tags for each sentence.\n",
    "                predicted = self.model.predict(batch.text)\n",
    "\n",
    "                # # Convert the integer-encoded tags to tag strings.\n",
    "                # for tokens, pred_sen in zip(sentences, predicted):\n",
    "                #     out.append([self.LABEL.vocab.itos[pred_id] for _, pred_id in zip(tokens, pred_sen[1:])])\n",
    "        return predicted #out\n",
    "\n",
    "    def train(self):\n",
    "        # Read training and validation data according to the predefined split.\n",
    "        # train_examples = read_data(\"../data/test_transformer/train.csv\", self.fields) #'data/eng.train.iob', self.fields)\n",
    "        # valid_examples = read_data(\"../data/test_transformer/validate.csv\", self.fields) #'data/eng.valid.iob', self.fields)\n",
    "\n",
    "        train_examples, valid_examples, test_examples = datasets.SequenceTaggingDataset.splits(path='../data/test_transformer/',\n",
    "                                           train = train_path,\n",
    "                                           validation = validate_path,\n",
    "                                           test = test_path,\n",
    "                                           fields=[(\"text\",TEXT),(\"lemmatized_text\",TEXT), (None, None), (None,None), (None, None), (\"label\",LABEL)]) #,\n",
    "\n",
    "\n",
    "        # Count the number of words and sentences.\n",
    "        n_tokens_train = 0\n",
    "        n_sentences_train = 0\n",
    "        for ex in train_examples:\n",
    "            n_tokens_train += len(ex.text) #+ 2\n",
    "            n_sentences_train += 1\n",
    "        n_tokens_valid = 0\n",
    "        for ex in valid_examples:\n",
    "            n_tokens_valid += len(ex.text)\n",
    "\n",
    "        # Load the pre-trained embeddings that come with the torchtext library.\n",
    "        use_pretrained = True\n",
    "        if use_pretrained:\n",
    "            print('We are using pre-trained word embeddings.')\n",
    "            self.TEXT.build_vocab(train_examples, vectors=\"glove.840B.300d\")\n",
    "        else:\n",
    "            print('We are training word embeddings from scratch.')\n",
    "            self.TEXT.build_vocab(train_examples, max_size=5000)\n",
    "        # self.LABEL.build_vocab(train_examples)\n",
    "\n",
    "        # Create one of the models defined above.\n",
    "        # self.model = RNNTagger(self.TEXT, self.LABEL, emb_dim=300, rnn_size=128, update_pretrained=False)\n",
    "        # self.model = RNNCRFTagger(self.TEXT, self.LABEL, emb_dim=300, rnn_size=128, update_pretrained=False)\n",
    "\n",
    "        self.model = TransformerEncoderModel(text_field=self.TEXT,\n",
    "                                             label_field=self.LABEL,\n",
    "                                             ntoken=300,\n",
    "                                             d_model=300,\n",
    "                                             d_hid=200,\n",
    "                                             nlayers=2,\n",
    "                                             nhead=2,\n",
    "                                             dropout=0.2)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        #                       BucketIterator\n",
    "        # -----------------------------------------------------------\n",
    "\n",
    "        batch_size = 5 #1024\n",
    "        n_batches = np.ceil(n_sentences_train / batch_size)\n",
    "\n",
    "        mean_n_tokens = n_tokens_train / n_batches\n",
    "\n",
    "        train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "                                                        (train_examples, valid_examples, test_examples),\n",
    "                                                        device=self.device,\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        sort_key=lambda x: len(x.text),\n",
    "                                                        repeat=False,\n",
    "                                                        sort=True)\n",
    "\n",
    "        # train_iterator = data.BucketIterator(\n",
    "        #     train_examples,\n",
    "        #     device=self.device,\n",
    "        #     batch_size=batch_size,\n",
    "        #     sort_key=lambda x: len(x.text),\n",
    "        #     repeat=False,\n",
    "        #     train=True,\n",
    "        #     sort=True)\n",
    "        #\n",
    "        # valid_iterator = data.BucketIterator(\n",
    "        #     valid_examples,\n",
    "        #     device=self.device,\n",
    "        #     batch_size= 2, #64,\n",
    "        #     sort_key=lambda x: len(x.text),\n",
    "        #     repeat=False,\n",
    "        #     train=False,\n",
    "        #     sort=True)\n",
    "\n",
    "        train_batches = train_iterator\n",
    "        valid_batches = valid_iterator\n",
    "        test_batches = valid_iterator\n",
    "\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        #                       Optimizer\n",
    "        # ---------------------------------------------------------------------\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        lr = 5.0  # learning rate\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "        # -------\n",
    "\n",
    "\n",
    "        # optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "\n",
    "        # n_labels = len(self.LABEL.vocab)\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        #                       Epoch Training\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        history = defaultdict(list)\n",
    "\n",
    "        n_epochs = 3 #25\n",
    "\n",
    "        # For each epoch\n",
    "        for i in range(1, n_epochs + 1):\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            loss_sum = 0\n",
    "\n",
    "            self.model.train()\n",
    "\n",
    "            # for transformer\n",
    "            scheduler.step()\n",
    "\n",
    "            # For each batch\n",
    "            for batch in train_batches:\n",
    "\n",
    "                # Compute the output and loss.\n",
    "                # loss = self.model(batch.text, batch.label) / mean_n_tokens\n",
    "\n",
    "                out = self.model(batch.text)\n",
    "                ntokens = 300\n",
    "                loss = criterion(out.view(-1, ntokens), batch.label)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # I added this\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "                # ---\n",
    "                optimizer.step()\n",
    "                loss_sum += loss.item()\n",
    "\n",
    "\n",
    "            train_loss = loss_sum / n_batches\n",
    "            history['train_loss'].append(train_loss)\n",
    "\n",
    "            # Evaluate on the validation set.\n",
    "            if i % 1 == 0:\n",
    "\n",
    "                stats = defaultdict(Counter)\n",
    "\n",
    "                # from transformers\n",
    "                # lr = scheduler.get_last_lr()[0]\n",
    "                # ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "                # cur_loss = total_loss / log_interval\n",
    "                # ppl = math.exp(cur_loss)\n",
    "                # print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                #       f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                #       f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "                # total_loss = 0\n",
    "\n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for batch in valid_batches:\n",
    "                        # Predict the model's output on a batch.\n",
    "                        predicted = self.model.predict(batch.text)\n",
    "                        print(\"predicted := \", predicted)\n",
    "                        # Update the evaluation statistics.\n",
    "                        # evaluate_iob(predicted, batch.label, self.LABEL, stats)\n",
    "\n",
    "                # # Compute the overall F-score for the validation set.\n",
    "                # _, _, val_f1 = prf(stats['total'])\n",
    "                #\n",
    "                # history['val_f1'].append(val_f1)\n",
    "                #\n",
    "                # t1 = time.time()\n",
    "                # print(f'Epoch {i}: train loss = {train_loss:.4f}, val f1: {val_f1:.4f}, time = {t1-t0:.4f}')\n",
    "\n",
    "        # # After the final evaluation, we print more detailed evaluation statistics, including\n",
    "        # # precision, recall, and F-scores for the different types of named entities.\n",
    "        # print()\n",
    "        # print('Final evaluation on the validation set:')\n",
    "        # p, r, f1 = prf(stats['total'])\n",
    "        # print(f'Overall: P = {p:.4f}, R = {r:.4f}, F1 = {f1:.4f}')\n",
    "        # for label in stats:\n",
    "        #     if label != 'total':\n",
    "        #         p, r, f1 = prf(stats[label])\n",
    "        #         print(f'{label:4s}: P = {p:.4f}, R = {r:.4f}, F1 = {f1:.4f}')\n",
    "        #\n",
    "        # plt.plot(history['train_loss'])\n",
    "        # plt.plot(history['val_f1'])\n",
    "        # plt.legend(['training loss', 'validation F-score'])\n",
    "\n",
    "tagger = Tagger(lower=False)\n",
    "tagger.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def decode_params(spatial_params):\n",
    "    l0 = spatial_params[0]\n",
    "    alpha = spatial_params[1]\n",
    "    alpha_rad = alpha * np.pi / 180\n",
    "    l_i = spatial_params[2]\n",
    "    beta_i = spatial_params[3]\n",
    "    beta_i_rad = beta_i * np.pi / 180\n",
    "    r = spatial_params[4]\n",
    "    return l0, alpha, alpha_rad, l_i, beta_i, beta_i_rad, r\n",
    "\n",
    "\n",
    "def point_in_space(spatial_params):\n",
    "    l0, alpha, alpha_rad, l_i, beta_i, beta_i_rad, r = decode_params(spatial_params)\n",
    "    # np.cos() and np.sin() take angles in radian as params\n",
    "    center_pt = np.array([l0*np.cos(alpha_rad), l0 * np.sin(alpha_rad)])\n",
    "    sense_pt = center_pt + np.array([l_i * np.cos(alpha_rad + beta_i_rad),\n",
    "                                     l_i * np.sin(alpha_rad + beta_i_rad)])\n",
    "    return sense_pt, center_pt\n",
    "\n",
    "\n",
    "def inside_sphere(point, sphere_coo):\n",
    "\n",
    "    pt = point_in_space(point)\n",
    "    sphere_sense, sphere_center = point_in_space(sphere_coo)\n",
    "\n",
    "    sphere_rad = sphere_coo[-1] # in angles\n",
    "\n",
    "    contained = (pt[0] - sphere_sense[0])**2 + (pt[1] - sphere_sense[1])**2 <= sphere_rad**2\n",
    "\n",
    "    if contained:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def distance(pred_pt, original_pt):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two sense points.\n",
    "    :param pred_pt:\n",
    "    :param original_pt:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pred_sense, pred_center = point_in_space(pred_pt)\n",
    "    orig_sense, orig_center = point_in_space(original_pt)\n",
    "\n",
    "    return np.linalg.norm(pred_sense - orig_sense)\n",
    "\n",
    "\n",
    "\n",
    "def sphere_dist(pred_pt, original_pt):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two 2D spheres.\n",
    "    :param pred_pt:\n",
    "    :param original_pt:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pred_sense, pred_center = point_in_space(pred_pt)\n",
    "    pred_radius = pred_pt[-1]\n",
    "    orig_sense, orig_center = point_in_space(original_pt)\n",
    "    orig_radius = original_pt[-1]\n",
    "\n",
    "    return (pred_radius + orig_radius -\n",
    "            np.linalg.norm(pred_sense - orig_sense))\n",
    "\n",
    "def decode_prediction(spatial_params, df=\"SPATIAL_WORDNET.pickle\") -> [str]:\n",
    "    \"\"\"\n",
    "    Projects the predicted spatial parameters into the embedding space.\n",
    "    Returns the synsets in the vacinity of the projected point.\n",
    "    :param spatial_params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    synsets = [] # sort from specific to most general\n",
    "\n",
    "    sense_pt, center_pt = point_in_space(spatial_params)\n",
    "\n",
    "    spatial_df = pd.read_pickle(df)\n",
    "    # get the spheres, where the point/point+radius is contained/overlaping/near\n",
    "\n",
    "    # 1. check if the predicted point is contained in some sense\n",
    "    spatial_df[\"contained\"] = spatial_df.apply(lambda row:\n",
    "                                               inside_sphere(spatial_params,\n",
    "                                                             row[['l0', 'alpha', 'l_i', 'beta_i', 'radius']]))\n",
    "\n",
    "    # 2. For those synsets, which is the nearest synset point\n",
    "    #use distance() to calculate distance between centers\n",
    "\n",
    "    # 3. If None of the synsets apply to that word sense\n",
    "    # use sphere_dist to find the nearest sphere (most general synset), and assign it to that synset\n",
    "    # (this maybe good for rare senses)\n",
    "\n",
    "\n",
    "    return synsets\n",
    "\n",
    "def train_loss(tmp_pred, synset_params):\n",
    "    # Loss is the distance between the two spheres/containment of the word within that sphere\n",
    "    # radius acts as tolerance!\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TEXT = data.Field(use_vocab=True,\n",
    "                  lower=True)\n",
    "\n",
    "LABEL = data.Field(is_target=True,\n",
    "                   use_vocab=False,\n",
    "                   unk_token=None,\n",
    "                   sequential=False,\n",
    "                   postprocessing=data.Pipeline(\n",
    "                       lambda x: torch.tensor(list(map(float, removeBra(x).split(' '))),\n",
    "                                              dtype=torch.double)),\n",
    "                   dtype=torch.DoubleTensor)\n",
    "                        #data.Pipeline(lambda x: torch.tensor(x, dtype=torch.double)))\n",
    "\n",
    "# LABEL.numericalize()\n",
    "fields = [(\"text\",TEXT),(\"lemmatized_text\",TEXT), (None, None), (None,None), (None, None), (\"label\",LABEL)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train, valid, test = datasets.SequenceTaggingDataset.splits(path='../data/test_transformer/',\n",
    "                                   train = train_path,\n",
    "                                   validation = validate_path,\n",
    "                                   test = test_path,\n",
    "                                   fields=[(\"text\",TEXT),(\"lemmatized_text\",TEXT), (None, None), (None,None), (None, None), (\"label\",LABEL)]) #,"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "\n",
    "use_pretrained = True\n",
    "if use_pretrained:\n",
    "    print('We are using pre-trained word embeddings.')\n",
    "    TEXT.build_vocab(train, vectors=\"glove.840B.300d\")\n",
    "else:\n",
    "    print('We are training word embeddings from scratch.')\n",
    "    TEXT.build_vocab(train, max_size=5000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LABEL.build_vocab(train.label)\n",
    "LABEL.numericalize(train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "                                                        (train, valid),\n",
    "                                                        device=DEVICE,\n",
    "                                                        batch_size=BATCH_SIZE,\n",
    "                                                        sort_key=lambda x: len(x.text),\n",
    "                                                        repeat=False,\n",
    "                                                        sort=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch = next(iter(train_iterator))\n",
    "print(\"Numericalize premises:\\n\", batch.text)\n",
    "print(\"Entailment labels:\\n\", batch.label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}