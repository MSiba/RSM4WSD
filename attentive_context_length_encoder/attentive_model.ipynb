{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "\n",
    "import torchtext\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import math\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import random\n",
    "import functools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina' \n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# To ensure that the code is reproducible, set random seeds\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pwngc_path = \"../data/test_transformer/pwngc4torchtext.csv\"\n",
    "pwngc_df = pd.read_csv(pwngc_path, delimiter=\"\\t\", header=None)\n",
    "pwngc_df.columns = [\"token\", \"stemm\", \"pos\", \"annotation\", \"synset\", \"tag\" ]\n",
    "pwngc_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "len(np.where(pwngc_df[\"tag\"] != 'O')[0])\n",
    "# 532.821 annotated tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training, validation and testing\n",
    "train = \"../data/test_transformer/train.csv\"\n",
    "validate = \"../../data/test_transformer/validate.csv\"\n",
    "test = \"../../data/test_transformer/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Even though the following code is created initially to deal with Universal Dependencies English Treebank (UDPD),\n",
    "# # I will try to run it on my sequence tagging task.\n",
    "# # Goal: create training/validation/testing data with torchtext\n",
    "# # source: https://github.com/arthurdjn/udpos\n",
    "# class PWNGC(datasets.sequence_tagging.SequenceTaggingDataset):\n",
    "#     # urls = ['https://github.com/arthurdjn/udpos/raw/master/data/fr-gsd-ud-15032020.zip'] # change to the dataset of your choice\n",
    "#     dirname = 'pwngc-wsd'  # don't forget to change me too !\n",
    "#     name = 'udpos'         # not obligatory to change here\n",
    "#\n",
    "#     @classmethod\n",
    "#     def splits(cls, fields, root=\".data\",\n",
    "#                train=\"pwngc-wsd-dev.csv\",\n",
    "#                validation=\"pwngc-wsd-dev.csv\",\n",
    "#                test=\"pwngc-wsd-dev.csv\", **kwargs):\n",
    "#         \"\"\"Downloads and loads the Universal Dependencies Version 2 POS Tagged\n",
    "#         data.\n",
    "#         \"\"\"\n",
    "#\n",
    "#         return super(PWNGC, cls).splits(\n",
    "#             fields=fields, root=root, train=train, validation=validation,\n",
    "#             test=test, **kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext import data\n",
    "# import PWNGC\n",
    "#\n",
    "# TEXT = data.Field(lower = True)\n",
    "# LEMMATIZED = data.Field(unk_token = None)\n",
    "# UD_TAGS = data.Field(unk_token = None)\n",
    "# fields = ((\"text\", TEXT), (\"lemmatized\", LEMMATIZED), (\"udtags\", UD_TAGS))\n",
    "#\n",
    "# # Load the UD french dataset\n",
    "# train_data, eval_data, test_data = PWNGC.splits(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_data(corpus_file, datafields):\n",
    "    \"\"\"\n",
    "    reads the stem word and the spatial tag of each token in the .csv file\n",
    "    :param corpus_file:\n",
    "    :param datafields:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        examples = []\n",
    "        words = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                examples.append(data.Example.fromlist([words, labels], datafields))\n",
    "                words = []\n",
    "                labels = []\n",
    "            else:\n",
    "                columns = line.split()\n",
    "                words.append(columns[1])\n",
    "                labels.append(columns[-1])\n",
    "        return data.Dataset(examples, datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Compares two lists of training parameters and records the results for future aggregation.\n",
    "# def compare(gold, pred, stats):\n",
    "#     for start, (lbl, end) in gold.items():\n",
    "#         stats['total']['gold'] += 1\n",
    "#         stats[lbl]['gold'] += 1\n",
    "#     for start, (lbl, end) in pred.items():\n",
    "#         stats['total']['pred'] += 1\n",
    "#         stats[lbl]['pred'] += 1\n",
    "#     for start, (glbl, gend) in gold.items():\n",
    "#         if start in pred:\n",
    "#             plbl, pend = pred[start]\n",
    "#             if glbl == plbl and gend == pend:\n",
    "#                 stats['total']['corr'] += 1\n",
    "#                 stats[glbl]['corr'] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        # Multi-head attention mechanism included already in TransformerEncoderLayer\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RNNTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, text_field, label_field, emb_dim, rnn_size, update_pretrained=False):\n",
    "        super().__init__()\n",
    "\n",
    "        voc_size = len(text_field.vocab)\n",
    "        self.n_labels = label_field#len(label_field.vocab)\n",
    "        #\n",
    "        # Embedding layer. If we're using pre-trained embeddings, copy them\n",
    "        # into our embedding module.\n",
    "        self.embedding = nn.Embedding(voc_size, emb_dim)\n",
    "        if text_field.vocab.vectors is not None:\n",
    "            self.embedding.weight = torch.nn.Parameter(text_field.vocab.vectors,\n",
    "                                                       requires_grad=update_pretrained)\n",
    "\n",
    "        # RNN layer. We're using a bidirectional GRU with one layer.\n",
    "        self.rnn = nn.GRU(input_size=emb_dim, hidden_size=rnn_size,\n",
    "                          bidirectional=True, num_layers=1)\n",
    "\n",
    "        # Output layer. As in the example last week, the input will be two times\n",
    "        # the RNN size since we are using a bidirectional RNN.\n",
    "        self.top_layer = nn.Linear(2*rnn_size, self.n_labels)\n",
    "\n",
    "        # To deal with the padding positions later, we need to know the\n",
    "        # encoding of the padding dummy word and the corresponding dummy output tag.\n",
    "        self.pad_word_id = text_field.vocab.stoi[text_field.pad_token]\n",
    "        # self.pad_label_id = label_field.vocab.stoi[label_field.pad_token]\n",
    "\n",
    "        # Loss function that we will use during training.\n",
    "        self.loss = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    def compute_outputs(self, sentences):\n",
    "        # The words in the documents are encoded as integers. The shape of the documents\n",
    "        # tensor is (max_len, n_docs), where n_docs is the number of documents in this batch,\n",
    "        # and max_len is the maximal length of a document in the batch.\n",
    "\n",
    "        # First look up the embeddings for all the words in the documents.\n",
    "        # The shape is now (max_len, n_sentences, emb_dim).\n",
    "        embedded = self.embedding(sentences)\n",
    "\n",
    "        # Apply the RNN.\n",
    "        # The shape of the RNN output tensor is (max_len, n_sentences, 2*rnn_size).\n",
    "        rnn_out, _ = self.rnn(embedded)\n",
    "\n",
    "        # Apply the linear output layer.\n",
    "        # The shape of the output tensor is (max_len, n_sentences, n_labels).\n",
    "        out = self.top_layer(rnn_out)\n",
    "\n",
    "        # Find the positions where the token is a dummy padding token.\n",
    "        pad_mask = (sentences == self.pad_word_id).float()\n",
    "\n",
    "        # For these positions, we add some large number in the column corresponding\n",
    "        # to the dummy padding label.\n",
    "        # out[:, :, self.pad_label_id] += pad_mask*10000\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(self, sentences, labels):\n",
    "        # As discussed above, this method first computes the predictions, and then\n",
    "        # the loss function.\n",
    "\n",
    "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
    "        scores = self.compute_outputs(sentences)\n",
    "\n",
    "        # Flatten the outputs and the gold-standard labels, to compute the loss.\n",
    "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
    "        scores = scores.view(-1, self.n_labels)\n",
    "        labels = labels.view(-1)\n",
    "        return self.loss(scores, labels)\n",
    "\n",
    "    def predict(self, sentences):\n",
    "        # Compute the outputs from the linear units.\n",
    "        scores = self.compute_outputs(sentences)\n",
    "\n",
    "        # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
    "        predicted = scores.argmax(dim=2)\n",
    "\n",
    "        # We transpose the prediction to (n_sentences, max_len), and convert it\n",
    "        # to a NumPy matrix.\n",
    "        return predicted.t().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "class Tagger:\n",
    "\n",
    "    def __init__(self, lower):\n",
    "        self.TEXT = data.Field(init_token='<bos>', eos_token='<eos>', sequential=True, lower=lower)\n",
    "        # I changed sequential = True to false, because my data is not sequential\n",
    "        # self.LABEL = data.Field(init_token='<bos>', eos_token='<eos>', sequential=True, unk_token=None)\n",
    "        self.LABEL = data.Field(is_target=True, sequential=False, unk_token=None, dtype=list)\n",
    "        # data.Field(init_token='<bos>', eos_token='<eos>',\n",
    "        #     sequential=False, use_vocab=False)\n",
    "        self.fields = [('text', self.TEXT), ('label', self.LABEL)]\n",
    "        # self.device = 'cuda'\n",
    "        self.device = 'cpu'\n",
    "\n",
    "    def tag(self, sentences):\n",
    "        # This method applies the trained model to a list of sentences.\n",
    "\n",
    "        # First, create a torchtext Dataset containing the sentences to tag.\n",
    "        examples = []\n",
    "        for sen in sentences:\n",
    "            labels = ['?']*len(sen) # placeholder\n",
    "            examples.append(data.Example.fromlist([sen, labels], self.fields))\n",
    "        dataset = data.Dataset(examples, self.fields)\n",
    "\n",
    "        iterator = data.Iterator(\n",
    "            dataset,\n",
    "            device=self.device,\n",
    "            batch_size=64,\n",
    "            repeat=False,\n",
    "            train=False,\n",
    "            sort=False)\n",
    "\n",
    "        # Apply the trained model to all batches.\n",
    "        out = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in iterator:\n",
    "                # Call the model's predict method. This returns a list of NumPy matrix\n",
    "                # containing the integer-encoded tags for each sentence.\n",
    "                predicted = self.model.predict(batch.text)\n",
    "\n",
    "                # # Convert the integer-encoded tags to tag strings.\n",
    "                # for tokens, pred_sen in zip(sentences, predicted):\n",
    "                #     out.append([self.LABEL.vocab.itos[pred_id] for _, pred_id in zip(tokens, pred_sen[1:])])\n",
    "        return out\n",
    "\n",
    "    def train(self):\n",
    "        # Read training and validation data according to the predefined split.\n",
    "        train_examples = read_data(\"../data/test_transformer/train.csv\", self.fields) #'data/eng.train.iob', self.fields)\n",
    "        valid_examples = read_data(\"../data/test_transformer/validate.csv\", self.fields) #'data/eng.valid.iob', self.fields)\n",
    "\n",
    "        # Count the number of words and sentences.\n",
    "        n_tokens_train = 0\n",
    "        n_sentences_train = 0\n",
    "        for ex in train_examples:\n",
    "            n_tokens_train += len(ex.text) + 2\n",
    "            n_sentences_train += 1\n",
    "        n_tokens_valid = 0\n",
    "        for ex in valid_examples:\n",
    "            n_tokens_valid += len(ex.text)\n",
    "\n",
    "        # Load the pre-trained embeddings that come with the torchtext library.\n",
    "        use_pretrained = True\n",
    "        if use_pretrained:\n",
    "            print('We are using pre-trained word embeddings.')\n",
    "            self.TEXT.build_vocab(train_examples, vectors=\"glove.840B.300d\")\n",
    "        else:\n",
    "            print('We are training word embeddings from scratch.')\n",
    "            self.TEXT.build_vocab(train_examples, max_size=5000)\n",
    "        # self.LABEL.build_vocab(train_examples)\n",
    "\n",
    "        # Create one of the models defined above.\n",
    "        self.model = RNNTagger(self.TEXT, self.LABEL, emb_dim=300, rnn_size=128, update_pretrained=False)\n",
    "        # self.model = RNNCRFTagger(self.TEXT, self.LABEL, emb_dim=300, rnn_size=128, update_pretrained=False)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        batch_size = 5 #1024\n",
    "        n_batches = np.ceil(n_sentences_train / batch_size)\n",
    "\n",
    "        mean_n_tokens = n_tokens_train / n_batches\n",
    "\n",
    "        train_iterator = data.BucketIterator(\n",
    "            train_examples,\n",
    "            device=self.device,\n",
    "            batch_size=batch_size,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            repeat=False,\n",
    "            train=True,\n",
    "            sort=True)\n",
    "\n",
    "        valid_iterator = data.BucketIterator(\n",
    "            valid_examples,\n",
    "            device=self.device,\n",
    "            batch_size= 2, #64,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            repeat=False,\n",
    "            train=False,\n",
    "            sort=True)\n",
    "\n",
    "        train_batches = list(train_iterator)\n",
    "        valid_batches = list(valid_iterator)\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "\n",
    "        # n_labels = len(self.LABEL.vocab)\n",
    "\n",
    "        history = defaultdict(list)\n",
    "\n",
    "        n_epochs = 3 #25\n",
    "\n",
    "        for i in range(1, n_epochs + 1):\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            loss_sum = 0\n",
    "\n",
    "            self.model.train()\n",
    "            for batch in train_batches:\n",
    "\n",
    "                # Compute the output and loss.\n",
    "                loss = self.model(batch.text, batch.label) / mean_n_tokens\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_sum += loss.item()\n",
    "\n",
    "            train_loss = loss_sum / n_batches\n",
    "            history['train_loss'].append(train_loss)\n",
    "\n",
    "            # Evaluate on the validation set.\n",
    "            if i % 1 == 0:\n",
    "                stats = defaultdict(Counter)\n",
    "\n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for batch in valid_batches:\n",
    "                        # Predict the model's output on a batch.\n",
    "                        predicted = self.model.predict(batch.text)\n",
    "                        print(\"predicted := \", predicted)\n",
    "                        # Update the evaluation statistics.\n",
    "                        # evaluate_iob(predicted, batch.label, self.LABEL, stats)\n",
    "\n",
    "                # # Compute the overall F-score for the validation set.\n",
    "                # _, _, val_f1 = prf(stats['total'])\n",
    "                #\n",
    "                # history['val_f1'].append(val_f1)\n",
    "                #\n",
    "                # t1 = time.time()\n",
    "                # print(f'Epoch {i}: train loss = {train_loss:.4f}, val f1: {val_f1:.4f}, time = {t1-t0:.4f}')\n",
    "\n",
    "        # # After the final evaluation, we print more detailed evaluation statistics, including\n",
    "        # # precision, recall, and F-scores for the different types of named entities.\n",
    "        # print()\n",
    "        # print('Final evaluation on the validation set:')\n",
    "        # p, r, f1 = prf(stats['total'])\n",
    "        # print(f'Overall: P = {p:.4f}, R = {r:.4f}, F1 = {f1:.4f}')\n",
    "        # for label in stats:\n",
    "        #     if label != 'total':\n",
    "        #         p, r, f1 = prf(stats[label])\n",
    "        #         print(f'{label:4s}: P = {p:.4f}, R = {r:.4f}, F1 = {f1:.4f}')\n",
    "        #\n",
    "        # plt.plot(history['train_loss'])\n",
    "        # plt.plot(history['val_f1'])\n",
    "        # plt.legend(['training loss', 'validation F-score'])\n",
    "\n",
    "tagger = Tagger(lower=False)\n",
    "tagger.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tagger.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}