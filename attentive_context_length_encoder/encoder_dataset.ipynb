{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available kernels:\n",
      "  python3    C:\\Users\\HP\\Anaconda3\\envs\\Ball4WSD\\share\\jupyter\\kernels\\python3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TerminalIPythonApp] WARNING | Subcommand `ipython kernelspec` is deprecated and will be removed in future versions.\n",
      "[TerminalIPythonApp] WARNING | You likely want to use `jupyter kernelspec` in the future\n",
      "[ListKernelSpecs] WARNING | Config option `kernel_spec_manager_class` not recognized by `ListKernelSpecs`.\n"
     ]
    }
   ],
   "source": [
    "# check \n",
    "!ipython kernelspec list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aiohttp', '3.8.0']\n",
      "['aiosignal', '1.2.0']\n",
      "['antlr4-python3-runtime @ file:///D:/bld/antlr-python-runtime_1602352319407/work']\n",
      "['anyio', '3.3.4']\n",
      "['argcomplete', '1.12.3']\n",
      "['argon2-cffi @ file:///opt/conda/conda-bld/argon2-cffi_1645000214183/work']\n",
      "['argon2-cffi-bindings @ file:///C:/ci/argon2-cffi-bindings_1644569848815/work']\n",
      "['async-timeout', '4.0.0']\n",
      "['asynctest', '0.13.0']\n",
      "['attrs @ file:///opt/conda/conda-bld/attrs_1642510447205/work']\n",
      "['autobahn', '21.3.1']\n",
      "['backcall @ file:///home/ktietz/src/ci/backcall_1611930011877/work']\n",
      "['bcolz', '1.2.1']\n",
      "['bert-embedding', '1.0.1']\n",
      "['bleach @ file:///opt/conda/conda-bld/bleach_1641577558959/work']\n",
      "['bokeh @ file:///C:/ci/bokeh_1638363000110/work']\n",
      "['Bottleneck', '1.3.2']\n",
      "['bpemb', '0.3.3']\n",
      "['cached-property', '1.5.2']\n",
      "['certifi', '2021.10.8']\n",
      "['cffi @ file:///C:/ci/cffi_1613247308275/work']\n",
      "['chardet', '3.0.4']\n",
      "['charset-normalizer', '2.0.7']\n",
      "['click', '8.0.3']\n",
      "['cloudpickle @ file:///tmp/build/80754af9/cloudpickle_1632508026186/work']\n",
      "['colorama', '0.4.4']\n",
      "['cryptography', '35.0.0']\n",
      "['cycler @ file:///tmp/build/80754af9/cycler_1637851556182/work']\n",
      "['Cython', '0.29.14']\n",
      "['cytoolz', '0.11.0']\n",
      "['dask', '2021.10.0']\n",
      "['dataclasses', '0.6']\n",
      "['debugpy @ file:///C:/ci/debugpy_1637091911212/work']\n",
      "['decorator @ file:///opt/conda/conda-bld/decorator_1643638310831/work']\n",
      "['defusedxml @ file:///tmp/build/80754af9/defusedxml_1615228127516/work']\n",
      "['Deprecated', '1.2.12']\n",
      "['distributed @ file:///C:/ci/distributed_1635950227219/work']\n",
      "['editdistance', '0.6.0']\n",
      "['entrypoints', '0.3']\n",
      "['fairseq @ file:///D:/bld/fairseq_1609882179141/work']\n",
      "['filelock', '3.3.1']\n",
      "['flair', '0.8.0.post1']\n",
      "['fonttools', '4.25.0']\n",
      "['frozenlist', '1.2.0']\n",
      "['fsspec @ file:///opt/conda/conda-bld/fsspec_1642510437511/work']\n",
      "['ftfy', '6.0.1']\n",
      "['future', '0.18.2']\n",
      "['gdown', '3.12.2']\n",
      "['gensim', '3.8.3']\n",
      "['gluonnlp', '0.6.0']\n",
      "['graphviz', '0.8.4']\n",
      "['h5py', '3.6.0']\n",
      "['HeapDict @ file:///Users/ktietz/demo/mc3/conda-bld/heapdict_1630598515714/work']\n",
      "['huggingface-hub', '0.0.19']\n",
      "['hydra-core @ file:///home/conda/feedstock_root/build_artifacts/hydra-core_1629433216282/work']\n",
      "['hyperlink', '21.0.0']\n",
      "['hyperopt', '0.2.5']\n",
      "['idna', '3.3']\n",
      "['importlib-metadata', '4.8.1']\n",
      "['importlib-resources', '5.4.0']\n",
      "['ipykernel @ file:///C:/ci/ipykernel_1647000985174/work/dist/ipykernel-6.9.1-py3-none-any.whl']\n",
      "['ipython @ file:///C:/ci/ipython_1643800131373/work']\n",
      "['ipython-genutils @ file:///tmp/build/80754af9/ipython_genutils_1606773439826/work']\n",
      "['ipywidgets', '7.6.5']\n",
      "['Janome', '0.4.1']\n",
      "['jedi @ file:///C:/ci/jedi_1644297241925/work']\n",
      "['Jinja2 @ file:///tmp/build/80754af9/jinja2_1635780242639/work']\n",
      "['joblib', '1.1.0']\n",
      "['jsonschema', '4.2.0']\n",
      "['jupyter', '1.0.0']\n",
      "['jupyter-client @ file:///opt/conda/conda-bld/jupyter_client_1643638337975/work']\n",
      "['jupyter-console', '6.4.0']\n",
      "['jupyter-core @ file:///C:/ci/jupyter_core_1646976467633/work']\n",
      "['jupyter-server', '1.11.2']\n",
      "['jupyter-server-proxy', '3.1.0']\n",
      "['jupyterlab-pygments @ file:///tmp/build/80754af9/jupyterlab_pygments_1601490720602/work']\n",
      "['jupyterlab-widgets', '1.0.2']\n",
      "['kiwisolver @ file:///C:/ci/kiwisolver_1612282618948/work']\n",
      "['konoha', '4.6.4']\n",
      "['langdetect', '1.0.8']\n",
      "['lda-classification', '0.0.29']\n",
      "['locket', '0.2.1']\n",
      "['lxml', '4.6.3']\n",
      "['MarkupSafe @ file:///C:/ci/markupsafe_1621528383308/work']\n",
      "['matplotlib', '3.3.3']\n",
      "['matplotlib-inline', '0.1.3']\n",
      "['mistune @ file:///C:/ci/mistune_1594373272338/work']\n",
      "['mkl-fft', '1.3.0']\n",
      "['mkl-service', '2.3.0']\n",
      "['mock @ file:///tmp/build/80754af9/mock_1607622725907/work']\n",
      "['mpld3', '0.3']\n",
      "['msgpack @ file:///C:/ci/msgpack-python_1612287191162/work']\n",
      "['multidict', '5.2.0']\n",
      "['munkres', '1.1.4']\n",
      "['mxnet', '1.4.0']\n",
      "['nb-conda', '2.2.1']\n",
      "['nb-conda-kernels @ file:///C:/ci/nb_conda_kernels_1606832727237/work']\n",
      "['nbclient @ file:///tmp/build/80754af9/nbclient_1645431659072/work']\n",
      "['nbconvert', '6.2.0']\n",
      "['nbformat @ file:///tmp/build/80754af9/nbformat_1617383369282/work']\n",
      "['nest-asyncio @ file:///tmp/build/80754af9/nest-asyncio_1613680548246/work']\n",
      "['networkx', '2.5.1']\n",
      "['nltk @ file:///tmp/build/80754af9/nltk_1618327084230/work']\n",
      "['notebook @ file:///C:/ci/notebook_1645002740769/work']\n",
      "['numexpr @ file:///C:/ci/numexpr_1614798720432/work']\n",
      "['numpy', '1.21.2']\n",
      "['olefile', '0.46']\n",
      "['omegaconf @ file:///D:/bld/omegaconf_1636817949024/work']\n",
      "['overrides', '3.1.0']\n",
      "['packaging', '21.0']\n",
      "['pandas @ file:///C:/ci/pandas_1627570311072/work']\n",
      "['pandocfilters @ file:///opt/conda/conda-bld/pandocfilters_1643405455980/work']\n",
      "['parso @ file:///opt/conda/conda-bld/parso_1641458642106/work']\n",
      "['partd @ file:///tmp/build/80754af9/partd_1618000087440/work']\n",
      "['pickleshare @ file:///tmp/build/80754af9/pickleshare_1606932040724/work']\n",
      "['Pillow', '8.1.0']\n",
      "['portalocker', '2.4.0']\n",
      "['prometheus-client @ file:///opt/conda/conda-bld/prometheus_client_1643788673601/work']\n",
      "['prompt-toolkit', '3.0.22']\n",
      "['psutil @ file:///C:/ci/psutil_1612298033174/work']\n",
      "['py-babelnet', '0.0.2']\n",
      "['py-cpuinfo', '7.0.0']\n",
      "['pyarrow', '6.0.1']\n",
      "['pycparser @ file:///tmp/build/80754af9/pycparser_1594388511720/work']\n",
      "['Pygments @ file:///opt/conda/conda-bld/pygments_1644249106324/work']\n",
      "['pyparsing', '2.4.7']\n",
      "['pyrsistent @ file:///C:/ci/pyrsistent_1636093257833/work']\n",
      "['PySocks', '1.7.1']\n",
      "['python-dateutil @ file:///tmp/build/80754af9/python-dateutil_1626374649649/work']\n",
      "['pytorch-nlp', '0.5.0']\n",
      "['pytz @ file:///tmp/build/80754af9/pytz_1612215392582/work']\n",
      "['pywin32', '302']\n",
      "['pywinpty @ file:///C:/ci_310/pywinpty_1644230983541/work/target/wheels/pywinpty-2.0.2-cp37-none-win_amd64.whl']\n",
      "['PyYAML', '6.0']\n",
      "['pyzmq @ file:///C:/ci/pyzmq_1638435182681/work']\n",
      "['qtconsole', '5.1.1']\n",
      "['QtPy', '1.11.2']\n",
      "['regex', '2021.10.8']\n",
      "['requests', '2.26.0']\n",
      "['sacrebleu @ file:///tmp/build/80754af9/sacrebleu_1634578602645/work']\n",
      "['sacremoses', '0.0.46']\n",
      "['scikit-learn', '0.23.2']\n",
      "['scipy @ file:///C:/bld/scipy_1637806996411/work']\n",
      "['seaborn @ file:///tmp/build/80754af9/seaborn_1629307859561/work']\n",
      "['segtok', '1.5.10']\n",
      "['Send2Trash @ file:///tmp/build/80754af9/send2trash_1632406701022/work']\n",
      "['sentencepiece', '0.1.95']\n",
      "['simpervisor', '0.4']\n",
      "['six', '1.16.0']\n",
      "['smart-open', '4.1.0']\n",
      "['sniffio', '1.2.0']\n",
      "['sortedcontainers @ file:///tmp/build/80754af9/sortedcontainers_1623949099177/work']\n",
      "['spacy', '2.3.5']\n",
      "['sqlitedict', '1.7.0']\n",
      "['tables', '3.6.1']\n",
      "['tabulate', '0.8.9']\n",
      "['tblib @ file:///Users/ktietz/demo/mc3/conda-bld/tblib_1629402031467/work']\n",
      "['terminado @ file:///C:/ci/terminado_1644322782754/work']\n",
      "['testpath @ file:///tmp/build/80754af9/testpath_1624638946665/work']\n",
      "['thinc', '7.4.5']\n",
      "['threadpoolctl', '2.1.0']\n",
      "['tokenizers', '0.10.3']\n",
      "['tomotopy', '0.9.1']\n",
      "['toolz @ file:///tmp/build/80754af9/toolz_1636545406491/work']\n",
      "['torch', '1.10.1']\n",
      "['torchtext', '0.11.1']\n",
      "['tornado @ file:///C:/ci/tornado_1606935947090/work']\n",
      "['tqdm', '4.62.3']\n",
      "['traitlets @ file:///tmp/build/80754af9/traitlets_1636710298902/work']\n",
      "['transformers', '4.11.3']\n",
      "['txaio', '21.2.1']\n",
      "['typing', '3.6.6']\n",
      "['typing-extensions', '3.10.0.2']\n",
      "['urllib3', '1.26.7']\n",
      "['vpython', '7.6.2']\n",
      "['wcwidth @ file:///Users/ktietz/demo/mc3/conda-bld/wcwidth_1629357192024/work']\n",
      "['webcolors', '1.3']\n",
      "['webencodings', '0.5.1']\n",
      "['websocket-client', '1.2.1']\n",
      "['widgetsnbextension', '3.5.2']\n",
      "['wincertstore', '0.2']\n",
      "['wrapt', '1.12.1']\n",
      "['yarl', '1.7.2']\n",
      "['z3-solver', '4.8.13.0']\n",
      "['zict', '2.0.0']\n",
      "['zipp', '3.6.0']\n"
     ]
    }
   ],
   "source": [
    "import types\n",
    "\n",
    "def imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            yield val.__name__\n",
    "\n",
    "excludes = ['builtins', 'types', 'sys']\n",
    "\n",
    "imported_modules = [module for module in imports() if module not in excludes]\n",
    "\n",
    "clean_modules = []\n",
    "\n",
    "for module in imported_modules:\n",
    "\n",
    "    sep = '.'  # to handle 'matplotlib.pyplot' cases\n",
    "    rest = module.split(sep, 1)[0]\n",
    "    clean_modules.append(rest)\n",
    "\n",
    "changed_imported_modules = list(set(clean_modules))  # drop duplicates\n",
    "\n",
    "pip_modules = !pip freeze  # you could also use `!conda list` with anaconda\n",
    "\n",
    "for module in pip_modules:\n",
    "    print(module.split('=='))\n",
    "    \n",
    "    #if name in changed_imported_modules:\n",
    "     #   print(name + '\\t' + version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install kiwisolver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import math\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import bcolz\n",
    "\n",
    "import time\n",
    "import random\n",
    "import functools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.10 (default, Feb 26 2021, 13:06:18) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"../data/test_transformer/\"\n",
    "# split the dataset into training, validation and testing\n",
    "train_path = \"train.csv\"\n",
    "validate_path = \"validate.csv\"\n",
    "test_path = \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "tensor([1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02])\n"
     ]
    }
   ],
   "source": [
    "def to_tensor(string_list):\n",
    "    l_str = []\n",
    "    for ele in string_list:\n",
    "        if ele[0] == \"[\":\n",
    "            l_str.append(ele[1:])\n",
    "        else:\n",
    "            if ele[-1] == \"]\":\n",
    "                l_str.append(ele[:-1])\n",
    "            else:\n",
    "                l_str.append(ele)\n",
    "\n",
    "    str_vec = \" \".join(l_str)\n",
    "    torch_labels = torch.tensor(list(map(float, str_vec.split(' '))), dtype=torch.float32)\n",
    "    return torch_labels\n",
    "\n",
    "t = to_tensor(['[142676.0', '107.17', '71890.08', '0.0', '106.5]'])\n",
    "print(type(t), type(t[0]))\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_data(file):\n",
    "    \"\"\"\n",
    "    reads the stem word and the spatial tag of each token in the .csv file\n",
    "    :param corpus_file:\n",
    "    :param datafields:\n",
    "    :return: List of training data of the form [[tokenized_sentence-1, spatial_tensors],\n",
    "                                                [tokenized_sentence-1, spatial_tensors], ...]\n",
    "    \"\"\"\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        examples = []\n",
    "        words = []\n",
    "        lemmas = []\n",
    "        synset_offset = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                examples.append([lemmas, synset_offset, labels])\n",
    "                words = []\n",
    "                lemmas = []\n",
    "                synset_offset = []\n",
    "                labels = []\n",
    "            else:\n",
    "                columns = line.split()\n",
    "                words.append(columns[0])\n",
    "                lemmas.append(columns[1])\n",
    "                synset_offset.append(columns[-6])\n",
    "                lab = to_tensor(columns[-5:])\n",
    "                labels.append(lab)\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['have',\n",
       "   'the',\n",
       "   'necessary',\n",
       "   'means',\n",
       "   'or',\n",
       "   'skill',\n",
       "   'or',\n",
       "   'know-how',\n",
       "   'or',\n",
       "   'authority',\n",
       "   'to',\n",
       "   'do',\n",
       "   'something'],\n",
       "  ['no-synset',\n",
       "   'no-synset',\n",
       "   'necessary.a.01',\n",
       "   'means.n.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'know-how.n.01',\n",
       "   'no-synset',\n",
       "   'authority.n.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.1174e+05, 9.8310e+01, 9.8013e+04, 0.0000e+00, 1.8500e+01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([3.7587e+04, 1.0434e+02, 1.9497e+05, 0.0000e+00, 7.5000e+00]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['not',\n",
       "   'have',\n",
       "   'the',\n",
       "   'necessary',\n",
       "   'means',\n",
       "   'or',\n",
       "   'skill',\n",
       "   'or',\n",
       "   'know-how'],\n",
       "  ['not.r.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'necessary.a.01',\n",
       "   'means.n.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'know-how.n.01'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.1174e+05, 9.8310e+01, 9.8013e+04, 0.0000e+00, 1.8500e+01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02])]],\n",
       " [['have',\n",
       "   'or',\n",
       "   'be',\n",
       "   'many%3|more%3|much%3|more%4|much%4',\n",
       "   'than',\n",
       "   'normal',\n",
       "   'or',\n",
       "   'necessary',\n",
       "   ':'],\n",
       "  ['no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'normal.a.01',\n",
       "   'no-synset',\n",
       "   'necessary.a.01',\n",
       "   'no-synset'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.2872e+05, 2.0550e+01, 5.4535e+04, 7.8460e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['involve',\n",
       "   'deductive_reasoning',\n",
       "   'from',\n",
       "   'a',\n",
       "   'general',\n",
       "   'principle',\n",
       "   'to',\n",
       "   'a',\n",
       "   'necessary',\n",
       "   'effect'],\n",
       "  ['no-synset',\n",
       "   'deduction.n.04',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'general.a.01',\n",
       "   'principle.n.03',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'necessary.a.01',\n",
       "   'no-synset'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([4.9319e+04, 1.0515e+02, 1.6306e+05, 0.0000e+00, 1.5000e+00]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.1143e+05, 3.0800e+01, 8.3244e+04, 7.8460e+01, 5.0000e-01]),\n",
       "   tensor([1.1814e+05, 1.0754e+02, 9.5664e+04, 0.0000e+00, 1.4500e+01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['not', 'support', 'by', 'fact'],\n",
       "  ['not.r.01', 'corroborate.v.03', 'no-synset', 'no-synset'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.4906e+04, 1.0194e+02, 1.1614e+05, 1.8000e+02, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['not', 'support', 'by', 'fact'],\n",
       "  ['not.r.01', 'confirm.v.01', 'no-synset', 'no-synset'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.4906e+04, 1.0194e+02, 1.1483e+05, 1.8000e+02, 2.8500e+01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['lack', 'necessary', 'physical_ability', 'or', 'mental_ability'],\n",
       "  ['miss.v.06',\n",
       "   'necessary.a.01',\n",
       "   'physical_ability.n.01',\n",
       "   'no-synset',\n",
       "   'capacity.n.08'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([5.1520e+03, 1.1805e+02, 2.2714e+05, 0.0000e+00, 2.5000e+00]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([8.6015e+04, 1.0743e+02, 1.2885e+05, 0.0000e+00, 1.5000e+00])]],\n",
       " [['the', 'necessary', 'consequence', 'of', 'one', 'action'],\n",
       "  ['no-synset',\n",
       "   'necessary.s.02',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 7.6603e+04, 9.5740e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['have',\n",
       "   'every',\n",
       "   'necessary',\n",
       "   'or',\n",
       "   'normal',\n",
       "   'part',\n",
       "   'or',\n",
       "   'component',\n",
       "   'or',\n",
       "   'step'],\n",
       "  ['no-synset',\n",
       "   'no-synset',\n",
       "   'necessary.a.01',\n",
       "   'no-synset',\n",
       "   'normal.a.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.2872e+05, 2.0550e+01, 5.4535e+04, 7.8460e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['have',\n",
       "   'or',\n",
       "   'display',\n",
       "   'all',\n",
       "   'the',\n",
       "   'characteristic',\n",
       "   'necessary',\n",
       "   'for',\n",
       "   'completeness'],\n",
       "  ['no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'feature.n.01',\n",
       "   'necessary.a.01',\n",
       "   'no-synset',\n",
       "   'completeness.n.01'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([5.0349e+04, 1.0389e+02, 1.6340e+05, 0.0000e+00, 2.4500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.6165e+04, 1.1673e+02, 2.1134e+05, 0.0000e+00, 3.5000e+00])]],\n",
       " [['have',\n",
       "   'or',\n",
       "   'display',\n",
       "   'all',\n",
       "   'the',\n",
       "   'characteristic',\n",
       "   'necessary',\n",
       "   'for',\n",
       "   'completeness'],\n",
       "  ['no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'characteristic.n.02',\n",
       "   'necessary.a.01',\n",
       "   'no-synset',\n",
       "   'completeness.n.01'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([5.0349e+04, 1.0389e+02, 1.8358e+05, 0.0000e+00, 3.9500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.6165e+04, 1.1673e+02, 2.1134e+05, 0.0000e+00, 3.5000e+00])]],\n",
       " [['(',\n",
       "   'of',\n",
       "   'a',\n",
       "   'boat',\n",
       "   'or',\n",
       "   'vessel',\n",
       "   ')',\n",
       "   'furnished',\n",
       "   'with',\n",
       "   'necessary',\n",
       "   'official_document',\n",
       "   'specify',\n",
       "   'ownership',\n",
       "   'etc.'],\n",
       "  ['no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'boat.n.01',\n",
       "   'no-synset',\n",
       "   'vessel.n.02',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'necessary.a.01',\n",
       "   'legal_document.n.01',\n",
       "   'no-synset',\n",
       "   'ownership.n.03',\n",
       "   'and_so_forth.r.01'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([3.4035e+04, 7.8060e+01, 1.7478e+05, 0.0000e+00, 6.4500e+01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([6.9658e+04, 7.3010e+01, 1.3975e+05, 0.0000e+00, 1.8950e+02]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([7.7181e+04, 1.1327e+02, 1.4597e+05, 0.0000e+00, 2.8850e+02]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([6.2580e+03, 1.0677e+02, 2.2125e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([8.7896e+04, 1.7910e+02, 6.2122e+04, 0.0000e+00, 5.0000e-01])]],\n",
       " [['lack',\n",
       "   'necessary',\n",
       "   'document',\n",
       "   '(',\n",
       "   'as',\n",
       "   'for',\n",
       "   'e.g.',\n",
       "   'permission',\n",
       "   'to',\n",
       "   'live',\n",
       "   'or',\n",
       "   'work',\n",
       "   'in',\n",
       "   'a',\n",
       "   'country',\n",
       "   ')'],\n",
       "  ['miss.v.06',\n",
       "   'necessary.a.01',\n",
       "   'document.n.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'for_example.r.01',\n",
       "   'license.n.04',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'country.n.02',\n",
       "   'no-synset'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([6.4798e+04, 1.0888e+02, 1.5864e+05, 0.0000e+00, 3.9050e+02]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([6.6105e+04, 1.7943e+02, 8.3903e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.5885e+04, 1.0822e+02, 1.6395e+05, 0.0000e+00, 1.3500e+01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.4370e+03, 9.3970e+01, 2.0489e+05, 0.0000e+00, 2.2750e+02]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['lack', 'necessary', 'force', 'for', 'effectiveness'],\n",
       "  ['miss.v.06',\n",
       "   'necessary.a.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'effectiveness.n.01'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3004e+05, 1.1761e+02, 1.0131e+05, 0.0000e+00, 2.5000e+00])]],\n",
       " [['use',\n",
       "   'the',\n",
       "   'minimum',\n",
       "   'of',\n",
       "   'time',\n",
       "   'or',\n",
       "   'resource',\n",
       "   'necessary',\n",
       "   'for',\n",
       "   'effectiveness'],\n",
       "  ['no-synset',\n",
       "   'no-synset',\n",
       "   'minimum.n.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'resource.n.01',\n",
       "   'necessary.a.01',\n",
       "   'no-synset',\n",
       "   'effectiveness.n.01'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([8.8864e+04, 7.5350e+01, 1.4785e+05, 5.3130e+01, 1.5000e+00]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.8408e+04, 1.0795e+02, 1.8866e+05, 0.0000e+00, 1.2500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3004e+05, 1.1761e+02, 1.0131e+05, 0.0000e+00, 2.5000e+00])]],\n",
       " [['possible', 'but', 'not', 'necessary'],\n",
       "  ['possible.a.01', 'no-synset', 'not.r.01', 'necessary.a.01'],\n",
       "  [tensor([8.5479e+04, 6.0380e+01, 1.2985e+05, 9.5740e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " [['leave', 'to', 'personal', 'choice'],\n",
       "  ['no-synset', 'no-synset', 'personal.a.01', 'choice.n.02'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([2.7140e+04, 1.9770e+01, 1.2471e+05, 2.5840e+01, 5.0000e-01]),\n",
       "   tensor([1.4028e+05, 7.7850e+01, 1.0613e+05, 6.0000e+01, 5.9500e+01])]],\n",
       " [['morally', 'bind', 'or', 'necessary'],\n",
       "  ['morally.r.01', 'no-synset', 'no-synset', 'necessary.a.01'],\n",
       "  [tensor([2.4772e+04, 1.7990e+02, 1.2524e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " [['urgently', 'need'],\n",
       "  ['urgently.r.01', 'need.v.03'],\n",
       "  [tensor([2.6208e+04, 1.7894e+02, 1.2382e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([8.9570e+03, 1.0177e+02, 7.9864e+04, 1.8000e+02, 5.0000e-01])]],\n",
       " [['absolutely', 'necessary'],\n",
       "  ['absolutely.r.02', 'necessary.a.01'],\n",
       "  [tensor([8.1604e+04, 1.7865e+02, 6.8438e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = parse_data(path + train_path)\n",
    "# validation_data = parse_data(path + validate_path)\n",
    "# testing_data = parse_data(path + test_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data: [lemma, synset offset, spatial label]\n",
    "# remove all the words, where synset offset is 'no-synset'\n",
    "\n",
    "def clean_untagged(data):\n",
    "    original_data = data\n",
    "    for entry in data:\n",
    "        \n",
    "        idx = [i for i, syn in enumerate(entry[1]) if syn == 'no-synset']\n",
    "\n",
    "        # remove those from the data\n",
    "        for s in reversed(idx):\n",
    "            del entry[0][s]\n",
    "            del entry[1][s]\n",
    "            del entry[2][s]\n",
    "\n",
    "    return original_data, data\n",
    "\n",
    "orig, data = clean_untagged(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['necessary', 'means', 'know-how', 'authority'],\n",
       "  ['necessary.a.01', 'means.n.01', 'know-how.n.01', 'authority.n.01'],\n",
       "  [tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.1174e+05, 9.8310e+01, 9.8013e+04, 0.0000e+00, 1.8500e+01]),\n",
       "   tensor([1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02]),\n",
       "   tensor([3.7587e+04, 1.0434e+02, 1.9497e+05, 0.0000e+00, 7.5000e+00])]],\n",
       " [['not', 'necessary', 'means', 'know-how'],\n",
       "  ['not.r.01', 'necessary.a.01', 'means.n.01', 'know-how.n.01'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.1174e+05, 9.8310e+01, 9.8013e+04, 0.0000e+00, 1.8500e+01]),\n",
       "   tensor([1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02])]],\n",
       " [['normal', 'necessary'],\n",
       "  ['normal.a.01', 'necessary.a.01'],\n",
       "  [tensor([1.2872e+05, 2.0550e+01, 5.4535e+04, 7.8460e+01, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " [['deductive_reasoning', 'general', 'principle', 'necessary'],\n",
       "  ['deduction.n.04', 'general.a.01', 'principle.n.03', 'necessary.a.01'],\n",
       "  [tensor([4.9319e+04, 1.0515e+02, 1.6306e+05, 0.0000e+00, 1.5000e+00]),\n",
       "   tensor([1.1143e+05, 3.0800e+01, 8.3244e+04, 7.8460e+01, 5.0000e-01]),\n",
       "   tensor([1.1814e+05, 1.0754e+02, 9.5664e+04, 0.0000e+00, 1.4500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " [['not', 'support'],\n",
       "  ['not.r.01', 'corroborate.v.03'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.4906e+04, 1.0194e+02, 1.1614e+05, 1.8000e+02, 5.0000e-01])]],\n",
       " [['not', 'support'],\n",
       "  ['not.r.01', 'confirm.v.01'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.4906e+04, 1.0194e+02, 1.1483e+05, 1.8000e+02, 2.8500e+01])]],\n",
       " [['lack', 'necessary', 'physical_ability', 'mental_ability'],\n",
       "  ['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([5.1520e+03, 1.1805e+02, 2.2714e+05, 0.0000e+00, 2.5000e+00]),\n",
       "   tensor([8.6015e+04, 1.0743e+02, 1.2885e+05, 0.0000e+00, 1.5000e+00])]],\n",
       " [['necessary'],\n",
       "  ['necessary.s.02'],\n",
       "  [tensor([1.3514e+05, 2.5010e+01, 7.6603e+04, 9.5740e+01, 5.0000e-01])]],\n",
       " [['necessary', 'normal'],\n",
       "  ['necessary.a.01', 'normal.a.01'],\n",
       "  [tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.2872e+05, 2.0550e+01, 5.4535e+04, 7.8460e+01, 5.0000e-01])]],\n",
       " [['characteristic', 'necessary', 'completeness'],\n",
       "  ['feature.n.01', 'necessary.a.01', 'completeness.n.01'],\n",
       "  [tensor([5.0349e+04, 1.0389e+02, 1.6340e+05, 0.0000e+00, 2.4500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.6165e+04, 1.1673e+02, 2.1134e+05, 0.0000e+00, 3.5000e+00])]],\n",
       " [['characteristic', 'necessary', 'completeness'],\n",
       "  ['characteristic.n.02', 'necessary.a.01', 'completeness.n.01'],\n",
       "  [tensor([5.0349e+04, 1.0389e+02, 1.8358e+05, 0.0000e+00, 3.9500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.6165e+04, 1.1673e+02, 2.1134e+05, 0.0000e+00, 3.5000e+00])]],\n",
       " [['boat', 'vessel', 'necessary', 'official_document', 'ownership', 'etc.'],\n",
       "  ['boat.n.01',\n",
       "   'vessel.n.02',\n",
       "   'necessary.a.01',\n",
       "   'legal_document.n.01',\n",
       "   'ownership.n.03',\n",
       "   'and_so_forth.r.01'],\n",
       "  [tensor([3.4035e+04, 7.8060e+01, 1.7478e+05, 0.0000e+00, 6.4500e+01]),\n",
       "   tensor([6.9658e+04, 7.3010e+01, 1.3975e+05, 0.0000e+00, 1.8950e+02]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([7.7181e+04, 1.1327e+02, 1.4597e+05, 0.0000e+00, 2.8850e+02]),\n",
       "   tensor([6.2580e+03, 1.0677e+02, 2.2125e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([8.7896e+04, 1.7910e+02, 6.2122e+04, 0.0000e+00, 5.0000e-01])]],\n",
       " [['lack', 'necessary', 'document', 'e.g.', 'permission', 'country'],\n",
       "  ['miss.v.06',\n",
       "   'necessary.a.01',\n",
       "   'document.n.01',\n",
       "   'for_example.r.01',\n",
       "   'license.n.04',\n",
       "   'country.n.02'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([6.4798e+04, 1.0888e+02, 1.5864e+05, 0.0000e+00, 3.9050e+02]),\n",
       "   tensor([6.6105e+04, 1.7943e+02, 8.3903e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.5885e+04, 1.0822e+02, 1.6395e+05, 0.0000e+00, 1.3500e+01]),\n",
       "   tensor([1.4370e+03, 9.3970e+01, 2.0489e+05, 0.0000e+00, 2.2750e+02])]],\n",
       " [['lack', 'necessary', 'effectiveness'],\n",
       "  ['miss.v.06', 'necessary.a.01', 'effectiveness.n.01'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.3004e+05, 1.1761e+02, 1.0131e+05, 0.0000e+00, 2.5000e+00])]],\n",
       " [['minimum', 'resource', 'necessary', 'effectiveness'],\n",
       "  ['minimum.n.01', 'resource.n.01', 'necessary.a.01', 'effectiveness.n.01'],\n",
       "  [tensor([8.8864e+04, 7.5350e+01, 1.4785e+05, 5.3130e+01, 1.5000e+00]),\n",
       "   tensor([1.8408e+04, 1.0795e+02, 1.8866e+05, 0.0000e+00, 1.2500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.3004e+05, 1.1761e+02, 1.0131e+05, 0.0000e+00, 2.5000e+00])]],\n",
       " [['possible', 'not', 'necessary'],\n",
       "  ['possible.a.01', 'not.r.01', 'necessary.a.01'],\n",
       "  [tensor([8.5479e+04, 6.0380e+01, 1.2985e+05, 9.5740e+01, 5.0000e-01]),\n",
       "   tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " [['personal', 'choice'],\n",
       "  ['personal.a.01', 'choice.n.02'],\n",
       "  [tensor([2.7140e+04, 1.9770e+01, 1.2471e+05, 2.5840e+01, 5.0000e-01]),\n",
       "   tensor([1.4028e+05, 7.7850e+01, 1.0613e+05, 6.0000e+01, 5.9500e+01])]],\n",
       " [['morally', 'necessary'],\n",
       "  ['morally.r.01', 'necessary.a.01'],\n",
       "  [tensor([2.4772e+04, 1.7990e+02, 1.2524e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " [['urgently', 'need'],\n",
       "  ['urgently.r.01', 'need.v.03'],\n",
       "  [tensor([2.6208e+04, 1.7894e+02, 1.2382e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([8.9570e+03, 1.0177e+02, 7.9864e+04, 1.8000e+02, 5.0000e-01])]],\n",
       " [['absolutely', 'necessary'],\n",
       "  ['absolutely.r.02', 'necessary.a.01'],\n",
       "  [tensor([8.1604e+04, 1.7865e+02, 6.8438e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def data_id(data):\n",
    "\n",
    "    # data_collector = {\"0\": [[], []], \"1\": [[],[]], ...}\n",
    "    data_collector = {}\n",
    "    for i, instance in enumerate(data):\n",
    "        data_collector[str(i)] = instance\n",
    "\n",
    "    return data_collector\n",
    "\n",
    "# in my case, this data must be shuffled before continuing!\n",
    "datasetID = data_id(data)\n",
    "# datasetID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': [['necessary', 'means', 'know-how', 'authority'],\n",
       "  ['necessary.a.01', 'means.n.01', 'know-how.n.01', 'authority.n.01'],\n",
       "  [tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.1174e+05, 9.8310e+01, 9.8013e+04, 0.0000e+00, 1.8500e+01]),\n",
       "   tensor([1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02]),\n",
       "   tensor([3.7587e+04, 1.0434e+02, 1.9497e+05, 0.0000e+00, 7.5000e+00])]],\n",
       " '1': [['not', 'necessary', 'means', 'know-how'],\n",
       "  ['not.r.01', 'necessary.a.01', 'means.n.01', 'know-how.n.01'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.1174e+05, 9.8310e+01, 9.8013e+04, 0.0000e+00, 1.8500e+01]),\n",
       "   tensor([1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02])]],\n",
       " '2': [['normal', 'necessary'],\n",
       "  ['normal.a.01', 'necessary.a.01'],\n",
       "  [tensor([1.2872e+05, 2.0550e+01, 5.4535e+04, 7.8460e+01, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " '3': [['deductive_reasoning', 'general', 'principle', 'necessary'],\n",
       "  ['deduction.n.04', 'general.a.01', 'principle.n.03', 'necessary.a.01'],\n",
       "  [tensor([4.9319e+04, 1.0515e+02, 1.6306e+05, 0.0000e+00, 1.5000e+00]),\n",
       "   tensor([1.1143e+05, 3.0800e+01, 8.3244e+04, 7.8460e+01, 5.0000e-01]),\n",
       "   tensor([1.1814e+05, 1.0754e+02, 9.5664e+04, 0.0000e+00, 1.4500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " '4': [['not', 'support'],\n",
       "  ['not.r.01', 'corroborate.v.03'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.4906e+04, 1.0194e+02, 1.1614e+05, 1.8000e+02, 5.0000e-01])]],\n",
       " '5': [['not', 'support'],\n",
       "  ['not.r.01', 'confirm.v.01'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.4906e+04, 1.0194e+02, 1.1483e+05, 1.8000e+02, 2.8500e+01])]],\n",
       " '6': [['lack', 'necessary', 'physical_ability', 'mental_ability'],\n",
       "  ['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([5.1520e+03, 1.1805e+02, 2.2714e+05, 0.0000e+00, 2.5000e+00]),\n",
       "   tensor([8.6015e+04, 1.0743e+02, 1.2885e+05, 0.0000e+00, 1.5000e+00])]],\n",
       " '7': [['necessary'],\n",
       "  ['necessary.s.02'],\n",
       "  [tensor([1.3514e+05, 2.5010e+01, 7.6603e+04, 9.5740e+01, 5.0000e-01])]],\n",
       " '8': [['necessary', 'normal'],\n",
       "  ['necessary.a.01', 'normal.a.01'],\n",
       "  [tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.2872e+05, 2.0550e+01, 5.4535e+04, 7.8460e+01, 5.0000e-01])]],\n",
       " '9': [['characteristic', 'necessary', 'completeness'],\n",
       "  ['feature.n.01', 'necessary.a.01', 'completeness.n.01'],\n",
       "  [tensor([5.0349e+04, 1.0389e+02, 1.6340e+05, 0.0000e+00, 2.4500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.6165e+04, 1.1673e+02, 2.1134e+05, 0.0000e+00, 3.5000e+00])]],\n",
       " '10': [['characteristic', 'necessary', 'completeness'],\n",
       "  ['characteristic.n.02', 'necessary.a.01', 'completeness.n.01'],\n",
       "  [tensor([5.0349e+04, 1.0389e+02, 1.8358e+05, 0.0000e+00, 3.9500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.6165e+04, 1.1673e+02, 2.1134e+05, 0.0000e+00, 3.5000e+00])]],\n",
       " '11': [['boat',\n",
       "   'vessel',\n",
       "   'necessary',\n",
       "   'official_document',\n",
       "   'ownership',\n",
       "   'etc.'],\n",
       "  ['boat.n.01',\n",
       "   'vessel.n.02',\n",
       "   'necessary.a.01',\n",
       "   'legal_document.n.01',\n",
       "   'ownership.n.03',\n",
       "   'and_so_forth.r.01'],\n",
       "  [tensor([3.4035e+04, 7.8060e+01, 1.7478e+05, 0.0000e+00, 6.4500e+01]),\n",
       "   tensor([6.9658e+04, 7.3010e+01, 1.3975e+05, 0.0000e+00, 1.8950e+02]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([7.7181e+04, 1.1327e+02, 1.4597e+05, 0.0000e+00, 2.8850e+02]),\n",
       "   tensor([6.2580e+03, 1.0677e+02, 2.2125e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([8.7896e+04, 1.7910e+02, 6.2122e+04, 0.0000e+00, 5.0000e-01])]],\n",
       " '12': [['lack', 'necessary', 'document', 'e.g.', 'permission', 'country'],\n",
       "  ['miss.v.06',\n",
       "   'necessary.a.01',\n",
       "   'document.n.01',\n",
       "   'for_example.r.01',\n",
       "   'license.n.04',\n",
       "   'country.n.02'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([6.4798e+04, 1.0888e+02, 1.5864e+05, 0.0000e+00, 3.9050e+02]),\n",
       "   tensor([6.6105e+04, 1.7943e+02, 8.3903e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.5885e+04, 1.0822e+02, 1.6395e+05, 0.0000e+00, 1.3500e+01]),\n",
       "   tensor([1.4370e+03, 9.3970e+01, 2.0489e+05, 0.0000e+00, 2.2750e+02])]],\n",
       " '13': [['lack', 'necessary', 'effectiveness'],\n",
       "  ['miss.v.06', 'necessary.a.01', 'effectiveness.n.01'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.3004e+05, 1.1761e+02, 1.0131e+05, 0.0000e+00, 2.5000e+00])]],\n",
       " '14': [['minimum', 'resource', 'necessary', 'effectiveness'],\n",
       "  ['minimum.n.01', 'resource.n.01', 'necessary.a.01', 'effectiveness.n.01'],\n",
       "  [tensor([8.8864e+04, 7.5350e+01, 1.4785e+05, 5.3130e+01, 1.5000e+00]),\n",
       "   tensor([1.8408e+04, 1.0795e+02, 1.8866e+05, 0.0000e+00, 1.2500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.3004e+05, 1.1761e+02, 1.0131e+05, 0.0000e+00, 2.5000e+00])]],\n",
       " '15': [['possible', 'not', 'necessary'],\n",
       "  ['possible.a.01', 'not.r.01', 'necessary.a.01'],\n",
       "  [tensor([8.5479e+04, 6.0380e+01, 1.2985e+05, 9.5740e+01, 5.0000e-01]),\n",
       "   tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " '16': [['personal', 'choice'],\n",
       "  ['personal.a.01', 'choice.n.02'],\n",
       "  [tensor([2.7140e+04, 1.9770e+01, 1.2471e+05, 2.5840e+01, 5.0000e-01]),\n",
       "   tensor([1.4028e+05, 7.7850e+01, 1.0613e+05, 6.0000e+01, 5.9500e+01])]],\n",
       " '17': [['morally', 'necessary'],\n",
       "  ['morally.r.01', 'necessary.a.01'],\n",
       "  [tensor([2.4772e+04, 1.7990e+02, 1.2524e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " '18': [['urgently', 'need'],\n",
       "  ['urgently.r.01', 'need.v.03'],\n",
       "  [tensor([2.6208e+04, 1.7894e+02, 1.2382e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([8.9570e+03, 1.0177e+02, 7.9864e+04, 1.8000e+02, 5.0000e-01])]],\n",
       " '19': [['absolutely', 'necessary'],\n",
       "  ['absolutely.r.02', 'necessary.a.01'],\n",
       "  [tensor([8.1604e+04, 1.7865e+02, 6.8438e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create dict of key-synset\n",
    "train_val_syn = {}\n",
    "train_val_syn.fromkeys(datasetID.keys())\n",
    "for key, ele in datasetID.items():\n",
    "    train_val_syn[key] = ele[1]\n",
    "# train_val_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': ['necessary.a.01', 'means.n.01', 'know-how.n.01', 'authority.n.01'],\n",
       " '1': ['not.r.01', 'necessary.a.01', 'means.n.01', 'know-how.n.01'],\n",
       " '2': ['normal.a.01', 'necessary.a.01'],\n",
       " '3': ['deduction.n.04', 'general.a.01', 'principle.n.03', 'necessary.a.01'],\n",
       " '4': ['not.r.01', 'corroborate.v.03'],\n",
       " '5': ['not.r.01', 'confirm.v.01'],\n",
       " '6': ['miss.v.06',\n",
       "  'necessary.a.01',\n",
       "  'physical_ability.n.01',\n",
       "  'capacity.n.08'],\n",
       " '7': ['necessary.s.02'],\n",
       " '8': ['necessary.a.01', 'normal.a.01'],\n",
       " '9': ['feature.n.01', 'necessary.a.01', 'completeness.n.01'],\n",
       " '10': ['characteristic.n.02', 'necessary.a.01', 'completeness.n.01'],\n",
       " '11': ['boat.n.01',\n",
       "  'vessel.n.02',\n",
       "  'necessary.a.01',\n",
       "  'legal_document.n.01',\n",
       "  'ownership.n.03',\n",
       "  'and_so_forth.r.01'],\n",
       " '12': ['miss.v.06',\n",
       "  'necessary.a.01',\n",
       "  'document.n.01',\n",
       "  'for_example.r.01',\n",
       "  'license.n.04',\n",
       "  'country.n.02'],\n",
       " '13': ['miss.v.06', 'necessary.a.01', 'effectiveness.n.01'],\n",
       " '14': ['minimum.n.01',\n",
       "  'resource.n.01',\n",
       "  'necessary.a.01',\n",
       "  'effectiveness.n.01'],\n",
       " '15': ['possible.a.01', 'not.r.01', 'necessary.a.01'],\n",
       " '16': ['personal.a.01', 'choice.n.02'],\n",
       " '17': ['morally.r.01', 'necessary.a.01'],\n",
       " '18': ['urgently.r.01', 'need.v.03'],\n",
       " '19': ['absolutely.r.02', 'necessary.a.01']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# partition data in training/validation\n",
    "splittings = {}\n",
    "labels = {}\n",
    "labels.fromkeys(datasetID.keys())\n",
    "for key, ele in datasetID.items():\n",
    "    labels[key] = ele[2]\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_train = 10\n",
    "N_valid = 5\n",
    "N_test = 5\n",
    "# choose N training instances, randomly!\n",
    "splittings[\"train\"] = random.sample(list(datasetID), N_train)\n",
    "splittings[\"validate\"] = random.sample(list(set(datasetID) - set(splittings[\"train\"])), N_valid)\n",
    "# splittings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Saving and loading data to/from .pt\n",
    "# save\n",
    "#torch.save(datasetID, path + \"pwngc_id.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': [['necessary', 'means', 'know-how', 'authority'],\n",
       "  ['necessary.a.01', 'means.n.01', 'know-how.n.01', 'authority.n.01'],\n",
       "  [tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.1174e+05, 9.8310e+01, 9.8013e+04, 0.0000e+00, 1.8500e+01]),\n",
       "   tensor([1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02]),\n",
       "   tensor([3.7587e+04, 1.0434e+02, 1.9497e+05, 0.0000e+00, 7.5000e+00])]],\n",
       " '1': [['not', 'necessary', 'means', 'know-how'],\n",
       "  ['not.r.01', 'necessary.a.01', 'means.n.01', 'know-how.n.01'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.1174e+05, 9.8310e+01, 9.8013e+04, 0.0000e+00, 1.8500e+01]),\n",
       "   tensor([1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02])]],\n",
       " '2': [['normal', 'necessary'],\n",
       "  ['normal.a.01', 'necessary.a.01'],\n",
       "  [tensor([1.2872e+05, 2.0550e+01, 5.4535e+04, 7.8460e+01, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " '3': [['deductive_reasoning', 'general', 'principle', 'necessary'],\n",
       "  ['deduction.n.04', 'general.a.01', 'principle.n.03', 'necessary.a.01'],\n",
       "  [tensor([4.9319e+04, 1.0515e+02, 1.6306e+05, 0.0000e+00, 1.5000e+00]),\n",
       "   tensor([1.1143e+05, 3.0800e+01, 8.3244e+04, 7.8460e+01, 5.0000e-01]),\n",
       "   tensor([1.1814e+05, 1.0754e+02, 9.5664e+04, 0.0000e+00, 1.4500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " '4': [['not', 'support'],\n",
       "  ['not.r.01', 'corroborate.v.03'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.4906e+04, 1.0194e+02, 1.1614e+05, 1.8000e+02, 5.0000e-01])]],\n",
       " '5': [['not', 'support'],\n",
       "  ['not.r.01', 'confirm.v.01'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.4906e+04, 1.0194e+02, 1.1483e+05, 1.8000e+02, 2.8500e+01])]],\n",
       " '6': [['lack', 'necessary', 'physical_ability', 'mental_ability'],\n",
       "  ['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([5.1520e+03, 1.1805e+02, 2.2714e+05, 0.0000e+00, 2.5000e+00]),\n",
       "   tensor([8.6015e+04, 1.0743e+02, 1.2885e+05, 0.0000e+00, 1.5000e+00])]],\n",
       " '7': [['necessary'],\n",
       "  ['necessary.s.02'],\n",
       "  [tensor([1.3514e+05, 2.5010e+01, 7.6603e+04, 9.5740e+01, 5.0000e-01])]],\n",
       " '8': [['necessary', 'normal'],\n",
       "  ['necessary.a.01', 'normal.a.01'],\n",
       "  [tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.2872e+05, 2.0550e+01, 5.4535e+04, 7.8460e+01, 5.0000e-01])]],\n",
       " '9': [['characteristic', 'necessary', 'completeness'],\n",
       "  ['feature.n.01', 'necessary.a.01', 'completeness.n.01'],\n",
       "  [tensor([5.0349e+04, 1.0389e+02, 1.6340e+05, 0.0000e+00, 2.4500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.6165e+04, 1.1673e+02, 2.1134e+05, 0.0000e+00, 3.5000e+00])]],\n",
       " '10': [['characteristic', 'necessary', 'completeness'],\n",
       "  ['characteristic.n.02', 'necessary.a.01', 'completeness.n.01'],\n",
       "  [tensor([5.0349e+04, 1.0389e+02, 1.8358e+05, 0.0000e+00, 3.9500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.6165e+04, 1.1673e+02, 2.1134e+05, 0.0000e+00, 3.5000e+00])]],\n",
       " '11': [['boat',\n",
       "   'vessel',\n",
       "   'necessary',\n",
       "   'official_document',\n",
       "   'ownership',\n",
       "   'etc.'],\n",
       "  ['boat.n.01',\n",
       "   'vessel.n.02',\n",
       "   'necessary.a.01',\n",
       "   'legal_document.n.01',\n",
       "   'ownership.n.03',\n",
       "   'and_so_forth.r.01'],\n",
       "  [tensor([3.4035e+04, 7.8060e+01, 1.7478e+05, 0.0000e+00, 6.4500e+01]),\n",
       "   tensor([6.9658e+04, 7.3010e+01, 1.3975e+05, 0.0000e+00, 1.8950e+02]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([7.7181e+04, 1.1327e+02, 1.4597e+05, 0.0000e+00, 2.8850e+02]),\n",
       "   tensor([6.2580e+03, 1.0677e+02, 2.2125e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([8.7896e+04, 1.7910e+02, 6.2122e+04, 0.0000e+00, 5.0000e-01])]],\n",
       " '12': [['lack', 'necessary', 'document', 'e.g.', 'permission', 'country'],\n",
       "  ['miss.v.06',\n",
       "   'necessary.a.01',\n",
       "   'document.n.01',\n",
       "   'for_example.r.01',\n",
       "   'license.n.04',\n",
       "   'country.n.02'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([6.4798e+04, 1.0888e+02, 1.5864e+05, 0.0000e+00, 3.9050e+02]),\n",
       "   tensor([6.6105e+04, 1.7943e+02, 8.3903e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.5885e+04, 1.0822e+02, 1.6395e+05, 0.0000e+00, 1.3500e+01]),\n",
       "   tensor([1.4370e+03, 9.3970e+01, 2.0489e+05, 0.0000e+00, 2.2750e+02])]],\n",
       " '13': [['lack', 'necessary', 'effectiveness'],\n",
       "  ['miss.v.06', 'necessary.a.01', 'effectiveness.n.01'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.3004e+05, 1.1761e+02, 1.0131e+05, 0.0000e+00, 2.5000e+00])]],\n",
       " '14': [['minimum', 'resource', 'necessary', 'effectiveness'],\n",
       "  ['minimum.n.01', 'resource.n.01', 'necessary.a.01', 'effectiveness.n.01'],\n",
       "  [tensor([8.8864e+04, 7.5350e+01, 1.4785e+05, 5.3130e+01, 1.5000e+00]),\n",
       "   tensor([1.8408e+04, 1.0795e+02, 1.8866e+05, 0.0000e+00, 1.2500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.3004e+05, 1.1761e+02, 1.0131e+05, 0.0000e+00, 2.5000e+00])]],\n",
       " '15': [['possible', 'not', 'necessary'],\n",
       "  ['possible.a.01', 'not.r.01', 'necessary.a.01'],\n",
       "  [tensor([8.5479e+04, 6.0380e+01, 1.2985e+05, 9.5740e+01, 5.0000e-01]),\n",
       "   tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " '16': [['personal', 'choice'],\n",
       "  ['personal.a.01', 'choice.n.02'],\n",
       "  [tensor([2.7140e+04, 1.9770e+01, 1.2471e+05, 2.5840e+01, 5.0000e-01]),\n",
       "   tensor([1.4028e+05, 7.7850e+01, 1.0613e+05, 6.0000e+01, 5.9500e+01])]],\n",
       " '17': [['morally', 'necessary'],\n",
       "  ['morally.r.01', 'necessary.a.01'],\n",
       "  [tensor([2.4772e+04, 1.7990e+02, 1.2524e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " '18': [['urgently', 'need'],\n",
       "  ['urgently.r.01', 'need.v.03'],\n",
       "  [tensor([2.6208e+04, 1.7894e+02, 1.2382e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([8.9570e+03, 1.0177e+02, 7.9864e+04, 1.8000e+02, 5.0000e-01])]],\n",
       " '19': [['absolutely', 'necessary'],\n",
       "  ['absolutely.r.02', 'necessary.a.01'],\n",
       "  [tensor([8.1604e+04, 1.7865e+02, 6.8438e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = torch.load(path + \"pwngc_id.pt\")#[\"0\"] #[0]\n",
    "li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Preparing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, list_ids, labels, train_val_syn):\n",
    "        self.labels = labels\n",
    "        self.list_ids = list_ids\n",
    "        self.train_val_syn = train_val_syn\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Total Number of samples.\"\n",
    "\n",
    "        return len(self.list_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Extracts one Example of data.\"\n",
    "\n",
    "        id = self.list_ids[index]\n",
    "\n",
    "        # data\n",
    "        X = torch.load(path + \"pwngc_id.pt\")[id][0]\n",
    "        y = self.labels[id]\n",
    "        tag_y = self.train_val_syn[id]\n",
    "\n",
    "        return X, y, tag_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Downloading GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I downloaded it from torchtext\n",
    "glove_path = \"./.vector_cache\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#import bcolz\n",
    "# # initial code from: https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n",
    "## list of words in GloVe\n",
    "#words = []\n",
    "## index of words in GloVe\n",
    "#idx = 0\n",
    "## assign an index to each word in GloVe to ease embedding while training\n",
    "## e.g. {\"Hi\":0, \"the\":1, ...}\n",
    "##word2idx = {}\n",
    "\n",
    "# #I downloaded it from torchtext\n",
    "#glove_path = \"./.vector_cache\"\n",
    "\n",
    "#vectors = bcolz.carray(np.zeros((1,300), dtype=np.float32), rootdir=f'{glove_path}/840B.300d.dat', mode='w')\n",
    "\n",
    "#glove = {}\n",
    "\n",
    "# with open(f'{glove_path}/glove.840B.300d.txt', 'rb') as f:\n",
    "#     for l in f:\n",
    "#         try:\n",
    "#             line = l.decode().split()\n",
    "#             word = line[0]\n",
    "            \n",
    "#             #word2idx[word] = idx\n",
    "#             #idx += 1\n",
    "#             vec = np.array(line[1:], dtype=np.float32)\n",
    "#             # print(vec)\n",
    "#             vectors.append(vec)\n",
    "#             words.append(word)\n",
    "#             #print(word)\n",
    "#             idx += 1\n",
    "#             print(idx)\n",
    "#             glove[word] = vec\n",
    "#             #idx += 1\n",
    "#         except:\n",
    "#             f.__next__()\n",
    "#             #words = [\".....\", \". . .\", \"atname@domain.com\"]\n",
    "#             #splits = [len(word.split()) for word in words]\n",
    "#             #for i, j in enumerate(splits):\n",
    "#             #    if line[0: j] == words[i].split() and isinstance(line[j], np.float32):\n",
    "#             #        words.append(words[i])\n",
    "#              #       word2idx[words[i]] = idx\n",
    "#               #      #idx += 1\n",
    "#                #     vec = np.array(line[j:], dtype=np.float32)\n",
    "#                 #    vectors.append(vec)\n",
    "#                  #   idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(vectors.shape)\n",
    "# wordy = [\".....\", \". . .\", \"atname@domain.com\"]\n",
    "# w = wordy[2].split()\n",
    "# print(w)\n",
    "# #word2idx['.\\xa0.\\xa0.']\n",
    "#\n",
    "# print(\". . .\" in words)\n",
    "# print(\". . . . .\" in words) # False, because it transforms '.' to float, then checks if it is a float!\n",
    "# print(\"atname@domain.com\" in words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(len(words))\n",
    "# print(len(vectors))\n",
    "# print(len(glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# From their website: https://nlp.stanford.edu/projects/glove/\n",
    "# Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download)\n",
    "# vectors = bcolz.carray(vectors[1:], rootdir=f'{glove_path}/840B.300.dat', mode='w')\n",
    "# vectors.flush()\n",
    "# pickle.dump(words, open(f'{glove_path}/840B.300_words.pkl', 'wb'))\n",
    "# #pickle.dump(word2idx, open(f'{glove_path}/840B.300_idx.pkl', 'wb'))\n",
    "# pickle.dump(glove, open(f'{glove_path}/840B.300_glove.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vectors = bcolz.open(f'{glove_path}/840B.300.dat')[:]\n",
    "# words = pickle.load(open(f'{glove_path}/840B.300_words.pkl', 'rb'))\n",
    "# #word2idx = pickle.load(open(f'{glove_path}/840B.300_idx.pkl', 'rb'))\n",
    "# #glove = {w: vectors[word2idx[w]] for w in words}\n",
    "glove = pickle.load(open(f'{glove_path}/840B.300_glove.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2195846\n"
     ]
    }
   ],
   "source": [
    "print(len(glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_vocab(data, embed_size=300):\n",
    "    \n",
    "    # insert all dataset vocabulary\n",
    "    dataset_vocab = []\n",
    "    if isinstance(data[0], str):\n",
    "        dataset_vocab = data\n",
    "    else:\n",
    "        for instance in data:            \n",
    "            dataset_vocab += instance[0]\n",
    "        \n",
    "    # print(len(dataset_vocab))\n",
    "    \n",
    "    # remove duplicates\n",
    "    target_vocab = set(dataset_vocab)\n",
    "    \n",
    "    # generate weights matrix using glove\n",
    "    matrix_len = len(target_vocab)\n",
    "    \n",
    "    weights_matrix = np.zeros((matrix_len, embed_size))\n",
    "    \n",
    "    words_found = 0\n",
    "\n",
    "    for i, word in enumerate(target_vocab):\n",
    "        #print(i, word)\n",
    "        try:\n",
    "            weights_matrix[i] = glove[word]\n",
    "            #print(weights_matrix[i])\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            weights_matrix[i] = np.random.normal(scale=0.6, size=(embed_size, ))\n",
    "            #print(weights_matrix[i])\n",
    "    #print(words_found)\n",
    "    \n",
    "    return target_vocab, weights_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preparing Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate the Dataset in dataloader\n",
    "# Define the model\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 5, #64,\n",
    "          'shuffle': True,\n",
    "          'collate_fn': lambda x: x,\n",
    "          'num_workers': 0}#6}\n",
    "max_epochs = 10 #100\n",
    "\n",
    "# Datasets\n",
    "# partition = # IDs\n",
    "# labels = # Labels\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(splittings['train'], labels, train_val_syn)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(splittings['validate'], labels, train_val_syn)\n",
    "validation_generator = torch.utils.data.DataLoader(validation_set, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "<class 'list'>\n",
      "['necessary', 'means', 'know-how', 'authority'] [tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]), tensor([1.1174e+05, 9.8310e+01, 9.8013e+04, 0.0000e+00, 1.8500e+01]), tensor([1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02]), tensor([3.7587e+04, 1.0434e+02, 1.9497e+05, 0.0000e+00, 7.5000e+00])] ['necessary.a.01', 'means.n.01', 'know-how.n.01', 'authority.n.01']\n",
      "5\n",
      "<class 'list'>\n",
      "['not', 'support'] [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]), tensor([4.4906e+04, 1.0194e+02, 1.1483e+05, 1.8000e+02, 2.8500e+01])] ['not.r.01', 'confirm.v.01']\n"
     ]
    }
   ],
   "source": [
    "for batch in training_generator:\n",
    "    print(len(batch))\n",
    "    print(type(batch))\n",
    "    for local_batch, local_label, syn in batch:\n",
    "        print(local_batch,local_label, syn)\n",
    "        break\n",
    "        #print(local_batch, local_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def numericalize(tokens_list, vocab):\n",
    "    \n",
    "    str2num = {word: index for index, word in enumerate(vocab)}\n",
    "    num_list = []\n",
    "    for token in tokens_list:\n",
    "        num_list.append(str2num[token])\n",
    "        \n",
    "    return torch.tensor(num_list, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Create the target vocab dataframe containing senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spatial_wordnet = pd.read_pickle(\"../data/wordnet_dataframes/SPATIAL_WORDNET.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>synset</th>\n",
       "      <th>word_point</th>\n",
       "      <th>l0</th>\n",
       "      <th>alpha</th>\n",
       "      <th>pos</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>radius</th>\n",
       "      <th>cx</th>\n",
       "      <th>mod_start</th>\n",
       "      <th>mod_end</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>l_i</th>\n",
       "      <th>beta_i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.22-caliber</td>\n",
       "      <td>.22_caliber.a.01</td>\n",
       "      <td>[142479.53, 17125.09]</td>\n",
       "      <td>143505.0</td>\n",
       "      <td>6.85</td>\n",
       "      <td>a</td>\n",
       "      <td>36185</td>\n",
       "      <td>36186</td>\n",
       "      <td>0.5</td>\n",
       "      <td>36185.5</td>\n",
       "      <td>18028.5</td>\n",
       "      <td>18029.5</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>18029.0</td>\n",
       "      <td>7574.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.22-calibre</td>\n",
       "      <td>.22_caliber.a.01</td>\n",
       "      <td>[111617.65, 13415.7]</td>\n",
       "      <td>112421.0</td>\n",
       "      <td>6.85</td>\n",
       "      <td>a</td>\n",
       "      <td>36185</td>\n",
       "      <td>36186</td>\n",
       "      <td>0.5</td>\n",
       "      <td>36185.5</td>\n",
       "      <td>18028.5</td>\n",
       "      <td>18029.5</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>18029.0</td>\n",
       "      <td>38658.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.22_caliber</td>\n",
       "      <td>.22_caliber.a.01</td>\n",
       "      <td>[65612.76, 7886.22]</td>\n",
       "      <td>66085.0</td>\n",
       "      <td>6.85</td>\n",
       "      <td>a</td>\n",
       "      <td>36185</td>\n",
       "      <td>36186</td>\n",
       "      <td>0.5</td>\n",
       "      <td>36185.5</td>\n",
       "      <td>18028.5</td>\n",
       "      <td>18029.5</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>18029.0</td>\n",
       "      <td>84994.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.22_calibre</td>\n",
       "      <td>.22_caliber.a.01</td>\n",
       "      <td>[125342.87, 15065.38]</td>\n",
       "      <td>126245.0</td>\n",
       "      <td>6.85</td>\n",
       "      <td>a</td>\n",
       "      <td>36185</td>\n",
       "      <td>36186</td>\n",
       "      <td>0.5</td>\n",
       "      <td>36185.5</td>\n",
       "      <td>18028.5</td>\n",
       "      <td>18029.5</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>18029.0</td>\n",
       "      <td>24834.59</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.38-caliber</td>\n",
       "      <td>.38_caliber.a.01</td>\n",
       "      <td>[59924.61, 7203.34]</td>\n",
       "      <td>60356.0</td>\n",
       "      <td>6.85</td>\n",
       "      <td>a</td>\n",
       "      <td>36187</td>\n",
       "      <td>36188</td>\n",
       "      <td>0.5</td>\n",
       "      <td>36187.5</td>\n",
       "      <td>18030.5</td>\n",
       "      <td>18031.5</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>18031.0</td>\n",
       "      <td>90723.83</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>.38-calibre</td>\n",
       "      <td>.38_caliber.a.01</td>\n",
       "      <td>[109763.82, 13194.34]</td>\n",
       "      <td>110554.0</td>\n",
       "      <td>6.85</td>\n",
       "      <td>a</td>\n",
       "      <td>36187</td>\n",
       "      <td>36188</td>\n",
       "      <td>0.5</td>\n",
       "      <td>36187.5</td>\n",
       "      <td>18030.5</td>\n",
       "      <td>18031.5</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>18031.0</td>\n",
       "      <td>40525.84</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.38_caliber</td>\n",
       "      <td>.38_caliber.a.01</td>\n",
       "      <td>[70719.89, 8501.0]</td>\n",
       "      <td>71229.0</td>\n",
       "      <td>6.85</td>\n",
       "      <td>a</td>\n",
       "      <td>36187</td>\n",
       "      <td>36188</td>\n",
       "      <td>0.5</td>\n",
       "      <td>36187.5</td>\n",
       "      <td>18030.5</td>\n",
       "      <td>18031.5</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>18031.0</td>\n",
       "      <td>79850.84</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.38_calibre</td>\n",
       "      <td>.38_caliber.a.01</td>\n",
       "      <td>[133435.41, 16039.83]</td>\n",
       "      <td>134396.0</td>\n",
       "      <td>6.85</td>\n",
       "      <td>a</td>\n",
       "      <td>36187</td>\n",
       "      <td>36188</td>\n",
       "      <td>0.5</td>\n",
       "      <td>36187.5</td>\n",
       "      <td>18030.5</td>\n",
       "      <td>18031.5</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>18031.0</td>\n",
       "      <td>16683.84</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.45-caliber</td>\n",
       "      <td>.45_caliber.a.01</td>\n",
       "      <td>[102923.9, 12373.51]</td>\n",
       "      <td>103665.0</td>\n",
       "      <td>6.86</td>\n",
       "      <td>a</td>\n",
       "      <td>36189</td>\n",
       "      <td>36190</td>\n",
       "      <td>0.5</td>\n",
       "      <td>36189.5</td>\n",
       "      <td>18032.5</td>\n",
       "      <td>18033.5</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>18033.0</td>\n",
       "      <td>47415.07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>.45-calibre</td>\n",
       "      <td>.45_caliber.a.01</td>\n",
       "      <td>[68694.37, 8258.44]</td>\n",
       "      <td>69189.0</td>\n",
       "      <td>6.86</td>\n",
       "      <td>a</td>\n",
       "      <td>36189</td>\n",
       "      <td>36190</td>\n",
       "      <td>0.5</td>\n",
       "      <td>36189.5</td>\n",
       "      <td>18032.5</td>\n",
       "      <td>18033.5</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>18033.0</td>\n",
       "      <td>81891.07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word            synset             word_point        l0  alpha pos  \\\n",
       "0  .22-caliber  .22_caliber.a.01  [142479.53, 17125.09]  143505.0   6.85   a   \n",
       "1  .22-calibre  .22_caliber.a.01   [111617.65, 13415.7]  112421.0   6.85   a   \n",
       "2  .22_caliber  .22_caliber.a.01    [65612.76, 7886.22]   66085.0   6.85   a   \n",
       "3  .22_calibre  .22_caliber.a.01  [125342.87, 15065.38]  126245.0   6.85   a   \n",
       "4  .38-caliber  .38_caliber.a.01    [59924.61, 7203.34]   60356.0   6.85   a   \n",
       "5  .38-calibre  .38_caliber.a.01  [109763.82, 13194.34]  110554.0   6.85   a   \n",
       "6  .38_caliber  .38_caliber.a.01     [70719.89, 8501.0]   71229.0   6.85   a   \n",
       "7  .38_calibre  .38_caliber.a.01  [133435.41, 16039.83]  134396.0   6.85   a   \n",
       "8  .45-caliber  .45_caliber.a.01   [102923.9, 12373.51]  103665.0   6.86   a   \n",
       "9  .45-calibre  .45_caliber.a.01    [68694.37, 8258.44]   69189.0   6.86   a   \n",
       "\n",
       "   start    end  radius       cx  mod_start  mod_end         x        y  \\\n",
       "0  36185  36186     0.5  36185.5    18028.5  18029.5  150000.0  18029.0   \n",
       "1  36185  36186     0.5  36185.5    18028.5  18029.5  150000.0  18029.0   \n",
       "2  36185  36186     0.5  36185.5    18028.5  18029.5  150000.0  18029.0   \n",
       "3  36185  36186     0.5  36185.5    18028.5  18029.5  150000.0  18029.0   \n",
       "4  36187  36188     0.5  36187.5    18030.5  18031.5  150000.0  18031.0   \n",
       "5  36187  36188     0.5  36187.5    18030.5  18031.5  150000.0  18031.0   \n",
       "6  36187  36188     0.5  36187.5    18030.5  18031.5  150000.0  18031.0   \n",
       "7  36187  36188     0.5  36187.5    18030.5  18031.5  150000.0  18031.0   \n",
       "8  36189  36190     0.5  36189.5    18032.5  18033.5  150000.0  18033.0   \n",
       "9  36189  36190     0.5  36189.5    18032.5  18033.5  150000.0  18033.0   \n",
       "\n",
       "        l_i beta_i  \n",
       "0   7574.60    0.0  \n",
       "1  38658.60    0.0  \n",
       "2  84994.60    0.0  \n",
       "3  24834.59    0.0  \n",
       "4  90723.83    0.0  \n",
       "5  40525.84    0.0  \n",
       "6  79850.84    0.0  \n",
       "7  16683.84    0.0  \n",
       "8  47415.07    0.0  \n",
       "9  81891.07    0.0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spatial_wordnet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add a column denoting the word-sense coordinates based on [l0, alpha, l_i, beta_i, radius]\n",
    "\n",
    "def decode_params(spatial_params):\n",
    "    l0 = spatial_params[0]\n",
    "    alpha = spatial_params[1]\n",
    "    alpha_rad = alpha * np.pi / 180\n",
    "    l_i = spatial_params[2]\n",
    "    beta_i = spatial_params[3]\n",
    "    beta_i_rad = beta_i * np.pi / 180\n",
    "    r = spatial_params[4]\n",
    "    return l0, alpha, alpha_rad, l_i, beta_i, beta_i_rad, r\n",
    "\n",
    "\n",
    "def point_in_space(spatial_params):\n",
    "    l0, alpha, alpha_rad, l_i, beta_i, beta_i_rad, r = decode_params(spatial_params)\n",
    "    # np.cos() and np.sin() take angles in radian as params\n",
    "    center_pt = np.array([l0*np.cos(alpha_rad), l0 * np.sin(alpha_rad)])\n",
    "    sense_pt = center_pt + np.array([l_i * np.cos(alpha_rad + beta_i_rad),\n",
    "                                     l_i * np.sin(alpha_rad + beta_i_rad)])\n",
    "    return sense_pt, center_pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spatial_wordnet[\"sense_coo\"] = spatial_wordnet.apply(lambda row: point_in_space([row.l0, row.alpha, row.l_i, row.beta_i, row.radius])[0], axis=1)\n",
    "spatial_wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop the rows of verb_root, adjective_root, and adverb_root\n",
    "spatial_wordnet  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arr = list(spatial_wordnet.sense_coo)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ex = np.random.rand(300) \n",
    "ex[0] += arr[0]\n",
    "ex[1] += arr[1]\n",
    "print(ex)\n",
    "len(ex)\n",
    "#normalize\n",
    "no = ex/np.linalg.norm(ex)\n",
    "no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng()\n",
    "numbers = rng.choice(20, size=10, replace=False)\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cpp = list(zip(spatial_wordnet.l0, spatial_wordnet.alpha, spatial_wordnet.l_i, spatial_wordnet.beta_i, spatial_wordnet.radius))\n",
    "type(cpp[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now I need to transform this dataset to a target vocab\n",
    "# How is a target vocab defined?\n",
    "# each word/synset? has a number (it can be the index of in the dataframe)\n",
    "# each of word has a Glove vector\n",
    "# add the point to it\n",
    "# add the sense coordinate\n",
    "# normalize\n",
    "def embed_target_vocab(df, embed_size=300):\n",
    "    \n",
    "    # store (word, synset)-tuples in a list\n",
    "    word_synset = list(zip(df.word, df.synset))\n",
    "    \n",
    "    # store indices in a list to numericalize the vocabulary of wordnet\n",
    "    indices = list(df.index)\n",
    "    \n",
    "    senses_coo = list(df.sense_coo)\n",
    "    \n",
    "    spatial_tags = list(zip(df.l0, df.alpha, df.l_i, df.beta_i, df.radius))\n",
    "    \n",
    "    \n",
    "    # generate weights matrix using glove\n",
    "    matrix_len = len(word_synset)\n",
    "    \n",
    "    weights_matrix = np.zeros((matrix_len, embed_size))\n",
    "    \n",
    "    words_found = 0\n",
    "\n",
    "    for i, word in enumerate(word_synset):\n",
    "        #print(i, word)\n",
    "        try:\n",
    "            weights_matrix[i] = glove[word[0]]\n",
    "            print(\"found GLOVE :) \")\n",
    "            #print(weights_matrix[i])\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            weights_matrix[i] = np.random.normal(scale=0.6, size=(embed_size, ))\n",
    "            print(\"Random Vector\")\n",
    "            #print(weights_matrix[i])\n",
    "    #print(words_found)\n",
    "    \n",
    "    # add the parameters to the sense vector\n",
    "    # add them in a way, that no all of them are in the same part of the 300-dim Glove vector\n",
    "    # Why?\n",
    "    # because this will help learn some patterns in the backpropagation, instead of adding all vectors to the first two columns\n",
    "    # only\n",
    "    # the randomness for choosing the indices to add the coordinates could result in better diversity, and thus better learning\n",
    "    # choose radom integers without replacement\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    x_y_indices = rng.choice(embed_size, size=matrix_len*2, replace=True)\n",
    "    x_col = x_y_indices[:matrix_len]\n",
    "    y_col = x_y_indices[matrix_len:]\n",
    "    \n",
    "    \n",
    "    sense_matrix = np.copy(weights_matrix)\n",
    "    for i, coo in enumerate(senses_coo):\n",
    "        sense_matrix[i][x_col[i]] += coo[0]\n",
    "        sense_matrix[i][y_col[i]] += coo[1]\n",
    "        # normalize\n",
    "        sense_matrix[i] = sense_matrix[i] / np.linalg.norm(sense_matrix[i])\n",
    "    \n",
    "\n",
    "    return weights_matrix, sense_matrix, word_synset, spatial_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# glove_wordnet, sense_matrix, word_synset_VOCAB, spatial_tags = embed_target_vocab(spatial_wordnet, embed_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.save(\"target_sense_matrix_exp_01.npy\", sense_matrix)\n",
    "\n",
    "# # to load:\n",
    "# np.load(\"target_sense_matrix_exp_01.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.save(\"glove_wordnet_matrix_exp_01.npy\", glove_wordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.save(\"WORDNET_VOCAB_exp_01.npy\", word_synset_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.save(\"WORDNET_SPATIAL_TAGS_exp_01.npy\", spatial_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Choosing the argmax sense from softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227733, 300)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsense_matrix = np.load(\"target_sense_matrix_exp_01.npy\")\n",
    "tsense_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227733, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['.22-caliber', '.22_caliber.a.01'],\n",
       "       ['.22-calibre', '.22_caliber.a.01']], dtype='<U76')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_VOCAB = np.load(\"WORDNET_VOCAB_exp_01.npy\")\n",
    "print(target_VOCAB.shape)\n",
    "target_VOCAB[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227733, 5)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPATIAL_TAGS = np.load(\"WORDNET_SPATIAL_TAGS_exp_01.npy\")\n",
    "SPATIAL_TAGS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def inside_sphere(point, sphere_coo):\n",
    "\n",
    "    pt = point_in_space(point)\n",
    "    sphere_sense, sphere_center = point_in_space(sphere_coo)\n",
    "\n",
    "    sphere_rad = sphere_coo[-1] # in angles\n",
    "\n",
    "    contained = (pt[0] - sphere_sense[0])**2 + (pt[1] - sphere_sense[1])**2 <= sphere_rad**2\n",
    "\n",
    "    if contained:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def distance_loss(pred_pt, original_pt, include_r=False):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two sense points, including radii.\n",
    "    :param pred_pt:\n",
    "    :param original_pt:\n",
    "    :param include_r: if set to true, include radius in the distance. \n",
    "                      It gives more freedom/tolerance degrees to the loss function. \n",
    "                      Loss is satisfied once the predicted point is part of original point.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    r1 = pred_pt[-1]\n",
    "    r2 = original_pt[-1]\n",
    "    pred_sense, pred_center = point_in_space(pred_pt)\n",
    "    orig_sense, orig_center = point_in_space(original_pt)\n",
    "    \n",
    "    loss = np.linalg.norm(pred_sense - orig_sense)\n",
    "    tolerant_loss = r1 + loss - r2\n",
    "    if loss < 0:\n",
    "        tolerant_loss = 0\n",
    "    \n",
    "    if include_r:\n",
    "        return tolerant_loss\n",
    "    else:\n",
    "        return loss \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sphere_dist(pred_pt, original_pt):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two 2D spheres.\n",
    "    :param pred_pt:\n",
    "    :param original_pt:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pred_sense, pred_center = point_in_space(pred_pt)\n",
    "    pred_radius = pred_pt[-1]\n",
    "    orig_sense, orig_center = point_in_space(original_pt)\n",
    "    orig_radius = original_pt[-1]\n",
    "\n",
    "    return (pred_radius + orig_radius -\n",
    "            np.linalg.norm(pred_sense - orig_sense))\n",
    "\n",
    "def decode_prediction(spatial_params, df=\"SPATIAL_WORDNET.pickle\") -> [str]:\n",
    "    \"\"\"\n",
    "    Projects the predicted spatial parameters into the embedding space.\n",
    "    Returns the synsets in the vacinity of the projected point.\n",
    "    :param spatial_params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    synsets = [] # sort from specific to most general\n",
    "\n",
    "    sense_pt, center_pt = point_in_space(spatial_params)\n",
    "\n",
    "    spatial_df = pd.read_pickle(df)\n",
    "    # get the spheres, where the point/point+radius is contained/overlaping/near\n",
    "\n",
    "    # 1. check if the predicted point is contained in some sense\n",
    "    spatial_df[\"contained\"] = spatial_df.apply(lambda row:\n",
    "                                               inside_sphere(spatial_params,\n",
    "                                                             row[['l0', 'alpha', 'l_i', 'beta_i', 'radius']]))\n",
    "\n",
    "    # 2. For those synsets, which is the nearest synset point\n",
    "    #use distance() to calculate distance between centers\n",
    "\n",
    "    # 3. If None of the synsets apply to that word sense\n",
    "    # use sphere_dist to find the nearest sphere (most general synset), and assign it to that synset\n",
    "    # (this maybe good for rare senses)\n",
    "\n",
    "\n",
    "    return synsets\n",
    "\n",
    "def train_loss(tmp_pred, synset_params):\n",
    "    # Loss is the distance between the two spheres/containment of the word within that sphere\n",
    "    # radius acts as tolerance!\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    weights_matrix = torch.from_numpy(weights_matrix)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# emb, n, d = create_emb_layer(weights_matrix)\n",
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokens = torch.tensor([0,5,9], dtype=torch.long)\n",
    "# len(emb(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: np.ndarray, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self, weights_matrix:np.ndarray, \n",
    "                 ntoken: int, out_features:int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.weights_matrix = weights_matrix\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(self.weights_matrix, True)\n",
    "        \n",
    "        # Multi-head attention mechanism is included in TransformerEncoderLayer\n",
    "        # d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=<function relu>, \n",
    "        # layer_norm_eps=1e-05, batch_first=False, norm_first=False, device=None, dtype=None\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout) # activation\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers, norm=None)\n",
    "        \n",
    "        \n",
    "#         padding_idx (int, optional)  If specified, the entries at padding_idx do not contribute to the gradient;\n",
    "#         therefore, the embedding vector at padding_idx is not updated during training,\n",
    "#         i.e. it remains as a fixed pad. For a newly constructed Embedding, the embedding vector at\n",
    "#         padding_idx will default to all zeros, but can be updated to another value to be used as the padding vector.\n",
    "        self.emb = nn.Embedding(ntoken, d_model) \n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Linear layer: returns the last hidden state of the encoder \n",
    "        self.fc = nn.Linear(d_model, embedding_dim)\n",
    "        \n",
    "        # No! Here I am just redoing fully connected connections\n",
    "        # Linear Layer: affine transformation of last hidden layer into shape (1, embedding_dim)\n",
    "        #self.context_vec = nn.Linear(d_model, embedding_dim)\n",
    "        \n",
    "        #self.decoder = nn.Linear(d_model, ntoken)\n",
    "        \n",
    "        # Now, I need to have a Linear space that takes the whole/subset dataframe as input, extracts its spatial_context_vec,\n",
    "        # based on Glove-word-vector + spatial_point,\n",
    "        # then calculates softmax on this distribution\n",
    "        # choose the argmax\n",
    "        # get its spatial tags\n",
    "        # calculate distance loss between them\n",
    "        # do backprop! \n",
    "        # Nx300 into Nx227733: matmul product of two matrices Nx300 and 300x227733 --> Nx227733\n",
    "        # apply softmax to get the probabilities\n",
    "        # apply argmax to get the maximum indices\n",
    "        # use the indices to get the synset names as well as the mapping to coordinates\n",
    "        # into Nx5: mapping to the coordinates\n",
    "        \n",
    "        self.output = nn.Linear(embedding_dim, 5)\n",
    "        #self.wn_embeddings = nn.Linear(1, target_matrix.shape[0])\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "#         weights_matrix = weights_matrix, \n",
    "#                                     ntoken= # false: 300,\n",
    "#                                     out_features=5,\n",
    "#                                     d_model=300,\n",
    "#                                     d_hid=200,\n",
    "#                                     nlayers=2,\n",
    "#                                     nhead=2,\n",
    "#                                     dropout=0.2\n",
    "        \n",
    "        \n",
    "        # -------------------------------------\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        \"initialize weights using uniform distribution\"\n",
    "        initrange = 0.1\n",
    "        self.emb.weight.data.uniform_(-initrange, initrange)\n",
    "        # self.decoder.bias.data.zero_()\n",
    "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        #self.output.bias.data.zero_()\n",
    "        #self.output.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        \n",
    "        #src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = torch.mul(self.emb(src), math.sqrt(self.d_model)) #? 1/sqrt!\n",
    "#         print(\"Embedding\", src.shape)\n",
    "#         print('-' * 80)\n",
    "        \n",
    "        \n",
    "        src = self.pos_encoder(src)\n",
    "#         print(\"Positional Encoding\", src.shape)\n",
    "#         print('-' * 80)\n",
    "        \n",
    "        \n",
    "        encoder_output = self.transformer_encoder(src) #, src_mask)\n",
    "#         print(\"Encoder\", encoder_output.shape)\n",
    "#         # print(encoder_output)\n",
    "#         print('-' * 80)\n",
    "        \n",
    "        \n",
    "        linear_layer = self.fc(encoder_output)\n",
    "#         print(\"Linear Layer\", linear_layer.shape)\n",
    "#         # print(linear_layer)\n",
    "#         print('-' * 80)\n",
    "\n",
    "        # calculate the sum/weighted sum/ ?? on the linear layer to get the context vector of size (1, embd_dim)\n",
    "        context_vec = torch.sum(linear_layer, dim=1)\n",
    "#         print(\"Final Context Vector\", context_vec.shape)\n",
    "#         # print(context_vec)\n",
    "#         print('-' * 80)\n",
    "        \n",
    "        # regression output\n",
    "        coordinates = self.output(context_vec)\n",
    "#         print(\"Coordinates from Context Vector\", coordinates.size())\n",
    "#         # print(coordinates)\n",
    "#         print('-'*80)\n",
    "        return coordinates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Geometric Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def coo2point(coo):\n",
    "    # print(coo)\n",
    "    l0 = coo[0]\n",
    "    alpha = coo[1]\n",
    "    alpha_rad = alpha * math.pi / 180\n",
    "    l_i = coo[2]\n",
    "    beta_i = coo[3]\n",
    "    beta_i_rad = beta_i * math.pi / 180\n",
    "    r = coo[4]\n",
    "    \n",
    "    # np.cos() and np.sin() take angles in radian as params\n",
    "    center_pt = torch.tensor([l0 * math.cos(alpha_rad), l0 * math.sin(alpha_rad)], dtype=torch.float64, requires_grad=True)\n",
    "    sense_pt = center_pt + torch.tensor([l_i * math.cos(alpha_rad + beta_i_rad),\n",
    "                                     l_i * math.sin(alpha_rad + beta_i_rad)], dtype=torch.float64, requires_grad=True)\n",
    "    return sense_pt, center_pt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def distance_loss(pred_pt, original_pt, include_r=False, pt_sphere=False):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two sense points, including radii.\n",
    "    :param pred_pt:\n",
    "    :param original_pt:\n",
    "    :param include_r: if set to true, include radius in the distance. \n",
    "                      It gives more freedom/tolerance degrees to the loss function. \n",
    "                      Loss is satisfied once the predicted point is part of original point.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "        \n",
    "    # original_pt = torch.from_numpy(original_pt)\n",
    "    # print(\"original point\", type(original_pt), original_pt)\n",
    "    \n",
    "    r1 = pred_pt[-1]\n",
    "    r2 = original_pt[-1]\n",
    "\n",
    "    pred_sense, pred_center = coo2point(pred_pt)\n",
    "    orig_sense, orig_center = coo2point(original_pt)\n",
    "    \n",
    "    \n",
    "    loss = torch.linalg.norm(torch.sub(pred_sense, orig_sense)) - r2\n",
    "    \n",
    "    # very strong assumption for the words that are not sense-tagged\n",
    "    # If I want more tolerance, I could neglect those tokens from the beginning\n",
    "    if torch.all(torch.eq(original_pt, torch.zeros(original_pt.size(0))), dim=0):\n",
    "        return loss\n",
    "    \n",
    "    if pt_sphere:\n",
    "        dist = torch.linalg.norm(torch.sub(pred_sense, orig_sense)) + r2\n",
    "        return dist\n",
    "\n",
    "    \n",
    "    if include_r:\n",
    "        \n",
    "        tolerant_loss = r1 + loss - r2\n",
    "    \n",
    "        if tolerant_loss < 0:\n",
    "            tolerant_loss = 0.0\n",
    "        \n",
    "#         if r1 > r2: #case the predicted radius is bigger than actual one\n",
    "#             tolerant_loss = torch.abs(torch.sub(r1, r2))\n",
    "           \n",
    "        return tolerant_loss\n",
    "    \n",
    "    else:\n",
    "        return loss \n",
    "   \n",
    "\n",
    "\n",
    "def geometric_loss(pred_list, label_list, include_r=False):\n",
    "    \n",
    "    # assert that the two lists must be of equal size\n",
    "    pred_size = pred_list.size()[0]\n",
    "    lab_size = label_list.size()[0]\n",
    "    assert pred_size == lab_size\n",
    "    \n",
    "    sentence_loss = 0.0\n",
    "    \n",
    "    # sum over all the tokens in the sentence\n",
    "    for i in range(pred_size):\n",
    "        sentence_loss += distance_loss(pred_list[i], label_list[i], include_r)\n",
    "        \n",
    "    return sentence_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sense Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_contained(pred, sphere_coo, compare_spheres=False):\n",
    "\n",
    "    pt, word = coo2point(pred)\n",
    "    sphere_sense, sphere_center = coo2point(sphere_coo)\n",
    "\n",
    "    pt_rad = pred[-1]\n",
    "    sphere_rad = sphere_coo[-1] # in angles\n",
    "    \n",
    "    \n",
    "    \n",
    "    if compare_spheres == False:\n",
    "        contained = (pt[0] - sphere_sense[0])**2 + (pt[1] - sphere_sense[1])**2 <= sphere_rad**2\n",
    "    else:\n",
    "        contained = pt_rad + torch.linalg.norm(pt - sphere_sense) - sphere_rad <= 0\n",
    "\n",
    "    if contained:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "def vicinity_matrix(spatial_params, target_vocab: np.ndarray, spatial_tags: np.ndarray, k=5):#, include_sphere=True, include_r=True) -> [str]:\n",
    "    \"\"\"\n",
    "    Projects the predicted spatial parameters into the embedding space.\n",
    "    Returns the synsets in the vicinity of the projected point.\n",
    "    :param spatial_params:\n",
    "    :return: Vicinity matrix, synsets dict\n",
    "    \"\"\"\n",
    "    N = len(spatial_tags)\n",
    "    \n",
    "    #convert spatial_tags to tensor\n",
    "    spatial_tags = torch.from_numpy(spatial_tags)\n",
    "    \n",
    "    synsets = {} # sort from most specific to most general\n",
    "    \n",
    "    indices = {}\n",
    "\n",
    "    sense_pt, center_pt = coo2point(spatial_params)\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------------------------\n",
    "    # Prepare distance and containment calculations\n",
    "    # ----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # distance calculations\n",
    "    dist_spheres = torch.empty(N) \n",
    "    dist_pt_sphere = torch.empty(N) \n",
    "    dist_pts = torch.empty(N)\n",
    "    \n",
    "    for i, tag in enumerate(spatial_tags):\n",
    "        dist_spheres[i] = distance_loss(spatial_params, tag, include_r=True)\n",
    "        dist_pt_sphere[i] = distance_loss(spatial_params, tag, pt_sphere=True)\n",
    "        dist_pts[i] = distance_loss(spatial_params, tag, include_r=False)\n",
    "    \n",
    "    # containment calculations\n",
    "    full_contained = torch.empty(N) \n",
    "    part_contained = torch.empty(N)\n",
    "    disconnected = torch.empty(N) # handles points only\n",
    "    \n",
    "    for j, tag in enumerate(spatial_tags):\n",
    "        full_contained[j] = is_contained(spatial_params, tag, compare_spheres=True)\n",
    "        part_contained[j] = distance_loss(spatial_params, tag, include_r=True) > 0\n",
    "        disconnected[j] = ~ is_contained(spatial_params, tag, compare_spheres=True) # reverse the True <----> False\n",
    "    \n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Initialize the Vicinity Matrix\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    # row=3, col=3, topk=2, 2 indicates the column of indices and the distances\n",
    "    vicinity_matrix = torch.zeros((3,3, k, 2))\n",
    "    \n",
    "    ####################################################################################################################\n",
    "    # # Full contained + min dist between sense points\n",
    "    ####################################################################################################################\n",
    "    \n",
    "#     print(\"True elements\")\n",
    "    true_indices1 = (full_contained == True).nonzero(as_tuple=True)[0]\n",
    "#     print(true_indices1)\n",
    "    \n",
    "    if true_indices1.size(0) != 0:\n",
    "        dist1 = torch.index_select(dist_pts, 0, true_indices1)\n",
    "#         print(\"dist1\", dist1)\n",
    "#         print(\"k = \", k)\n",
    "        # sort in ascending order\n",
    "        # select top k \n",
    "        sort_dist1, sort_indices = torch.topk(dist1, k, largest=False)  \n",
    "#         print(\"SORTING\", sort_dist1, sort_indices)\n",
    "        synsets1 = np.take(target_vocab, sort_indices, 0)\n",
    "        synsets[\"A\"] = [synsets1, sort_dist1]\n",
    "        indices[\"A\"] = sort_indices\n",
    "        # index, distance (without synsets because this would result in conflicts for torch.tensor that do not support str)\n",
    "        vicinity_matrix[2][0] = torch.stack((sort_indices, sort_dist1), dim=1)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    ####################################################################################################################\n",
    "    # # Partially contained + min dist between sense points\n",
    "    ####################################################################################################################\n",
    "    true_indices2 = (part_contained == True).nonzero(as_tuple=True)[0]\n",
    "#     print(\"True Indices 2\", true_indices2)\n",
    "    \n",
    "    if true_indices2.size(0) != 0:\n",
    "        dist1 = torch.index_select(dist_pts, 0, true_indices2)\n",
    "        # sort in ascending order\n",
    "        # select top k \n",
    "        sort_dist2, sort_indices2 = torch.topk(dist1, k, largest=False)     \n",
    "        synsets2 = np.take(target_vocab, sort_indices2, 0)\n",
    "#         print(\"synset 2\", synsets2)\n",
    "        synsets[\"B\"] = [synsets2, sort_dist2]\n",
    "        indices[\"B\"] = sort_indices2\n",
    "        # index, distance (without synsets because this would result in conflicts for torch.tensor that do not support str)\n",
    "        vicinity_matrix[2][1] = torch.stack((sort_indices2, sort_dist2), dim=1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # # Disconnected + min dist between spheres/point2sphere/sense points ---> acts as Nearest neighbor\n",
    "    ####################################################################################################################\n",
    "    # get indices, where disconnected is true\n",
    "    true_indices3 = (disconnected == True).nonzero(as_tuple=True)[0]\n",
    "#     print(\"True Indices 3\", true_indices3)\n",
    "\n",
    "    if true_indices3.size(0) != 0:\n",
    "        # get the distances at those indices\n",
    "        dist_spheres3 = torch.index_select(dist_spheres, 0, true_indices3)\n",
    "        dist_pt_sphere3 = torch.index_select(dist_pt_sphere, 0, true_indices3)\n",
    "        dist_pts3 = torch.index_select(dist_pts, 0, true_indices3)\n",
    "\n",
    "        # sort-select top k minimum distances\n",
    "        sort_dist_spheres3, sort_sph_indices3 = torch.topk(dist_spheres3, k, largest=False)\n",
    "        sort_dist_pt_sphere3, sort_pt_sph_indices3 = torch.topk(dist_pt_sphere3, k, largest=False)\n",
    "        sort_dist_pts3, sort_pts_indices3 = torch.topk(dist_pts3, k, largest=False)\n",
    "\n",
    "        # get their corresponding synsets\n",
    "        synsets30 = np.take(target_vocab, sort_sph_indices3, 0)\n",
    "        #print(\"synset30\", synsets30)\n",
    "        synsets[\"C\"] = [synsets30, sort_dist_spheres3]\n",
    "        indices[\"C\"] = sort_sph_indices3\n",
    "        \n",
    "        synsets31 = np.take(target_vocab, sort_pt_sph_indices3, 0)\n",
    "        synsets[\"D\"] = [synsets31, sort_dist_pt_sphere3]\n",
    "        indices[\"D\"] = sort_pt_sph_indices3\n",
    "        \n",
    "        synsets32 = np.take(target_vocab, sort_pts_indices3, 0)\n",
    "        synsets[\"E\"] = [synsets32, sort_dist_pts3]\n",
    "        indices[\"E\"] = sort_pts_indices3\n",
    "        \n",
    "        # insert them into the vicinity matrix    \n",
    "        vicinity_matrix[0][3] = torch.stack((sort_sph_indices3, sort_dist_spheres3), dim=1)\n",
    "        vicinity_matrix[1][3] = torch.stack((sort_pt_sph_indices3, sort_dist_pt_sphere3), dim=1)\n",
    "        vicinity_matrix[2][3] = torch.stack((sort_pts_indices3, sort_dist_pts3), dim=1)  \n",
    "    \n",
    "\n",
    "\n",
    "#     # get the spheres, where the point/point+radius is contained/overlaping/near\n",
    "\n",
    "#     # 1. check if the predicted point is contained in some sense\n",
    "#     contained = torch.empty(N)\n",
    "    \n",
    "#     for i, tag in enumerate(spatial_tags):\n",
    "#         contained[i] = is_contained(spatial_params, tag, compare_spheres=include_sphere)\n",
    "    \n",
    "#     # 2. For those synsets, which is the nearest synset point\n",
    "#     #use distance() to calculate distance between centers\n",
    "#     distances = torch.empty(N)\n",
    "#     for i, tag in enumerate(spatial_tags):\n",
    "#         distances[i] = distance_loss(spatial_params, tag, include_r=include_r)\n",
    "    \n",
    "#     # sort dist--> indices\n",
    "#     # check if for those distances the containment is true\n",
    "#     # if true: choose the one having min_dist as sense\n",
    "#     # top k senses must be stored in a dict \n",
    "    \n",
    "#     # check if for those distances the containment is false, then, only the radius is falsly predicted (not priority now)\n",
    "#     # if false and min_dist: choose it as potential sense\n",
    "    \n",
    "    \n",
    "\n",
    "#     # 3. If None of the synsets apply to that word sense\n",
    "#     # use sphere_dist to find the nearest sphere (most general synset), and assign it to that synset\n",
    "#     # (this maybe good for rare senses)\n",
    "#     # acts as a second chance\n",
    "#     rare_contained = torch.empty(N)\n",
    "#     rare_distances = torch.empty(N)\n",
    "#     for i, tag in enumerate(spatial_tags):\n",
    "#         rare_contained[i] = is_contained(spatial_params, tag, compare_spheres=False) #only consider sense point\n",
    "#         rare_distances[i] = distance_loss(spatial_params, tag, include_r=False)\n",
    "\n",
    "\n",
    "    return indices, vicinity_matrix, synsets\n",
    "\n",
    "def decode_key(key, mtx):\n",
    "    if key == \"A\":\n",
    "        return mtx[2, 0]\n",
    "    if key == \"B\":\n",
    "        return mtx[2, 1]\n",
    "    if key == \"C\":\n",
    "        return mtx[0, 2]\n",
    "    if key == \"D\":\n",
    "        return mtx[1, 2]\n",
    "    if key == \"E\":\n",
    "        return mtx[2, 2]\n",
    "    \n",
    "\n",
    "def label_in_vicinity(vicinity_matrix, vicinity_synsets, target_vocab, spatial_tags, true_label):\n",
    "    \n",
    "    checked_synsets = []\n",
    "    contained = []\n",
    "    checks = 0\n",
    "    predicted = []\n",
    "    distances = []\n",
    "    \n",
    "    in_vicinity = False\n",
    "    associated_syn = []\n",
    "    \n",
    "    # true label is either one of the possibilities [word, synset] or a randomly chosen one\n",
    "    \n",
    "    # induce subset of word-synset name \n",
    "    \n",
    "    #spatial_tags = torch.from_numpy(spatial_tags)\n",
    "    #idx_label = (spatial_tags == true_label).nonzero(as_tuple=True)[0]\n",
    "    # transform to numpy to \n",
    "    true_label = np.array(true_label, dtype=np.float64)\n",
    "    # keep spatial tag an np.ndarray\n",
    "    rounded_l = np.round(true_label, decimals=2)\n",
    "    \n",
    "    if np.all(rounded_l == np.zeros(5)): #true_label): #torch.all(torch.eq(rounded_l, true_label)):\n",
    "        in_vicinity = False #True\n",
    "        associated_syn.append('no-synset')\n",
    "        return in_vicinity, associated_syn\n",
    "    \n",
    "    try:\n",
    "        # detecting the true label from the spatial_tags\n",
    "        idx = [[np.array_equal(rounded_l, tag) for tag in spatial_tags].index(True)]\n",
    "#         print(\"Found {} matching word-synset tags.\".format(len(idx)))\n",
    "        word_synset = target_vocab[idx] #list of list \n",
    "#         print(\"Matching word-synset\", word_synset)\n",
    "        # check if word_synset is within the vicinity matrix\n",
    "        if len(word_synset) != 0:\n",
    "            for e in word_synset:\n",
    "                for key, val in vicinity_synsets.items():\n",
    "#                     print(\"Searching in vicinity ... \")\n",
    "\n",
    "#                     print(\"Checking if true label is in vicinity ...\")\n",
    "                    checked_synsets.append(e)\n",
    "                    is_there = e[1] in val[:, 1]\n",
    "                    checks += 1\n",
    "                    contained.append(is_there)\n",
    "                    \n",
    "#                     print(\"1\")\n",
    "#                     print(checked_synsets)\n",
    "#                     print(checks)\n",
    "#                     print(contained)\n",
    "                    \n",
    "                    if is_there:\n",
    "#                         print(\"The main true label <{}> is in the vacinity of the predicted tag.\".format(e))\n",
    "                        idx_e = np.where(val[:, 1] == e[1])\n",
    "                        predicted.append(val[idx_e])\n",
    "#                         print(\"Predicted 1: \", predicted)\n",
    "                        distances.append(decode_key(key, vicinity_matrix)[idx_e][1])\n",
    "#                         print(\"Distances 1: \", distances)\n",
    "                    else:\n",
    "#                         print(\"The main true label is not in vicinity ... \")\n",
    "                        distances.append('no-distance')\n",
    "#                         print(\"Searching if alternative true label synsets are in vicinity ... \")\n",
    "                    # induce all the word-synset tuples that have same synset as true label.\n",
    "                    # This double check is necessary since I choose the spatial tags in the training data randomly sometimes.\n",
    "                    # get indices of all word-synsets sharing same synset (not same word)\n",
    "                    ix = np.where(target_vocab == [_, e[1]])[0] # add [0] to indicate only the row index, not the column\n",
    "#                     print(\"Indices \", ix)\n",
    "                    if len(ix) != 0:\n",
    "                        pos_syn = target_vocab[ix]\n",
    "                        \n",
    "#                         print(\"Possible synsets: \", pos_syn)\n",
    "#                         print(target_vocab[:10])\n",
    "                        for t in pos_syn:\n",
    "                            checks += 1\n",
    "                            checked_synsets.append(t)\n",
    "                            is_near = t[1] in val[:, -1]\n",
    "                            contained.append(is_near)\n",
    "#                             print(\"2\")\n",
    "#                             print(checked_synsets)\n",
    "#                             print(checks)\n",
    "#                             print(contained)\n",
    "                            if is_near == True:                                    \n",
    "#                                 print(\"... The word-synset <{}> is in the vicinity of the predicted tag.\".format(t))\n",
    "                                idx_t = np.where(val[:, -1] == t[1])\n",
    "                                predicted.append(val[idx_t])\n",
    "#                                 print(\"Predicted 2: \", predicted)\n",
    "                                distances.append(decode_key(key, vicinity_matrix)[idx_t][1])\n",
    "#                                 print(\"Distances 2: \", distances)\n",
    "                            else:\n",
    "                                distances.append('no-distance')\n",
    "                    else: \n",
    "                        print(\"... There are no other possibilites for word-synset <{}>\".format(e))\n",
    "                            \n",
    "        else:\n",
    "            print(\"Cannot find the suitable synset of this spatial tag!\")\n",
    "\n",
    "        \n",
    "    except ValueError as ve:\n",
    "        print(ve)\n",
    "#         print(\"Found no index for the true label. Something went wrong ...\")\n",
    "#         print(\"Comparing <true label = {}> with <rounded label = {}>\".format(true_label, rounded_l))\n",
    "    \n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Statistics\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "#     print(\"~\" * 80)\n",
    "#     print(\"Statistics\")\n",
    "#     print(\"~\" * 80)\n",
    "    \n",
    "#     print(\"Predicted Spatial Tag = \", spatial_params)\n",
    "#     print(\"Checked Spatial Tag(s) ; contained? ; Predicted ; distances = ({}):\".format(len(checked_synsets)))\n",
    "    for s, c, p, d in zip(checked_synsets, contained, predicted, distances):\n",
    "        print(s, \";\", c, \";\", \"\\n\", p, \";\", d)\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "#     print(\"True Spatial Tag(s) is in vicinity of predicted tag: \", contained)\n",
    "    contained_idx = np.where(np.array(contained) == True)\n",
    "    \n",
    "#     print(\"contained_idx\", contained_idx)\n",
    "#     print(\"checked_idx\", np.array(checked_synsets)[contained_idx])\n",
    "#     print(\"slice\", np.array(checked_synsets)[:, 1])\n",
    "#     print(\"check_slice\", np.array(checked_synsets)[:, 1][contained_idx])\n",
    "\n",
    "    if len(contained_idx[0]) > 0:\n",
    "#         print()\n",
    "#         print(contained_idx)\n",
    "        only_syn = set(np.array(checked_synsets)[contained_idx])#[:, 1])\n",
    "        associated_syn.append(only_syn)\n",
    "#         print(\"True Sense Tag(s) = ({}) --> \".format(len(only_syn)), only_syn)\n",
    "#         print(\"Prediction is correct!\")\n",
    "        in_vicinity = True\n",
    "#         print(\"Distance(predicted_sense, nearest_true_sense) = ({}): \".format(len(np.array(predicted)[contained_idx])))\n",
    "#         for p, d in zip(np.array(predicted), distances):\n",
    "#               print(p, d)\n",
    "              \n",
    "    else:\n",
    "#         print(\"Prediction is false ..\")\n",
    "#         print(\"All synsets in the vicinity of the predicted tag are not true senses ..\")\n",
    "#         print(\"Please check manually if the synsets in the vicinity are generalizations of the true labels.\")\n",
    "        in_vicinity = False\n",
    "        associated_syn.append(\"no-synset\")\n",
    "    \n",
    "    \n",
    "    return in_vicinity, associated_syn\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s=np.array([1,4,2,8,3,2])\n",
    "contained_idx = np.where(s == 9)\n",
    "print(type(contained_idx), contained_idx[0])\n",
    "print(len(contained_idx[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    'Counts the parameters of the model to allow comparision between different models.'\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def spatial_stats(pred, original, zeros_tags=True):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 6. Training/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I need to split all training data beforehand\n",
    "\n",
    "class RegTagger:\n",
    "    \n",
    "    def __init__(self, use_cuda, device):\n",
    "        self.use_cuda = use_cuda\n",
    "        self.device = device\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "        \n",
    "    def train(self, batch_size: int, num_workers: int, max_epochs: int, \n",
    "              splittings: dict, labels: list, train_val_syn: list, data: list, embed_size: int,\n",
    "              target_vocab: list, spatial_tags: list,\n",
    "              k=5,\n",
    "              d_model=300, d_hid=200, nlayers=2, nhead=2, dropout=0.2,\n",
    "              lr=5.0, gamma=0.95,\n",
    "              shuffle=True):\n",
    "        \n",
    "        # create batches\n",
    "        \n",
    "        # parameters\n",
    "        params = {'batch_size': batch_size, #64,\n",
    "                  'shuffle': shuffle,\n",
    "                  'collate_fn': lambda x: x,\n",
    "                  'num_workers': num_workers} #6} #set 0 if training on Windows machine\n",
    "\n",
    "        # Training and validation data generators\n",
    "        training_set = Dataset(splittings['train'], labels, train_val_syn)\n",
    "        training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "\n",
    "        validation_set = Dataset(splittings['validate'], labels, train_val_syn)\n",
    "        validation_generator = torch.utils.data.DataLoader(validation_set, **params)\n",
    "\n",
    "        # -------------------------------------------------\n",
    "    \n",
    "        # history to store the losses\n",
    "        history = defaultdict(list)\n",
    "\n",
    "        VOCAB, weights_matrix = load_vocab(data, embed_size=embed_size)\n",
    "\n",
    "        # target_VOCAB\n",
    "        # SPATIAL_TAGS\n",
    "\n",
    "    \n",
    "\n",
    "        #######################################################################################################################\n",
    "        #        Count sentences and number of words in training and validation datasets to normalize the loss\n",
    "        #######################################################################################################################\n",
    "        nb_words_training = 0\n",
    "        nb_train_sentences = 0\n",
    "        nb_words_validation = 0\n",
    "\n",
    "        for batch in training_generator:\n",
    "            for sentence, label, syn in batch:\n",
    "                nb_train_sentences += 1\n",
    "                nb_words_training += len(sentence)\n",
    "\n",
    "        for batch in validation_generator:\n",
    "            for sentence, label, syn in batch:\n",
    "                nb_words_validation += len(sentence)\n",
    "\n",
    "#         print(\"Count results:\")\n",
    "#         print(\"nb_words_training = {}\".format(nb_words_training))\n",
    "#         print(\"nb_train_sentences = {}\".format(nb_train_sentences))\n",
    "#         print(\"nb_words_validation = {}\".format(nb_words_validation))\n",
    "\n",
    "#         print(params[\"batch_size\"])\n",
    "        n_batches = np.ceil(nb_train_sentences / batch_size)\n",
    "#         print(\"ceiling\", n_batches)\n",
    "\n",
    "        mean_words = nb_words_training / n_batches\n",
    "#         print(\"mean_words\", mean_words)\n",
    "\n",
    "\n",
    "        self.model = TransformerEncoderRegressor(weights_matrix = weights_matrix, \n",
    "                                            ntoken= len(VOCAB), #300,\n",
    "                                            out_features=5,\n",
    "                                            d_model=d_model,\n",
    "                                            d_hid=d_hid,\n",
    "                                            nlayers=nlayers,\n",
    "                                            nhead=nhead,\n",
    "                                            dropout=dropout)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        #                       Optimizer\n",
    "        # ---------------------------------------------------------------------\n",
    "        # criterion = nn.CrossEntropyLoss()\n",
    "        criterion = nn.MSELoss()\n",
    "#             lr = 5.0  # learning rate\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=gamma)\n",
    "        # -------\n",
    "\n",
    "\n",
    "        # Loop over epochs\n",
    "        for epoch in range(max_epochs):\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            loss_sum = 0\n",
    "\n",
    "            self.model.train()\n",
    "\n",
    "            # for transformer\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "            print(\"Training ...\")\n",
    "            # Training\n",
    "            for batch in training_generator:\n",
    "#                 print(\"New Batch for Training\")\n",
    "#                 print(\"#\" * 100)\n",
    "\n",
    "                for local_batch, local_labels, local_synsets in batch:\n",
    "\n",
    "                    # Transform list(<string>) to Tensor(<Tensor>)\n",
    "#                     print(\"Input Sentence:\")\n",
    "#                     print(local_batch)\n",
    "                    input_words = local_batch\n",
    "                    local_batch = numericalize(local_batch, VOCAB)\n",
    "#                     print(type(local_batch), local_batch)\n",
    "\n",
    "\n",
    "                    # Transform List(<Tensor>) to Tensor(<Tensor>)\n",
    "                    # I have labels of same length --> this should be no problem for Tensor\n",
    "                    local_labels = torch.stack(local_labels)\n",
    "#                     print(\"Labels:\")\n",
    "#                     print(local_synsets)\n",
    "#                     print(type(local_labels), len(local_labels), type(local_labels[0]))\n",
    "#                     print(local_labels)\n",
    "\n",
    "                    # Transfer to GPU\n",
    "                    local_batch, local_labels = local_batch.to(self.device), local_labels.to(self.device)\n",
    "\n",
    "                    # Model computations\n",
    "                    # out outputs the indices of wordnet database\n",
    "                    out = self.model(local_batch)\n",
    "#                     print(\"Model's Output\")\n",
    "#                     print(type(out), out.shape)\n",
    "                    # print(out)\n",
    "                    # predicted synsets\n",
    "#                     print(\"Current Predictions based on vacinity of prediction\")\n",
    "#                     print(\"*\" * 100)\n",
    "#                     print(\"*\" * 100)\n",
    "\n",
    "\n",
    "                    # ntokens = len(VOCAB)#300\n",
    "                    loss = geometric_loss(out, local_labels) / mean_words\n",
    "                    # criterion(out.view(-1), local_labels.view(-1))\n",
    "#                     print(\"Loss\")\n",
    "#                     print(type(loss), loss.size())\n",
    "#                     print(loss)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    # I added this\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "                    # ---\n",
    "                    optimizer.step()\n",
    "                    loss_sum += loss.item()\n",
    "#                     print(\"Loss Sum\", loss_sum)\n",
    "\n",
    "\n",
    "                    train_loss = loss_sum / len(local_batch)\n",
    "                    history['train_loss'].append(train_loss)\n",
    "#                     print(history)\n",
    "#                     print(len(history['train_loss']))\n",
    "\n",
    "\n",
    "            # Evaluate on the validation set.\n",
    "            # evaluate every 1 step:\n",
    "\n",
    "            print(\"Validation ...\")\n",
    "            vloss_sum = 0\n",
    "            if epoch % 1 == 0:\n",
    "\n",
    "                correct_sense = 0\n",
    "                sense_accuracy = 0\n",
    "\n",
    "                # set model to eval mode to ignore updating the weights of the model\n",
    "                self.model.eval()\n",
    "\n",
    "                # do not calculate gradients while evaluating\n",
    "                with torch.set_grad_enabled(False):\n",
    "\n",
    "                    for batch in validation_generator:\n",
    "                        print(\"New Batch for Validation\")\n",
    "                        print(\"#\" * 100)\n",
    "\n",
    "                        for local_batch, local_labels, local_synsets in batch:\n",
    "\n",
    "                            # Transform list(<string>) to Tensor(<Tensor>)\n",
    "                            print(\"Input Sentence\")\n",
    "                            print(local_batch)\n",
    "                            input_words = local_batch\n",
    "                            local_batch = numericalize(local_batch, VOCAB)\n",
    "        #                     print(type(local_batch), local_batch)\n",
    "\n",
    "\n",
    "                            # Transform List(<Tensor>) to Tensor(<Tensor>)\n",
    "                            # I have labels of same length --> this should be no problem for Tensor\n",
    "                            local_labels = torch.stack(local_labels)\n",
    "                            print(\"Labels:\")\n",
    "                            print(local_synsets)\n",
    "        #                     print(\"Labels\")\n",
    "        #                     print(type(local_labels), len(local_labels), type(local_labels[0]))\n",
    "        #                     print(local_labels)\n",
    "\n",
    "                            # Transfer to GPU\n",
    "                            local_batch, local_labels = local_batch.to(self.device), local_labels.to(self.device)\n",
    "\n",
    "                            # Model computations\n",
    "                            # out outputs the indices of wordnet database\n",
    "                            out = self.model(local_batch)\n",
    "\n",
    "                            # During validation and testing, I want to be less strict.\n",
    "                            # So, if a point resides within the label sphere, the sense is correctly identified.\n",
    "                            loss = geometric_loss(out, local_labels, include_r=True)\n",
    "\n",
    "                            vloss_sum += loss.item()                  \n",
    "\n",
    "                            validation_loss = vloss_sum / len(local_batch)\n",
    "                            history['validation_loss'].append(validation_loss)\n",
    "#                             print(history)\n",
    "#                             print(len(history['validation_loss']))\n",
    "\n",
    "                            correct_sense_batch = 0\n",
    "#                             print(\"Initializing the corrext sense batch = {}\".format(correct_sense_batch))\n",
    "\n",
    "                            true_pred = []\n",
    "                            predicted_synsets = []\n",
    "\n",
    "                            for i, word_tag in enumerate(out):\n",
    "#                                 print(\"i = \", i)\n",
    "#                                 print(\"+\"*150)\n",
    "#                                 print(\"word_tag = \", word_tag.size())\n",
    "#                                 print(word_tag)\n",
    "#                                 print(\"+\"*150)\n",
    "\n",
    "                                vindices, vmat, vsyn = vicinity_matrix(spatial_params=word_tag,\n",
    "                                                               target_vocab=target_vocab[:100],\n",
    "                                                               spatial_tags=spatial_tags[:100], k=k)\n",
    "#                                 print(\"Vicinity Matrix-Synsets: {}\".format(vsyn))\n",
    "                                print(\"Vicinity indices: {}\".format(vindices))\n",
    "                                print(\"Vicinity synsets: {}\".format(vsyn))\n",
    "\n",
    "\n",
    "                                in_vic, pred_syn = label_in_vicinity(vicinity_matrix=vmat, vicinity_synsets=vsyn,\n",
    "                                                           target_vocab=target_vocab[:100], \n",
    "                                                           spatial_tags=spatial_tags[:100], true_label=local_labels[i])\n",
    "                                \n",
    "                                true_pred.append(in_vic)\n",
    "                                predicted_synsets.append(pred_syn)\n",
    "                                \n",
    "#                                 print(\"In Vicinity? --> {}\".format(in_vic))\n",
    "#                                 print(\"Predicted synsets --> {}\".format(pred_syn))\n",
    "\n",
    "                                if in_vic==True:\n",
    "                                    correct_sense += 1\n",
    "                                    correct_sense_batch += 1\n",
    "\n",
    "                            print(true_pred)\n",
    "                            print(predicted_synsets)\n",
    "                        \n",
    "                            batch_acc = correct_sense_batch / len(local_batch)\n",
    "                            history[\"sense_accuracy\"].append(batch_acc)\n",
    "#                             print(\"correct sense batch ({}) / local_batch ({}) = {}\".format(correct_sense_batch, len(local_batch), batch_acc))\n",
    "\n",
    "\n",
    "                        t1 = time.time()\n",
    "                        print(f'Epoch {epoch}: train loss = {train_loss:.4f}, batch accuracy: {batch_acc:.4f}, time = {t1-t0:.4f}')\n",
    "\n",
    "\n",
    "                sense_accuracy = correct_sense / nb_words_validation\n",
    "\n",
    "                print(\"The sense accuracy on the validation set is {} %\".format(sense_accuracy * 100))\n",
    "                \n",
    "        # **************************************************************************************************************\n",
    "        # Plot Histogram \n",
    "        # **************************************************************************************************************\n",
    "        data1 = history[\"train_loss\"] \n",
    "        data2 = history[\"sense_accuracy\"]\n",
    "\n",
    "        fig, ax1 = plt.subplots()\n",
    "\n",
    "        color = 'tab:red'\n",
    "        ax1.set_xlabel('time (s)')\n",
    "        ax1.set_ylabel('loss', color=color)\n",
    "        ax1.plot(data1, color=color)\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "        color = 'tab:blue'\n",
    "        ax2.set_ylabel('accuracy', color=color)  # we already handled the x-label with ax1\n",
    "        ax2.plot(data2, color=color)\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        plt.show()\n",
    "\n",
    "        return history\n",
    "    \n",
    "    \n",
    "    # assuming the sentence is already splitted into tokens, e.g. ['fall', 'in', 'catastrophes']\n",
    "    def test(self, testing_data, labels, test_syn, data, batch_size, num_workers, target_vocab, spatial_tags, k=5, shuffle=True):\n",
    "        \n",
    "        # parameters\n",
    "        params = {'batch_size': batch_size, #64,\n",
    "                  'shuffle': shuffle,\n",
    "                  'collate_fn': lambda x: x,\n",
    "                  'num_workers': num_workers} #6} #set 0 if training on Windows machine\n",
    "\n",
    "        # Training and validation data generators\n",
    "        testing_set = Dataset(testing_data, labels, test_syn)\n",
    "        testing_generator = torch.utils.data.DataLoader(testing_set, **params)\n",
    "        \n",
    "        # ------\n",
    "        # Count words in sentence to calculate accuracy\n",
    "        # ------\n",
    "        nb_words_testing = 0\n",
    "\n",
    "        for batch in testing_generator:\n",
    "            for sentence, label, syn in batch:\n",
    "                nb_words_testing += len(sentence)\n",
    "                \n",
    "        # --------------------------\n",
    "        VOCAB, weights_matrix = load_vocab(data, embed_size=embed_size)\n",
    "\n",
    "\n",
    "        # ---------------------------  \n",
    "        # testing\n",
    "        # ---------------------------\n",
    "        correct_sense = 0\n",
    "        sense_accuracy = 0\n",
    "        \n",
    "        t0 = time.time()\n",
    "\n",
    "        # set model to eval mode to ignore updating the weights of the model\n",
    "        self.model.eval()\n",
    "\n",
    "        # do not calculate gradients while evaluating\n",
    "        with torch.set_grad_enabled(False):\n",
    "\n",
    "            for batch in testing_generator:\n",
    "                print(\"Batches for testing\")\n",
    "                print(\"#\" * 100)\n",
    "\n",
    "                for local_batch, local_labels, local_synsets in batch:\n",
    "\n",
    "                    # Transform list(<string>) to Tensor(<Tensor>)\n",
    "                    print(\"Input Sentence\")\n",
    "                    print(local_batch)\n",
    "                    input_words = local_batch\n",
    "                    local_batch = numericalize(local_batch, VOCAB)\n",
    "#                     print(type(local_batch), local_batch)\n",
    "\n",
    "\n",
    "                    # Transform List(<Tensor>) to Tensor(<Tensor>)\n",
    "                    # I have labels of same length --> this should be no problem for Tensor\n",
    "                    local_labels = torch.stack(local_labels)\n",
    "                    print(\"Labels:\")\n",
    "                    print(local_synsets)\n",
    "#                     print(\"Labels\")\n",
    "#                     print(type(local_labels), len(local_labels), type(local_labels[0]))\n",
    "#                     print(local_labels)\n",
    "\n",
    "                    # Transfer to GPU\n",
    "                    local_batch, local_labels = local_batch.to(self.device), local_labels.to(self.device)\n",
    "\n",
    "                    # Model computations\n",
    "                    # out outputs the indices of wordnet database\n",
    "                    out = self.model(local_batch)\n",
    "\n",
    "                    # During validation and testing, I want to be less strict.\n",
    "                    # So, if a point resides within the label sphere, the sense is correctly identified.\n",
    "                    loss = geometric_loss(out, local_labels, include_r=True)\n",
    "\n",
    "                    vloss_sum += loss.item()                  \n",
    "\n",
    "                    validation_loss = vloss_sum / len(local_batch)\n",
    "                    history['testing_loss'].append(validation_loss)\n",
    "#                             print(history)\n",
    "#                             print(len(history['validation_loss']))\n",
    "\n",
    "                    correct_sense_batch = 0\n",
    "#                             print(\"Initializing the corrext sense batch = {}\".format(correct_sense_batch))\n",
    "\n",
    "                    true_pred = []\n",
    "                    predicted_synsets = []\n",
    "\n",
    "                    for i, word_tag in enumerate(out):\n",
    "\n",
    "                        vindices, vmat, vsyn = vicinity_matrix(spatial_params=word_tag,\n",
    "                                                       target_vocab=target_vocab,\n",
    "                                                       spatial_tags=spatial_tags, k=k)\n",
    "#                                 print(\"Vicinity Matrix-Synsets: {}\".format(vsyn))\n",
    "\n",
    "        \n",
    "                        in_vic, pred_syn = label_in_vicinity(vicinity_matrix=vmat, vicinity_synsets=vsyn,\n",
    "                                                   target_vocab=target_vocab, \n",
    "                                                   spatial_tags=spatial_tags, true_label=local_labels[i])\n",
    "        \n",
    "                        print(\"In Vicinity? --> {}\".format(in_vic))\n",
    "                        print(\"Predicted synsets --> {}\".format(pred_syn))\n",
    "            \n",
    "                        true_pred.append(in_vic)\n",
    "                        predicted_synsets.append(pred_syn)\n",
    "                        \n",
    "\n",
    "                        if in_vic:\n",
    "                            correct_sense += 1\n",
    "                            correct_sense_batch += 1\n",
    "\n",
    "                    print(true_pred)\n",
    "                    print(predicted_synsets)\n",
    "                    \n",
    "                    batch_acc = correct_sense_batch / len(local_batch)\n",
    "                    history[\"sense_accuracy\"].append(batch_acc)\n",
    "#                             print(\"correct sense batch ({}) / local_batch ({}) = {}\".format(correct_sense_batch, len(local_batch), batch_acc))\n",
    "\n",
    "                    \n",
    "                t1 = time.time()\n",
    "                print(f'batch accuracy: {batch_acc:.4f}, time = {t1-t0:.4f}')\n",
    "\n",
    "\n",
    "        sense_accuracy = correct_sense / nb_words_validation\n",
    "\n",
    "        print(\"The sense accuracy on the testing set is {} %\".format(sense_accuracy * 100))\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    def tag(self, sentence, embed_size, target_vocab, spatial_tags, k):\n",
    "        print(\"Initial Input: \", sentence)\n",
    "        \n",
    "        if isinstance(sentence, str):\n",
    "            # preprocess the sentence, such that the lemmatized sentence is returned\n",
    "            lemm_sentence = preprocess(sentence)\n",
    "            tokens = list(map(lambda x: x[0], lemm_sentence))\n",
    "            \n",
    "        if isinstance(sentence, list):\n",
    "            lst2str = \" \".join(sentence)\n",
    "            lemm_sentence = preprocess(lst2str)\n",
    "            tokens = list(map(lambda x: x[0], lemm_sentence))\n",
    "            \n",
    "            \n",
    "        \n",
    "        N = len(tokens)\n",
    "        tags = '?' * N\n",
    "        print(\"Lemmatized Sentence: \", tokens)\n",
    "        \n",
    "        #print(tags)\n",
    "        \n",
    "        data = tokens\n",
    "        \n",
    "        # words embeddings\n",
    "        vocab, wmat = load_vocab(data, embed_size)\n",
    "        \n",
    "        # numericalize words\n",
    "        num_data = numericalize(data, vocab)\n",
    "        \n",
    "        num_data = num_data.to(self.device)\n",
    "        \n",
    "        out = self.model(num_data)\n",
    "        \n",
    "        distances = []\n",
    "        predicted_synsets = []\n",
    "\n",
    "        for i, word_tag in enumerate(out):\n",
    "\n",
    "            vindices, vmat, vsyn = vicinity_matrix(spatial_params=word_tag,\n",
    "                                           target_vocab=target_vocab,\n",
    "                                           spatial_tags=spatial_tags, k=k)\n",
    "#                                 print(\"Vicinity Matrix-Synsets: {}\".format(vsyn))\n",
    "            \n",
    "            predicted_synsets.append(vsyn)\n",
    "    \n",
    "            distances.append(vmat)\n",
    "            #distances.append(decode_key(vsyn.keys(), vmat))\n",
    "            \n",
    "        for i in range(N):\n",
    "            print(data[i], \"\\t\", tags[i], \"\\t\", predicted_synsets[i].items())\n",
    "            print()\n",
    "            \n",
    "        \n",
    "        return predicted_synsets\n",
    "\n",
    "        \n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from bert_embedding import BertEmbedding\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk import pos_tag, WordNetLemmatizer\n",
    "from pprint import pprint\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "# set of english stop words U set of punctuation\n",
    "EN_STOPWORDS_PUNCT = set(stopwords.words('english')).union(set(string.punctuation))\n",
    "WN_LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def tags4wn(tag):\n",
    "    \"\"\"Penn Treebank tags: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "    Converts PennTreeBank tags to WN tags, e.g. n, a, v\"\"\"\n",
    "    tag_conversion = {\"NN\": \"n\", # noun\n",
    "                      \"JJ\": \"a\", # adjective\n",
    "                      \"VB\": \"v\", # verb\n",
    "                      \"RB\": \"r\"} # adverb\n",
    "    # there are still many more tags\n",
    "    try:\n",
    "        # return the WN tags\n",
    "        return tag_conversion[tag[:2]]\n",
    "    except:\n",
    "        # if no tag is found, treat the word as a noun\n",
    "        return \"n\"\n",
    "        # I think that in our case it is better to consider them all\n",
    "        # return None\n",
    "\n",
    "def preprocess(sentence):\n",
    "    \"\"\"Preprocesses a raw input sentence and return a list of each word with its POS tag.\"\"\"\n",
    "    # Tokenization\n",
    "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "    # lowercase all words\n",
    "    lower = [word.lower() for word in tokenized_sentence]\n",
    "    # print(lower)\n",
    "    # delete stop words and punctuation\n",
    "    clean_sentence = [word for word in lower if word not in EN_STOPWORDS_PUNCT]\n",
    "    # print(clean_sentence)\n",
    "    # use wordNet Lemmatizer to do POS and then lemmatize\n",
    "    pos_tagging = pos_tag(clean_sentence)\n",
    "    # print(pos_tagging)\n",
    "    # Lemmatize\n",
    "    lemmatized_sentence = [(WN_LEMMATIZER.lemmatize(word, pos=tags4wn(tag)), tags4wn(tag)) for word, tag in pos_tagging]\n",
    "    # print(lemmatized_sentence)\n",
    "\n",
    "    return lemmatized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('frog', 'n'), ('jumping', 'n')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frog = preprocess(\"The frog is jumping.\")\n",
    "frog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frog', 'jumping']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: x[0], frog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\envs\\Ball4WSD\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Validation ...\n",
      "New Batch for Validation\n",
      "####################################################################################################\n",
      "Input Sentence\n",
      "['necessary', 'normal']\n",
      "Labels:\n",
      "['necessary.a.01', 'normal.a.01']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.6875, 147138.8438, 147406.7656, 149039.5625, 149612.2812])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.5625, 147138.7656, 147406.6719, 149039.4531, 149612.1562])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['personal', 'choice']\n",
      "Labels:\n",
      "['personal.a.01', 'choice.n.02']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.4688, 147138.7031, 147406.5938, 149039.3594, 149612.0625])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.3281, 147138.5781, 147406.4688, 149039.2188, 149611.9219])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary']\n",
      "Labels:\n",
      "['necessary.s.02']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.9375, 147138.2812, 147406.1406, 149038.8594, 149611.5312])]}\n",
      "True is not in list\n",
      "[False]\n",
      "[['no-synset']]\n",
      "Input Sentence\n",
      "['morally', 'necessary']\n",
      "Labels:\n",
      "['morally.r.01', 'necessary.a.01']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.4844, 147137.9531, 147405.7500, 149038.4375, 149611.0781])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.3438, 147137.8438, 147405.6250, 149038.2969, 149610.9375])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'physical_ability', 'mental_ability']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144844.1406, 147139.1719, 147407.1406, 149040.0000, 149612.7344])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.8594, 147138.9531, 147406.8906, 149039.7344, 149612.4531])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.4688, 147138.6719, 147406.5625, 149039.3750, 149612.0625])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.1875, 147138.4531, 147406.3281, 149039.1094, 149611.7812])]}\n",
      "True is not in list\n",
      "[False, False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Epoch 0: train loss = 59756.5338, batch accuracy: 0.0000, time = 4.3190\n",
      "The sense accuracy on the validation set is 0.0 %\n",
      "Training ...\n",
      "Validation ...\n",
      "New Batch for Validation\n",
      "####################################################################################################\n",
      "Input Sentence\n",
      "['necessary']\n",
      "Labels:\n",
      "['necessary.s.02']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.9375, 147138.2812, 147406.1406, 149038.8594, 149611.5312])]}\n",
      "True is not in list\n",
      "[False]\n",
      "[['no-synset']]\n",
      "Input Sentence\n",
      "['personal', 'choice']\n",
      "Labels:\n",
      "['personal.a.01', 'choice.n.02']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.4688, 147138.7031, 147406.5938, 149039.3594, 149612.0625])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.3281, 147138.5781, 147406.4688, 149039.2188, 149611.9219])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['morally', 'necessary']\n",
      "Labels:\n",
      "['morally.r.01', 'necessary.a.01']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.4844, 147137.9531, 147405.7500, 149038.4375, 149611.0781])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.3438, 147137.8438, 147405.6250, 149038.2969, 149610.9375])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary', 'normal']\n",
      "Labels:\n",
      "['necessary.a.01', 'normal.a.01']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.6875, 147138.8438, 147406.7656, 149039.5625, 149612.2812])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.5625, 147138.7656, 147406.6719, 149039.4531, 149612.1562])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'physical_ability', 'mental_ability']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144844.1406, 147139.1719, 147407.1406, 149040.0000, 149612.7344])]}\n",
      "True is not in list\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.8594, 147138.9531, 147406.8906, 149039.7344, 149612.4531])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.4688, 147138.6719, 147406.5625, 149039.3750, 149612.0625])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.1875, 147138.4531, 147406.3281, 149039.1094, 149611.7812])]}\n",
      "True is not in list\n",
      "[False, False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Epoch 1: train loss = 119513.0521, batch accuracy: 0.0000, time = 4.1379\n",
      "The sense accuracy on the validation set is 0.0 %\n",
      "Training ...\n",
      "Validation ...\n",
      "New Batch for Validation\n",
      "####################################################################################################\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'physical_ability', 'mental_ability']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144844.1406, 147139.1719, 147407.1406, 149040.0000, 149612.7344])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.8594, 147138.9531, 147406.8906, 149039.7344, 149612.4531])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.4688, 147138.6719, 147406.5625, 149039.3750, 149612.0625])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.1875, 147138.4531, 147406.3281, 149039.1094, 149611.7812])]}\n",
      "True is not in list\n",
      "[False, False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary']\n",
      "Labels:\n",
      "['necessary.s.02']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.9375, 147138.2812, 147406.1406, 149038.8594, 149611.5312])]}\n",
      "True is not in list\n",
      "[False]\n",
      "[['no-synset']]\n",
      "Input Sentence\n",
      "['personal', 'choice']\n",
      "Labels:\n",
      "['personal.a.01', 'choice.n.02']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.4688, 147138.7031, 147406.5938, 149039.3594, 149612.0625])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.3281, 147138.5781, 147406.4688, 149039.2188, 149611.9219])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['morally', 'necessary']\n",
      "Labels:\n",
      "['morally.r.01', 'necessary.a.01']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.4844, 147137.9531, 147405.7500, 149038.4375, 149611.0781])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.3438, 147137.8438, 147405.6250, 149038.2969, 149610.9375])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary', 'normal']\n",
      "Labels:\n",
      "['necessary.a.01', 'normal.a.01']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.6875, 147138.8438, 147406.7656, 149039.5625, 149612.2812])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.5625, 147138.7656, 147406.6719, 149039.4531, 149612.1562])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Epoch 2: train loss = 119513.0739, batch accuracy: 0.0000, time = 3.8438\n",
      "The sense accuracy on the validation set is 0.0 %\n",
      "Training ...\n",
      "Validation ...\n",
      "New Batch for Validation\n",
      "####################################################################################################\n",
      "Input Sentence\n",
      "['necessary', 'normal']\n",
      "Labels:\n",
      "['necessary.a.01', 'normal.a.01']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.6875, 147138.8438, 147406.7656, 149039.5625, 149612.2812])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.5625, 147138.7656, 147406.6719, 149039.4531, 149612.1562])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['personal', 'choice']\n",
      "Labels:\n",
      "['personal.a.01', 'choice.n.02']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.4688, 147138.7031, 147406.5938, 149039.3594, 149612.0625])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.3281, 147138.5781, 147406.4688, 149039.2188, 149611.9219])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary']\n",
      "Labels:\n",
      "['necessary.s.02']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.9375, 147138.2812, 147406.1406, 149038.8594, 149611.5312])]}\n",
      "True is not in list\n",
      "[False]\n",
      "[['no-synset']]\n",
      "Input Sentence\n",
      "['morally', 'necessary']\n",
      "Labels:\n",
      "['morally.r.01', 'necessary.a.01']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.4844, 147137.9531, 147405.7500, 149038.4375, 149611.0781])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.3438, 147137.8438, 147405.6250, 149038.2969, 149610.9375])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'physical_ability', 'mental_ability']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144844.1406, 147139.1719, 147407.1406, 149040.0000, 149612.7344])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.8594, 147138.9531, 147406.8906, 149039.7344, 149612.4531])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.4688, 147138.6719, 147406.5625, 149039.3750, 149612.0625])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.1875, 147138.4531, 147406.3281, 149039.1094, 149611.7812])]}\n",
      "True is not in list\n",
      "[False, False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Epoch 3: train loss = 119513.1219, batch accuracy: 0.0000, time = 3.8694\n",
      "The sense accuracy on the validation set is 0.0 %\n",
      "Training ...\n",
      "Validation ...\n",
      "New Batch for Validation\n",
      "####################################################################################################\n",
      "Input Sentence\n",
      "['personal', 'choice']\n",
      "Labels:\n",
      "['personal.a.01', 'choice.n.02']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.4688, 147138.7031, 147406.5938, 149039.3594, 149612.0625])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.3281, 147138.5781, 147406.4688, 149039.2188, 149611.9219])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['morally', 'necessary']\n",
      "Labels:\n",
      "['morally.r.01', 'necessary.a.01']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.4844, 147137.9531, 147405.7500, 149038.4375, 149611.0781])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.3438, 147137.8438, 147405.6250, 149038.2969, 149610.9375])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'physical_ability', 'mental_ability']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144844.1406, 147139.1719, 147407.1406, 149040.0000, 149612.7344])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.8594, 147138.9531, 147406.8906, 149039.7344, 149612.4531])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.4688, 147138.6719, 147406.5625, 149039.3750, 149612.0625])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.1875, 147138.4531, 147406.3281, 149039.1094, 149611.7812])]}\n",
      "True is not in list\n",
      "[False, False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary']\n",
      "Labels:\n",
      "['necessary.s.02']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.9375, 147138.2812, 147406.1406, 149038.8594, 149611.5312])]}\n",
      "True is not in list\n",
      "[False]\n",
      "[['no-synset']]\n",
      "Input Sentence\n",
      "['necessary', 'normal']\n",
      "Labels:\n",
      "['necessary.a.01', 'normal.a.01']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.6875, 147138.8438, 147406.7656, 149039.5625, 149612.2812])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.5625, 147138.7656, 147406.6719, 149039.4531, 149612.1562])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Epoch 4: train loss = 179269.5676, batch accuracy: 0.0000, time = 3.9285\n",
      "The sense accuracy on the validation set is 0.0 %\n",
      "Training ...\n",
      "Validation ...\n",
      "New Batch for Validation\n",
      "####################################################################################################\n",
      "Input Sentence\n",
      "['morally', 'necessary']\n",
      "Labels:\n",
      "['morally.r.01', 'necessary.a.01']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.4844, 147137.9531, 147405.7500, 149038.4375, 149611.0781])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.3438, 147137.8438, 147405.6250, 149038.2969, 149610.9375])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'physical_ability', 'mental_ability']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144844.1406, 147139.1719, 147407.1406, 149040.0000, 149612.7344])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.8594, 147138.9531, 147406.8906, 149039.7344, 149612.4531])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.4688, 147138.6719, 147406.5625, 149039.3750, 149612.0625])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.1875, 147138.4531, 147406.3281, 149039.1094, 149611.7812])]}\n",
      "True is not in list\n",
      "[False, False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary', 'normal']\n",
      "Labels:\n",
      "['necessary.a.01', 'normal.a.01']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.6875, 147138.8438, 147406.7656, 149039.5625, 149612.2812])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.5625, 147138.7656, 147406.6719, 149039.4531, 149612.1562])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary']\n",
      "Labels:\n",
      "['necessary.s.02']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.9375, 147138.2812, 147406.1406, 149038.8594, 149611.5312])]}\n",
      "True is not in list\n",
      "[False]\n",
      "[['no-synset']]\n",
      "Input Sentence\n",
      "['personal', 'choice']\n",
      "Labels:\n",
      "['personal.a.01', 'choice.n.02']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.4688, 147138.7031, 147406.5938, 149039.3594, 149612.0625])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.3281, 147138.5781, 147406.4688, 149039.2188, 149611.9219])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Epoch 5: train loss = 89634.7967, batch accuracy: 0.0000, time = 4.1473\n",
      "The sense accuracy on the validation set is 0.0 %\n",
      "Training ...\n",
      "Validation ...\n",
      "New Batch for Validation\n",
      "####################################################################################################\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'physical_ability', 'mental_ability']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144844.1406, 147139.1719, 147407.1406, 149040.0000, 149612.7344])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.8594, 147138.9531, 147406.8906, 149039.7344, 149612.4531])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.4688, 147138.6719, 147406.5625, 149039.3750, 149612.0625])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.1875, 147138.4531, 147406.3281, 149039.1094, 149611.7812])]}\n",
      "True is not in list\n",
      "[False, False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary', 'normal']\n",
      "Labels:\n",
      "['necessary.a.01', 'normal.a.01']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.6875, 147138.8438, 147406.7656, 149039.5625, 149612.2812])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.5625, 147138.7656, 147406.6719, 149039.4531, 149612.1562])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary']\n",
      "Labels:\n",
      "['necessary.s.02']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.9375, 147138.2812, 147406.1406, 149038.8594, 149611.5312])]}\n",
      "True is not in list\n",
      "[False]\n",
      "[['no-synset']]\n",
      "Input Sentence\n",
      "['morally', 'necessary']\n",
      "Labels:\n",
      "['morally.r.01', 'necessary.a.01']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.4844, 147137.9531, 147405.7500, 149038.4375, 149611.0781])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.3438, 147137.8438, 147405.6250, 149038.2969, 149610.9375])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['personal', 'choice']\n",
      "Labels:\n",
      "['personal.a.01', 'choice.n.02']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.4688, 147138.7031, 147406.5938, 149039.3594, 149612.0625])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.3281, 147138.5781, 147406.4688, 149039.2188, 149611.9219])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Epoch 6: train loss = 89634.7875, batch accuracy: 0.0000, time = 4.3384\n",
      "The sense accuracy on the validation set is 0.0 %\n",
      "Training ...\n",
      "Validation ...\n",
      "New Batch for Validation\n",
      "####################################################################################################\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'physical_ability', 'mental_ability']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144844.1406, 147139.1719, 147407.1406, 149040.0000, 149612.7344])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.8594, 147138.9531, 147406.8906, 149039.7344, 149612.4531])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.4688, 147138.6719, 147406.5625, 149039.3750, 149612.0625])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.1875, 147138.4531, 147406.3281, 149039.1094, 149611.7812])]}\n",
      "True is not in list\n",
      "[False, False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary', 'normal']\n",
      "Labels:\n",
      "['necessary.a.01', 'normal.a.01']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.6875, 147138.8438, 147406.7656, 149039.5625, 149612.2812])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.5625, 147138.7656, 147406.6719, 149039.4531, 149612.1562])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary']\n",
      "Labels:\n",
      "['necessary.s.02']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.9375, 147138.2812, 147406.1406, 149038.8594, 149611.5312])]}\n",
      "True is not in list\n",
      "[False]\n",
      "[['no-synset']]\n",
      "Input Sentence\n",
      "['morally', 'necessary']\n",
      "Labels:\n",
      "['morally.r.01', 'necessary.a.01']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.4844, 147137.9531, 147405.7500, 149038.4375, 149611.0781])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.3438, 147137.8438, 147405.6250, 149038.2969, 149610.9375])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['personal', 'choice']\n",
      "Labels:\n",
      "['personal.a.01', 'choice.n.02']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.4688, 147138.7031, 147406.5938, 149039.3594, 149612.0625])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.3281, 147138.5781, 147406.4688, 149039.2188, 149611.9219])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Epoch 7: train loss = 59756.5253, batch accuracy: 0.0000, time = 3.8695\n",
      "The sense accuracy on the validation set is 0.0 %\n",
      "Training ...\n",
      "Validation ...\n",
      "New Batch for Validation\n",
      "####################################################################################################\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'physical_ability', 'mental_ability']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144844.1406, 147139.1719, 147407.1406, 149040.0000, 149612.7344])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.8594, 147138.9531, 147406.8906, 149039.7344, 149612.4531])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.4688, 147138.6719, 147406.5625, 149039.3750, 149612.0625])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.1875, 147138.4531, 147406.3281, 149039.1094, 149611.7812])]}\n",
      "True is not in list\n",
      "[False, False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary', 'normal']\n",
      "Labels:\n",
      "['necessary.a.01', 'normal.a.01']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.6875, 147138.8438, 147406.7656, 149039.5625, 149612.2812])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.5625, 147138.7656, 147406.6719, 149039.4531, 149612.1562])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['personal', 'choice']\n",
      "Labels:\n",
      "['personal.a.01', 'choice.n.02']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.4688, 147138.7031, 147406.5938, 149039.3594, 149612.0625])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.3281, 147138.5781, 147406.4688, 149039.2188, 149611.9219])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary']\n",
      "Labels:\n",
      "['necessary.s.02']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.9375, 147138.2812, 147406.1406, 149038.8594, 149611.5312])]}\n",
      "True is not in list\n",
      "[False]\n",
      "[['no-synset']]\n",
      "Input Sentence\n",
      "['morally', 'necessary']\n",
      "Labels:\n",
      "['morally.r.01', 'necessary.a.01']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.4844, 147137.9531, 147405.7500, 149038.4375, 149611.0781])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.3438, 147137.8438, 147405.6250, 149038.2969, 149610.9375])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Epoch 8: train loss = 59756.5394, batch accuracy: 0.0000, time = 3.7398\n",
      "The sense accuracy on the validation set is 0.0 %\n",
      "Training ...\n",
      "Validation ...\n",
      "New Batch for Validation\n",
      "####################################################################################################\n",
      "Input Sentence\n",
      "['necessary', 'normal']\n",
      "Labels:\n",
      "['necessary.a.01', 'normal.a.01']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.6875, 147138.8438, 147406.7656, 149039.5625, 149612.2812])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.5625, 147138.7656, 147406.6719, 149039.4531, 149612.1562])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'physical_ability', 'mental_ability']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144844.1406, 147139.1719, 147407.1406, 149040.0000, 149612.7344])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.8594, 147138.9531, 147406.8906, 149039.7344, 149612.4531])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.4688, 147138.6719, 147406.5625, 149039.3750, 149612.0625])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.1875, 147138.4531, 147406.3281, 149039.1094, 149611.7812])]}\n",
      "True is not in list\n",
      "[False, False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary']\n",
      "Labels:\n",
      "['necessary.s.02']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.9375, 147138.2812, 147406.1406, 149038.8594, 149611.5312])]}\n",
      "True is not in list\n",
      "[False]\n",
      "[['no-synset']]\n",
      "Input Sentence\n",
      "['personal', 'choice']\n",
      "Labels:\n",
      "['personal.a.01', 'choice.n.02']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.4688, 147138.7031, 147406.5938, 149039.3594, 149612.0625])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.3281, 147138.5781, 147406.4688, 149039.2188, 149611.9219])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['morally', 'necessary']\n",
      "Labels:\n",
      "['morally.r.01', 'necessary.a.01']\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.4844, 147137.9531, 147405.7500, 149038.4375, 149611.0781])]}\n",
      "True is not in list\n",
      "Vicinity indices: {'B': tensor([84, 26, 79, 63, 37])}\n",
      "Vicinity synsets: {'B': [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.3438, 147137.8438, 147405.6250, 149038.2969, 149610.9375])]}\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Epoch 9: train loss = 179269.5207, batch accuracy: 0.0000, time = 3.7527\n",
      "The sense accuracy on the validation set is 0.0 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 389,
       "width": 568
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'train_loss': [10544.449470079808,\n",
       "              24778.57556225747,\n",
       "              42955.59578138414,\n",
       "              36493.06539010681,\n",
       "              36943.861048572966,\n",
       "              46468.02023682413,\n",
       "              41223.95054019504,\n",
       "              72445.79220365928,\n",
       "              152762.76890017404,\n",
       "              59756.53376801164,\n",
       "              5787.735220578315,\n",
       "              13658.931296396473,\n",
       "              14798.25280605485,\n",
       "              32807.246983140496,\n",
       "              42381.30418751948,\n",
       "              37089.80245325951,\n",
       "              65158.85976819891,\n",
       "              94734.48692500276,\n",
       "              81595.31607132866,\n",
       "              119513.05214698684,\n",
       "              8835.610629534676,\n",
       "              34378.029375217055,\n",
       "              21704.608205821933,\n",
       "              70901.5560652988,\n",
       "              45995.2254942601,\n",
       "              55519.385884174255,\n",
       "              84745.15962539849,\n",
       "              74168.74192911744,\n",
       "              83742.80786126062,\n",
       "              119513.07387775874,\n",
       "              10609.871034517306,\n",
       "              27007.48836459339,\n",
       "              19247.78520280022,\n",
       "              65614.54041202654,\n",
       "              42331.43077861757,\n",
       "              64297.916259591024,\n",
       "              57797.506798902585,\n",
       "              68341.95112919784,\n",
       "              54396.90602711827,\n",
       "              119513.1219180333,\n",
       "              7871.191805331637,\n",
       "              12869.01077029074,\n",
       "              28827.668556248187,\n",
       "              28054.048492148748,\n",
       "              51655.1363620809,\n",
       "              79592.8220137341,\n",
       "              87448.80944611241,\n",
       "              76131.05933016172,\n",
       "              86740.92018304237,\n",
       "              179269.5676445942,\n",
       "              10609.868235847955,\n",
       "              29090.93855091068,\n",
       "              30113.26525757383,\n",
       "              32109.10577508207,\n",
       "              41683.171744607855,\n",
       "              89154.08379361978,\n",
       "              67292.05228332673,\n",
       "              42481.6207091242,\n",
       "              52726.89979782729,\n",
       "              89634.79670704891,\n",
       "              10544.448082676066,\n",
       "              21915.25448483236,\n",
       "              38660.618429752845,\n",
       "              46531.81053257782,\n",
       "              33875.781498470475,\n",
       "              31419.460507174994,\n",
       "              56703.25253846605,\n",
       "              48047.44023770911,\n",
       "              106814.18245557242,\n",
       "              89634.78754189868,\n",
       "              9524.151660006799,\n",
       "              23418.171654512775,\n",
       "              28108.075543863648,\n",
       "              28983.98699755121,\n",
       "              54085.84988230084,\n",
       "              113959.42999967132,\n",
       "              83828.96733235414,\n",
       "              133614.64312294204,\n",
       "              76381.38770088441,\n",
       "              59756.5253176232,\n",
       "              10609.867217920098,\n",
       "              27007.460672438887,\n",
       "              28724.284120396173,\n",
       "              36580.29935442902,\n",
       "              37009.29623577268,\n",
       "              81889.76140523754,\n",
       "              51489.33123572204,\n",
       "              61013.48810005858,\n",
       "              49511.259432206316,\n",
       "              59756.53938185339,\n",
       "              7856.0134910305305,\n",
       "              16501.879956292025,\n",
       "              27046.32256556671,\n",
       "              26866.476021654933,\n",
       "              49873.779911780315,\n",
       "              43494.45146852693,\n",
       "              138354.549333279,\n",
       "              78701.41960189356,\n",
       "              115654.53012596734,\n",
       "              179269.52069194784],\n",
       "             'validation_loss': [149551.70315559412,\n",
       "              331336.5917368325,\n",
       "              811201.4154888605,\n",
       "              555410.5113003354,\n",
       "              444060.368840063,\n",
       "              148528.2320151954,\n",
       "              256049.0045888361,\n",
       "              405858.8081447412,\n",
       "              555410.5113003354,\n",
       "              444060.368840063,\n",
       "              166355.11318989526,\n",
       "              813948.6847747765,\n",
       "              588759.2309686267,\n",
       "              738569.0345245318,\n",
       "              888120.737680126,\n",
       "              149551.70315559412,\n",
       "              331336.5917368325,\n",
       "              811201.4154888605,\n",
       "              555410.5113003354,\n",
       "              444060.368840063,\n",
       "              181784.8885812384,\n",
       "              331594.6921371435,\n",
       "              332152.459258467,\n",
       "              1477138.0690490634,\n",
       "              888120.7376801257,\n",
       "              149809.80355590512,\n",
       "              241260.01496784782,\n",
       "              632071.7330912898,\n",
       "              1412671.6981977748,\n",
       "              888120.7376801257,\n",
       "              166355.11318989526,\n",
       "              482261.92953538464,\n",
       "              1113052.0910859646,\n",
       "              706335.8490988874,\n",
       "              888120.7376801257,\n",
       "              166355.11318989526,\n",
       "              482261.92953538464,\n",
       "              1113052.0910859646,\n",
       "              706335.8490988874,\n",
       "              888120.7376801257,\n",
       "              166355.11318989526,\n",
       "              482261.92953538464,\n",
       "              664046.818116623,\n",
       "              1476621.8682484413,\n",
       "              888120.7376801257,\n",
       "              149551.70315559412,\n",
       "              241130.96476769232,\n",
       "              1113052.0910859646,\n",
       "              738310.9341242206,\n",
       "              888120.7376801257],\n",
       "             'sense_accuracy': [0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0]})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "#torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "T = RegTagger(use_cuda=use_cuda, device=device)\n",
    "T.train(batch_size=5, num_workers=0, max_epochs=10,\n",
    "        splittings=splittings, labels=labels, train_val_syn=train_val_syn, data=data, embed_size=300,\n",
    "        target_vocab=target_VOCAB, spatial_tags=SPATIAL_TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentence = ['Hundred', 'babies', 'are', 'one', 'years', 'old', '.']\n",
    "sentag = T.tag(sentence, 300, target_VOCAB[:100], SPATIAL_TAGS[:100], 5)\n",
    "# sentag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hundred', 'babies', 'are', 'one', 'years', 'old', '.']\n",
      "['hundred', 'baby', 'one', 'year', 'old']\n",
      "Hundred \t ? \t dict_items([('B', [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.0469, 147138.3594, 147406.2188, 149038.9531, 149611.6406],\n",
      "       grad_fn=<TopkBackward0>)])])\n",
      "\n",
      "babies \t ? \t dict_items([('B', [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.8281, 147138.2031, 147406.0312, 149038.7500, 149611.4219],\n",
      "       grad_fn=<TopkBackward0>)])])\n",
      "\n",
      "are \t ? \t dict_items([('B', [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.3281, 147137.8281, 147405.6094, 149038.2812, 149610.9219],\n",
      "       grad_fn=<TopkBackward0>)])])\n",
      "\n",
      "one \t ? \t dict_items([('B', [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144841.8125, 147137.4688, 147405.2031, 149037.7969, 149610.4062],\n",
      "       grad_fn=<TopkBackward0>)])])\n",
      "\n",
      "years \t ? \t dict_items([('B', [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144841.5625, 147137.2812, 147404.9844, 149037.5625, 149610.1562],\n",
      "       grad_fn=<TopkBackward0>)])])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = ['Hundred', 'babies', 'are', 'one', 'years', 'old', '.']\n",
    "sentag = T.tag(sentence, 300, target_VOCAB[:100], SPATIAL_TAGS[:100], 5)\n",
    "sentag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence = ['Hundred', 'babies', 'are', 'one', 'years', 'old', '.']\n",
    "sentag = T.tag(sentence, 300, target_VOCAB[:100], SPATIAL_TAGS[:100], 5)\n",
    "sentag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ['Hundred', 'babies', 'are', 'one', 'years', 'old', '.']\n",
    "sentag = T.tag(sentence, 300, target_VOCAB[:100], SPATIAL_TAGS[:100], 5)\n",
    "sentag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k = sentag[0][0].keys()\n",
    "d = sentag[1][0]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d.fromkeys(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, val in sentag[0].items():\n",
    "#     print(k)\n",
    "\n",
    "for di in sentag[0]:\n",
    "    print(type(di))\n",
    "    print(di.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.fromkeys(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, val in sentag[0].items():\n",
    "#     print(k)\n",
    "\n",
    "for di in sentag[0]:\n",
    "    print(type(di))\n",
    "    print(di.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.test(testing_data=splittings[], test_syn=, batch_size=5, num_workers=0, target_vocab=target_VOCAB, spatial_tag=SPATIAL_TAGS, k=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['train_loss'])\n",
    "# plt.plot(history['sense_accuracy'])\n",
    "# plt.legend(['training loss', 'validation accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['train_loss'])\n",
    "# plt.plot(history['sense_accuracy'])\n",
    "# plt.legend(['training loss', 'validation accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['validation_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['sense_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = history[\"train_loss\"] \n",
    "data2 = history[\"sense_accuracy\"]\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('time (s)')\n",
    "ax1.set_ylabel('loss', color=color)\n",
    "ax1.plot(data1, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(data2, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = np.array([7,7,3])\n",
    "a[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sense Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def is_contained(pred, sphere_coo, compare_spheres=False):\n",
    "\n",
    "    pt, word = coo2point(pred)\n",
    "    sphere_sense, sphere_center = coo2point(sphere_coo)\n",
    "\n",
    "    pt_rad = pred[-1]\n",
    "    sphere_rad = sphere_coo[-1] # in angles\n",
    "    \n",
    "    \n",
    "    \n",
    "    if compare_spheres == False:\n",
    "        contained = (pt[0] - sphere_sense[0])**2 + (pt[1] - sphere_sense[1])**2 <= sphere_rad**2\n",
    "    else:\n",
    "        contained = pt_rad + torch.linalg.norm(pt - sphere_sense) - sphere_rad <= 0\n",
    "\n",
    "    if contained:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "def vicinity_matrix(spatial_params, target_vocab: np.ndarray, spatial_tags: np.ndarray, k=5):#, include_sphere=True, include_r=True) -> [str]:\n",
    "    \"\"\"\n",
    "    Projects the predicted spatial parameters into the embedding space.\n",
    "    Returns the synsets in the vicinity of the projected point.\n",
    "    :param spatial_params:\n",
    "    :return: Vicinity matrix, synsets dict\n",
    "    \"\"\"\n",
    "    N = len(spatial_tags)\n",
    "    \n",
    "    #convert spatial_tags to tensor\n",
    "    spatial_tags = torch.from_numpy(spatial_tags)\n",
    "    \n",
    "    synsets = {} # sort from most specific to most general\n",
    "    \n",
    "    indices = {}\n",
    "\n",
    "    sense_pt, center_pt = coo2point(spatial_params)\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------------------------\n",
    "    # Prepare distance and containment calculations\n",
    "    # ----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # distance calculations\n",
    "    dist_spheres = torch.empty(N) \n",
    "    dist_pt_sphere = torch.empty(N) \n",
    "    dist_pts = torch.empty(N)\n",
    "    \n",
    "    for i, tag in enumerate(spatial_tags):\n",
    "        dist_spheres[i] = distance_loss(spatial_params, tag, include_r=True)\n",
    "        dist_pt_sphere[i] = distance_loss(spatial_params, tag, pt_sphere=True)\n",
    "        dist_pts[i] = distance_loss(spatial_params, tag, include_r=False)\n",
    "    \n",
    "    # containment calculations\n",
    "    full_contained = torch.empty(N) \n",
    "    part_contained = torch.empty(N)\n",
    "    disconnected = torch.empty(N) # handles points only\n",
    "    \n",
    "    for j, tag in enumerate(spatial_tags):\n",
    "        full_contained[j] = is_contained(spatial_params, tag, compare_spheres=True)\n",
    "        part_contained[j] = distance_loss(spatial_params, tag, include_r=True) > 0\n",
    "        disconnected[j] = ~ is_contained(spatial_params, tag, compare_spheres=True) # reverse the True <----> False\n",
    "    \n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Initialize the Vicinity Matrix\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    # row=3, col=3, topk=2, 2 indicates the column of indices and the distances\n",
    "    vicinity_matrix = torch.zeros((3,3, k, 2))\n",
    "    \n",
    "    ####################################################################################################################\n",
    "    # # Full contained + min dist between sense points\n",
    "    ####################################################################################################################\n",
    "    \n",
    "    print(\"True elements\")\n",
    "    true_indices1 = (full_contained == True).nonzero(as_tuple=True)[0]\n",
    "    print(true_indices1)\n",
    "    \n",
    "    if true_indices1.size(0) != 0:\n",
    "        dist1 = torch.index_select(dist_pts, 0, true_indices1)\n",
    "        print(\"dist1\", dist1)\n",
    "        print(\"k = \", k)\n",
    "        # sort in ascending order\n",
    "        # select top k \n",
    "        sort_dist1, sort_indices = torch.topk(dist1, k, largest=False)  \n",
    "        print(\"SORTING\", sort_dist1, sort_indices)\n",
    "        synsets1 = np.take(target_vocab, sort_indices, 0)\n",
    "        synsets[\"A\"] = synsets1\n",
    "        indices[\"A\"] = sort_indices\n",
    "        # index, distance (without synsets because this would result in conflicts for torch.tensor that do not support str)\n",
    "        vicinity_matrix[2][0] = torch.stack((sort_indices, sort_dist1), dim=1)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    ####################################################################################################################\n",
    "    # # Partially contained + min dist between sense points\n",
    "    ####################################################################################################################\n",
    "    true_indices2 = (part_contained == True).nonzero(as_tuple=True)[0]\n",
    "    print(\"True Indices 2\", true_indices2)\n",
    "    \n",
    "    if true_indices2.size(0) != 0:\n",
    "        dist1 = torch.index_select(dist_pts, 0, true_indices2)\n",
    "        # sort in ascending order\n",
    "        # select top k \n",
    "        sort_dist2, sort_indices2 = torch.topk(dist1, k, largest=False)     \n",
    "        synsets2 = np.take(target_vocab, sort_indices2, 0)\n",
    "        print(\"synset 2\", synsets2)\n",
    "        synsets[\"B\"] = synsets2\n",
    "        indices[\"B\"] = sort_indices2\n",
    "        # index, distance (without synsets because this would result in conflicts for torch.tensor that do not support str)\n",
    "        vicinity_matrix[2][1] = torch.stack((sort_indices2, sort_dist2), dim=1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # # Disconnected + min dist between spheres/point2sphere/sense points ---> acts as Nearest neighbor\n",
    "    ####################################################################################################################\n",
    "    # get indices, where disconnected is true\n",
    "    true_indices3 = (disconnected == True).nonzero(as_tuple=True)[0]\n",
    "    print(\"True Indices 3\", true_indices3)\n",
    "\n",
    "    if true_indices3.size(0) != 0:\n",
    "        # get the distances at those indices\n",
    "        dist_spheres3 = torch.index_select(dist_spheres, 0, true_indices3)\n",
    "        dist_pt_sphere3 = torch.index_select(dist_pt_sphere, 0, true_indices3)\n",
    "        dist_pts3 = torch.index_select(dist_pts, 0, true_indices3)\n",
    "\n",
    "        # sort-select top k minimum distances\n",
    "        sort_dist_spheres3, sort_sph_indices3 = torch.topk(dist_spheres3, k, largest=False)\n",
    "        sort_dist_pt_sphere3, sort_pt_sph_indices3 = torch.topk(dist_pt_sphere3, k, largest=False)\n",
    "        sort_dist_pts3, sort_pts_indices3 = torch.topk(dist_pts3, k, largest=False)\n",
    "\n",
    "        # get their corresponding synsets\n",
    "        synsets30 = np.take(target_vocab, sort_sph_indices3, 0)\n",
    "        #print(\"synset30\", synsets30)\n",
    "        synsets[\"C\"] = synsets30\n",
    "        indices[\"C\"] = sort_sph_indices3\n",
    "        \n",
    "        synsets31 = np.take(target_vocab, sort_pt_sph_indices3, 0)\n",
    "        synsets[\"D\"] = synsets31\n",
    "        indices[\"D\"] = sort_pt_sph_indices3\n",
    "        \n",
    "        synsets32 = np.take(target_vocab, sort_pts_indices3, 0)\n",
    "        synsets[\"E\"] = synsets32\n",
    "        indices[\"E\"] = sort_pts_indices3\n",
    "        \n",
    "        # insert them into the vicinity matrix    \n",
    "        vicinity_matrix[0][3] = torch.stack((sort_sph_indices3, sort_dist_spheres3), dim=1)\n",
    "        vicinity_matrix[1][3] = torch.stack((sort_pt_sph_indices3, sort_dist_pt_sphere3), dim=1)\n",
    "        vicinity_matrix[2][3] = torch.stack((sort_pts_indices3, sort_dist_pts3), dim=1)  \n",
    "    \n",
    "\n",
    "\n",
    "#     # get the spheres, where the point/point+radius is contained/overlaping/near\n",
    "\n",
    "#     # 1. check if the predicted point is contained in some sense\n",
    "#     contained = torch.empty(N)\n",
    "    \n",
    "#     for i, tag in enumerate(spatial_tags):\n",
    "#         contained[i] = is_contained(spatial_params, tag, compare_spheres=include_sphere)\n",
    "    \n",
    "#     # 2. For those synsets, which is the nearest synset point\n",
    "#     #use distance() to calculate distance between centers\n",
    "#     distances = torch.empty(N)\n",
    "#     for i, tag in enumerate(spatial_tags):\n",
    "#         distances[i] = distance_loss(spatial_params, tag, include_r=include_r)\n",
    "    \n",
    "#     # sort dist--> indices\n",
    "#     # check if for those distances the containment is true\n",
    "#     # if true: choose the one having min_dist as sense\n",
    "#     # top k senses must be stored in a dict \n",
    "    \n",
    "#     # check if for those distances the containment is false, then, only the radius is falsly predicted (not priority now)\n",
    "#     # if false and min_dist: choose it as potential sense\n",
    "    \n",
    "    \n",
    "\n",
    "#     # 3. If None of the synsets apply to that word sense\n",
    "#     # use sphere_dist to find the nearest sphere (most general synset), and assign it to that synset\n",
    "#     # (this maybe good for rare senses)\n",
    "#     # acts as a second chance\n",
    "#     rare_contained = torch.empty(N)\n",
    "#     rare_distances = torch.empty(N)\n",
    "#     for i, tag in enumerate(spatial_tags):\n",
    "#         rare_contained[i] = is_contained(spatial_params, tag, compare_spheres=False) #only consider sense point\n",
    "#         rare_distances[i] = distance_loss(spatial_params, tag, include_r=False)\n",
    "\n",
    "\n",
    "    return indices, vicinity_matrix, synsets\n",
    "\n",
    "def decode_key(key, mtx):\n",
    "    if key == \"A\":\n",
    "        return mtx[2, 0]\n",
    "    if key == \"B\":\n",
    "        return mtx[2, 1]\n",
    "    if key == \"C\":\n",
    "        return mtx[0, 2]\n",
    "    if key == \"D\":\n",
    "        return mtx[1, 2]\n",
    "    if key == \"E\":\n",
    "        return mtx[2, 2]\n",
    "    \n",
    "\n",
    "def label_in_vicinity(vicinity_matrix, vicinity_synsets, target_vocab, spatial_tags, true_label):\n",
    "    \n",
    "    checked_synsets = []\n",
    "    contained = []\n",
    "    checks = 0\n",
    "    predicted = []\n",
    "    distances = []\n",
    "    \n",
    "    in_vicinity = False\n",
    "    \n",
    "    # true label is either one of the possibilities [word, synset] or a randomly chosen one\n",
    "    \n",
    "    # induce subset of word-synset name \n",
    "    \n",
    "    #spatial_tags = torch.from_numpy(spatial_tags)\n",
    "    #idx_label = (spatial_tags == true_label).nonzero(as_tuple=True)[0]\n",
    "    # transform to numpy to \n",
    "    true_label = np.array(true_label, dtype=np.float64)\n",
    "    # keep spatial tag an np.ndarray\n",
    "    rounded_l = np.round(true_label, decimals=2)\n",
    "    try:\n",
    "        # detecting the true label from the spatial_tags\n",
    "        idx = [[np.array_equal(rounded_l, tag) for tag in spatial_tags].index(True)]\n",
    "        print(\"Found {} matching word-synset tags.\".format(len(idx)))\n",
    "        word_synset = target_vocab[idx] #list of list \n",
    "        print(\"Matching word-synset\", word_synset)\n",
    "        # check if word_synset is within the vicinity matrix\n",
    "        if len(word_synset) != 0:\n",
    "            for e in word_synset:\n",
    "                for key, val in vicinity_synsets.items():\n",
    "                    print(\"Searching in vicinity ... \")\n",
    "\n",
    "                    print(\"Checking if true label is in vicinity ...\")\n",
    "                    checked_synsets.append(e)\n",
    "                    is_there = e[1] in val[:, 1]\n",
    "                    checks += 1\n",
    "                    contained.append(is_there)\n",
    "                    \n",
    "#                     print(\"1\")\n",
    "#                     print(checked_synsets)\n",
    "#                     print(checks)\n",
    "#                     print(contained)\n",
    "                    \n",
    "                    if is_there:\n",
    "                        print(\"The main true label <{}> is in the vacinity of the predicted tag.\".format(e))\n",
    "                        idx_e = np.where(val[:, 1] == e[1])\n",
    "                        predicted.append(val[idx_e])\n",
    "#                         print(\"Predicted 1: \", predicted)\n",
    "                        distances.append(decode_key(key, vicinity_matrix)[idx_e][1])\n",
    "#                         print(\"Distances 1: \", distances)\n",
    "                    else:\n",
    "                        print(\"The main true label is not in vicinity ... \")\n",
    "                        distances.append(0.0)\n",
    "                        print(\"Searching if alternative true label synsets are in vicinity ... \")\n",
    "                    # induce all the word-synset tuples that have same synset as true label.\n",
    "                    # This double check is necessary since I choose the spatial tags in the training data randomly sometimes.\n",
    "                    # get indices of all word-synsets sharing same synset (not same word)\n",
    "                    ix = np.where(target_vocab == [_, e[1]])[0] # add [0] to indicate only the row index, not the column\n",
    "#                     print(\"Indices \", ix)\n",
    "                    if len(ix) != 0:\n",
    "                        pos_syn = target_vocab[ix]\n",
    "                        \n",
    "#                         print(\"Possible synsets: \", pos_syn)\n",
    "#                         print(target_vocab[:10])\n",
    "                        for t in pos_syn:\n",
    "                            checks += 1\n",
    "                            checked_synsets.append(t)\n",
    "                            is_near = t[1] in val[:, -1]\n",
    "                            contained.append(is_near)\n",
    "#                             print(\"2\")\n",
    "#                             print(checked_synsets)\n",
    "#                             print(checks)\n",
    "#                             print(contained)\n",
    "                            if is_near == True:                                    \n",
    "                                print(\"... The word-synset <{}> is in the vicinity of the predicted tag.\".format(t))\n",
    "                                idx_t = np.where(val[:, -1] == t[1])\n",
    "                                predicted.append(val[idx_t])\n",
    "#                                 print(\"Predicted 2: \", predicted)\n",
    "                                distances.append(decode_key(key, vicinity_matrix)[idx_t][1])\n",
    "#                                 print(\"Distances 2: \", distances)\n",
    "                            else:\n",
    "                                distances.append(0.0)\n",
    "                    else: \n",
    "                        print(\"... There are no other possibilites for word-synset <{}>\".format(e))\n",
    "                            \n",
    "        else:\n",
    "            print(\"Cannot find the suitable synset of this spatial tag!\")\n",
    "\n",
    "        \n",
    "    except ValueError as ve:\n",
    "        print(ve)\n",
    "        print(\"Found no index for the true label. Something went wrong ...\")\n",
    "        print(\"Comparing <true label = {}> with <rounded label = {}>\".format(true_label, rounded_l))\n",
    "    \n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Statistics\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    print(\"~\" * 80)\n",
    "    print(\"Statistics\")\n",
    "    print(\"~\" * 80)\n",
    "    \n",
    "#     print(\"Predicted Spatial Tag = \", spatial_params)\n",
    "    print(\"Checked Spatial Tag(s) ; contained? ; Predicted ; distances = ({}):\".format(len(checked_synsets)))\n",
    "    for s, c, p, d in zip(checked_synsets, contained, predicted, distances):\n",
    "        print(s, \";\", c, \";\", \"\\n\", p, \";\", d)\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "#     print(\"True Spatial Tag(s) is in vicinity of predicted tag: \", contained)\n",
    "    contained_idx = np.where(np.array(contained) == True)\n",
    "    \n",
    "#     print(\"contained_idx\", contained_idx)\n",
    "#     print(\"checked_idx\", np.array(checked_synsets)[contained_idx])\n",
    "#     print(\"slice\", np.array(checked_synsets)[:, 1])\n",
    "#     print(\"check_slice\", np.array(checked_synsets)[:, 1][contained_idx])\n",
    "\n",
    "    if len(contained_idx) != 0:\n",
    "        print()\n",
    "        only_syn = set(np.array(checked_synsets)[contained_idx][:, 1])\n",
    "        print(\"True Sense Tag(s) = ({}) ..\".format(len(only_syn)), only_syn)\n",
    "        print(\"Prediction is correct!\")\n",
    "        in_vicinity = True\n",
    "#         print(\"Distance(predicted_sense, nearest_true_sense) = ({}): \".format(len(np.array(predicted)[contained_idx])))\n",
    "#         for p, d in zip(np.array(predicted), distances):\n",
    "#               print(p, d)\n",
    "              \n",
    "    else:\n",
    "        print(\"Prediction is false ..\")\n",
    "        print(\"All synsets in the vicinity of the predicted tag are not true senses ..\")\n",
    "        print(\"Please check manually if the synsets in the vicinity are generalizations of the true labels.\")\n",
    "        in_vicinity = False\n",
    "    \n",
    "    \n",
    "    return checked_synsets, contained, checks, predicted, distances\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.where(target_VOCAB==[_,\"boat.n.01\"])[0]\n",
    "SPATIAL_TAGS[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_VOCAB[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_VOCAB[60])\n",
    "print(SPATIAL_TAGS[60])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SPATIAL_TAGS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_15008\\3284831832.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;31m# I created this synset for thousand.n.01, at index 60, word is 'thou'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0msample_tag\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m36670\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m100.07\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m180000.00\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1.6\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mtrue_lab\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mSPATIAL_TAGS\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m60\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0midx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmat\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msyn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvicinity_matrix\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mspatial_params\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msample_tag\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget_vocab\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtarget_VOCAB\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m100\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mspatial_tags\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mSPATIAL_TAGS\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m100\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mk\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'SPATIAL_TAGS' is not defined"
     ]
    }
   ],
   "source": [
    "params = torch.tensor([34035, 78.06, 174780, 0.0, 64.5]) # synset boat\n",
    "\n",
    "# I created this synset for thousand.n.01, at index 60, word is 'thou'\n",
    "sample_tag = torch.tensor([36670, 100.07, 180000.00, 0.0, 1.6])\n",
    "true_lab = torch.tensor(SPATIAL_TAGS[60])\n",
    "\n",
    "idx, mat, syn = vicinity_matrix(spatial_params=sample_tag, target_vocab=target_VOCAB[:100], spatial_tags=SPATIAL_TAGS[:100], k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e = np.array(['thou', 'thousand.n.01'])\n",
    "e = np.array([\"thou\", \"thousand.n.01\"])\n",
    "val = np.array([['k', 'thousand.n.01'], ['c', 'hundred.n.01'], ['i', 'one.n.01'], ['ks', 'thousand.n.01'], ['cs', 'hundred.n.01']])\n",
    "e in val \n",
    "idx_e = np.where(val == e)\n",
    "idx_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s =np.where(val[:, 1] == e[1])\n",
    "val[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = np.array([True, False, True])\n",
    "\n",
    "co[np.where(co==True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csyn, cont, ch, pred, dist = label_in_vicinity(vicinity_matrix=mat, vicinity_synsets=syn, \n",
    "                      target_vocab=target_VOCAB[:100], spatial_tags=SPATIAL_TAGS[:100], true_label=true_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "poss = []\n",
    "dis = []\n",
    "for p, d in zip(pred, dist):\n",
    "    print(\"There are {} true values in the vicinity of predicted tag.\".format(len(p)))\n",
    "    if len(p) > 0:\n",
    "        for i in range(len(p)):\n",
    "            is_in = p[i] in poss\n",
    "            if is_in:\n",
    "                continue\n",
    "            else:\n",
    "                poss.append(p[i])\n",
    "                dis.append(d[i])\n",
    "    print(poss)\n",
    "    print(dis)\n",
    "    \n",
    "#     if p in poss:\n",
    "#         continue\n",
    "#     else:\n",
    "#         poss.append(p)\n",
    "#     if d in dis:\n",
    "#         continue\n",
    "#     else:\n",
    "#         dis.append(d)\n",
    "        \n",
    "        \n",
    "for p,d in zip(poss, dis):\n",
    "    print(p, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spatial_params = torch.tensor([111740.0, 98.3, 980130.0, 0.0, 18.5], dtype=torch.float64)\n",
    "# df=spatial_wordnet\n",
    "# st = list(zip(df.l0, df.alpha, df.l_i, df.beta_i, df.radius))\n",
    "# df.loc[df['l0'] == 111740.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The problem here was that there is transforming from torch to numpy leads to failures in rounding\n",
    "s = np.array(data[0][2][2], dtype=np.float64)\n",
    "print(s)\n",
    "spatial_params = np.array([1.351440e+05, 2.501000e+01, 6.417603e+04, 9.000000e+01, 5.000000e-01])\n",
    "print(\"spatial_params\", spatial_params)\n",
    "# index = np.where(SPATIAL_TAGS==spatial_params)\n",
    "# len(index[0])\n",
    "# len(SPATIAL_TAGS)\n",
    "sr = np.round(s, decimals=2)\n",
    "pr = np.round(spatial_params, decimals=2)\n",
    "print(\"rounded spatial_params\", pr)\n",
    "idx = [np.array_equal(pr,x) for x in SPATIAL_TAGS].index(True)\n",
    "idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [13, 304]\n",
    "target_VOCAB[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(target_VOCAB == [_, 'dog.n.01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target_VOCAB[111115]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tp = data[0][2][2]\n",
    "print(\"torch tensor:\", tp)\n",
    "tpr = torch.round(tp)\n",
    "print(\"rounded:\", tpr)\n",
    "ttags = torch.from_numpy(SPATIAL_TAGS) \n",
    "print(ttags[50801])\n",
    "# torch.nonzero((ttags == tp).sum(dim=1) == ttags.size(1))\n",
    "torch.all(ttags == tpr)#, x=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#spatial_params = torch.tensor([143500, 6.8500, 7574.6, 0.0, 0.5])\n",
    "torch.nonzero((torch.from_numpy(SPATIAL_TAGS) == spatial_params).sum(dim=1) == torch.from_numpy(SPATIAL_TAGS).size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#spatial_params = torch.tensor([143500, 6.8500, 7574.6, 0.0, 0.5])\n",
    "torch.nonzero((torch.from_numpy(SPATIAL_TAGS) == spatial_params).sum(dim=1) == torch.from_numpy(SPATIAL_TAGS).size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.all(torch.from_numpy(SPATIAL_TAGS) == spatial_params, x=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.from_numpy(SPATIAL_TAGS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spatial_params = torch.tensor([111740.0, 98.3, 980130.0, 0.0, 18.5])\n",
    "c0 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[0]).nonzero(as_tuple=True)\n",
    "print(c0)\n",
    "c1 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[1]).nonzero(as_tuple=True)\n",
    "print(c1)\n",
    "c2 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[2]).nonzero(as_tuple=True)\n",
    "print(c2)\n",
    "c3 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[3]).nonzero(as_tuple=True)\n",
    "print(c3)\n",
    "c5 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[4]).nonzero(as_tuple=True)\n",
    "print(c5)\n",
    "# for i in c[1]:\n",
    "#     if i != 3:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.where(torch.from_numpy(SPATIAL_TAGS)== spatial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.where(torch.from_numpy(SPATIAL_TAGS)== spatial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = torch.tensor([34035, 78.06, 174780, 0.0, 64.5]) # synset boat\n",
    "\n",
    "idx, mat, syn = vicinity_matrix(spatial_params=params, target_vocab=target_VOCAB[:20], spatial_tags=SPATIAL_TAGS[:20], k=5)\n",
    "syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tp = data[0][2][2]\n",
    "print(\"torch tensor:\", tp)\n",
    "tpr = torch.round(tp)\n",
    "print(\"rounded:\", tpr)\n",
    "ttags = torch.from_numpy(SPATIAL_TAGS) \n",
    "print(ttags[50801])\n",
    "# torch.nonzero((ttags == tp).sum(dim=1) == ttags.size(1))\n",
    "torch.all(ttags == tpr)#, x=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#spatial_params = torch.tensor([143500, 6.8500, 7574.6, 0.0, 0.5])\n",
    "torch.nonzero((torch.from_numpy(SPATIAL_TAGS) == spatial_params).sum(dim=1) == torch.from_numpy(SPATIAL_TAGS).size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#spatial_params = torch.tensor([143500, 6.8500, 7574.6, 0.0, 0.5])\n",
    "torch.nonzero((torch.from_numpy(SPATIAL_TAGS) == spatial_params).sum(dim=1) == torch.from_numpy(SPATIAL_TAGS).size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.all(torch.from_numpy(SPATIAL_TAGS) == spatial_params, x=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.from_numpy(SPATIAL_TAGS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spatial_params = torch.tensor([111740.0, 98.3, 980130.0, 0.0, 18.5])\n",
    "c0 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[0]).nonzero(as_tuple=True)\n",
    "print(c0)\n",
    "c1 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[1]).nonzero(as_tuple=True)\n",
    "print(c1)\n",
    "c2 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[2]).nonzero(as_tuple=True)\n",
    "print(c2)\n",
    "c3 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[3]).nonzero(as_tuple=True)\n",
    "print(c3)\n",
    "c5 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[4]).nonzero(as_tuple=True)\n",
    "print(c5)\n",
    "# for i in c[1]:\n",
    "#     if i != 3:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.where(torch.from_numpy(SPATIAL_TAGS)== spatial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.where(torch.from_numpy(SPATIAL_TAGS)== spatial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = torch.tensor([34035, 78.06, 174780, 0.0, 64.5]) # synset boat\n",
    "\n",
    "idx, mat, syn = vicinity_matrix(spatial_params=params, target_vocab=target_VOCAB[:20], spatial_tags=SPATIAL_TAGS[:20], k=5)\n",
    "syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat[2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in syn.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.tensor([]).size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not torch.all(torch.eq(spatial_params, torch.zeros(spatial_params.size(0))), dim=0):\n",
    "    print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = torch.tensor([True, False, False, True, True])\n",
    "(t == True).nonzero(as_tuple=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([-2.0000,  0.2000, 12.0000, 89.0000]),\n",
       "indices=tensor([2, 3, 0, 1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = torch.tensor([12, 89, -2, 0.2])\n",
    "torch.sort(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([-2.0000,  0.2000, 12.0000]),\n",
       "indices=tensor([2, 3, 0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk = torch.topk(ts, 3, largest=False)\n",
    "topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['c', 'd', 'a'], dtype='<U1')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy = np.array([\"a\", \"b\", \"c\", \"d\"])\n",
    "\n",
    "np.take(sy, topk[1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "index_select() received an invalid combination of arguments - got (index=Tensor, input=numpy.ndarray, dim=int, ), but expected one of:\n * (Tensor input, int dim, Tensor index, *, Tensor out)\n * (Tensor input, name dim, Tensor index, *, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_15008\\1579275698.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex_select\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msy\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtopk\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m: index_select() received an invalid combination of arguments - got (index=Tensor, input=numpy.ndarray, dim=int, ), but expected one of:\n * (Tensor input, int dim, Tensor index, *, Tensor out)\n * (Tensor input, name dim, Tensor index, *, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "torch.index_select(input=sy, dim=0, index=topk[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row = 3\n",
    "col = 3\n",
    "k = 3\n",
    "t = 2\n",
    "vicinity_matrix = torch.empty((row, col, k, t))\n",
    "print(vicinity_matrix)\n",
    "print(vicinity_matrix[2][0])\n",
    "# newt = torch.tensor(topk[1], topk[0])\n",
    "# print(newt)\n",
    "# vicinity_matrix[2, 0] = torch.stack((torch.topk[1], torch.topk[0]), dim=-1)\n",
    "print(vicinity_matrix[2][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Geometric Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write a custom loss function in pytorch for \n",
    "# I need a fct to transform 5 coordinates into 2\n",
    "# and another function to calculate the loss between 2 points, then the total loss is the loss of all losses in a sentence \n",
    "\n",
    "\n",
    "def coo2point(coo):\n",
    "    print(coo)\n",
    "    l0 = coo[0]\n",
    "    alpha = coo[1]\n",
    "    alpha_rad = alpha * math.pi / 180\n",
    "    l_i = coo[2]\n",
    "    beta_i = coo[3]\n",
    "    beta_i_rad = beta_i * math.pi / 180\n",
    "    r = coo[4]\n",
    "    \n",
    "    # np.cos() and np.sin() take angles in radian as params\n",
    "    center_pt = torch.tensor([l0 * math.cos(alpha_rad), l0 * math.sin(alpha_rad)], dtype=torch.float64, requires_grad=True)\n",
    "    sense_pt = center_pt + torch.tensor([l_i * math.cos(alpha_rad + beta_i_rad),\n",
    "                                     l_i * math.sin(alpha_rad + beta_i_rad)], dtype=torch.float64, requires_grad=True)\n",
    "    return sense_pt, center_pt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def distance_loss(pred_pt, original_pt, include_r=False, pt_sphere=False):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two sense points, including radii.\n",
    "    :param pred_pt:\n",
    "    :param original_pt:\n",
    "    :param include_r: if set to true, include radius in the distance. \n",
    "                      It gives more freedom/tolerance degrees to the loss function. \n",
    "                      Loss is satisfied once the predicted point is part of original point.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "        \n",
    "    # original_pt = torch.from_numpy(original_pt)\n",
    "    # print(\"original point\", type(original_pt), original_pt)\n",
    "    \n",
    "    r1 = pred_pt[-1]\n",
    "    r2 = original_pt[-1]\n",
    "\n",
    "    pred_sense, pred_center = coo2point(pred_pt)\n",
    "    orig_sense, orig_center = coo2point(original_pt)\n",
    "    \n",
    "    \n",
    "    loss = torch.linalg.norm(torch.sub(pred_sense, orig_sense)) - r2\n",
    "    \n",
    "    # very strong assumption for the words that are not sense-tagged\n",
    "    # If I want more tolerance, I could neglect those tokens from the beginning\n",
    "    if torch.all(torch.eq(original_pt, torch.zeros(original_pt.size(0))), dim=0):\n",
    "        return loss\n",
    "    \n",
    "    if pt_sphere:\n",
    "        dist = torch.linalg.norm(torch.sub(pred_sense, orig_sense)) + r2\n",
    "        return dist\n",
    "\n",
    "    \n",
    "    if include_r:\n",
    "        \n",
    "        tolerant_loss = r1 + loss - r2\n",
    "    \n",
    "        if tolerant_loss < 0:\n",
    "            tolerant_loss = 0.0\n",
    "        \n",
    "#         if r1 > r2: #case the predicted radius is bigger than actual one\n",
    "#             tolerant_loss = torch.abs(torch.sub(r1, r2))\n",
    "           \n",
    "        return tolerant_loss\n",
    "    \n",
    "    else:\n",
    "        return loss \n",
    "   \n",
    "\n",
    "\n",
    "def geometric_loss(pred_list, label_list, include_r=False):\n",
    "    \n",
    "    # assert that the two lists must be of equal size\n",
    "    pred_size = pred_list.size()[0]\n",
    "    lab_size = label_list.size()[0]\n",
    "    assert pred_size == lab_size\n",
    "    \n",
    "    sentence_loss = 0.0\n",
    "    \n",
    "    # sum over all the tokens in the sentence\n",
    "    for i in range(pred_size):\n",
    "        sentence_loss += distance_loss(pred_list[i], label_list[i], include_r)\n",
    "        \n",
    "    return sentence_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example = torch.tensor([2.8, 45, 1.4, 0, 3.5])\n",
    "example2 = torch.tensor([0.0,0.0,0.0,0.0,0.0])\n",
    "geometric_loss(example, example2, include_r=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example = torch.tensor([2.8, 45, 1.4, 0, 3.5])\n",
    "example2 = torch.tensor([0.0,0.0,0.0,0.0,0.0])\n",
    "geometric_loss(example, example2, include_r=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sy = np.array([\"a\", \"b\", \"c\", \"d\"])\n",
    "\n",
    "np.take(sy, topk[1], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write a custom loss function in pytorch for \n",
    "# I need a fct to transform 5 coordinates into 2\n",
    "# and another function to calculate the loss between 2 points, then the total loss is the loss of all losses in a sentence \n",
    "\n",
    "\n",
    "def coo2point(coo):\n",
    "    print(coo)\n",
    "    l0 = coo[0]\n",
    "    alpha = coo[1]\n",
    "    alpha_rad = alpha * math.pi / 180\n",
    "    l_i = coo[2]\n",
    "    beta_i = coo[3]\n",
    "    beta_i_rad = beta_i * math.pi / 180\n",
    "    r = coo[4]\n",
    "    \n",
    "    # np.cos() and np.sin() take angles in radian as params\n",
    "    center_pt = torch.tensor([l0 * math.cos(alpha_rad), l0 * math.sin(alpha_rad)], dtype=torch.float64, requires_grad=True)\n",
    "    sense_pt = center_pt + torch.tensor([l_i * math.cos(alpha_rad + beta_i_rad),\n",
    "                                     l_i * math.sin(alpha_rad + beta_i_rad)], dtype=torch.float64, requires_grad=True)\n",
    "    return sense_pt, center_pt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def distance_loss(pred_pt, original_pt, include_r=False, pt_sphere=False):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two sense points, including radii.\n",
    "    :param pred_pt:\n",
    "    :param original_pt:\n",
    "    :param include_r: if set to true, include radius in the distance. \n",
    "                      It gives more freedom/tolerance degrees to the loss function. \n",
    "                      Loss is satisfied once the predicted point is part of original point.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    r1 = pred_pt[-1]\n",
    "    r2 = original_pt[-1]\n",
    "\n",
    "    pred_sense, pred_center = coo2point(pred_pt)\n",
    "    orig_sense, orig_center = coo2point(original_pt)\n",
    "    \n",
    "    \n",
    "    loss = torch.linalg.norm(torch.sub(pred_sense, orig_sense)) - r2\n",
    "    \n",
    "    # very strong assumption for the words that are not sense-tagged\n",
    "    # If I want more tolerance, I could neglect those tokens from the beginning\n",
    "    if torch.equal(original_pt, torch.zeros(original_pt.size()[0])):\n",
    "        return loss\n",
    "    \n",
    "    if pt_sphere:\n",
    "        dist = torch.linalg.norm(torch.sub(pred_sense, orig_sense)) + r2\n",
    "        return dist\n",
    "\n",
    "    \n",
    "    if include_r:\n",
    "        \n",
    "        tolerant_loss = r1 + loss - r2\n",
    "    \n",
    "        if tolerant_loss < 0:\n",
    "            tolerant_loss = 0.0\n",
    "        \n",
    "#         if r1 > r2: #case the predicted radius is bigger than actual one\n",
    "#             tolerant_loss = torch.abs(torch.sub(r1, r2))\n",
    "           \n",
    "        return tolerant_loss\n",
    "    \n",
    "    else:\n",
    "        return loss \n",
    "   \n",
    "\n",
    "\n",
    "def geometric_loss(pred_list, label_list, include_r=False):\n",
    "    \n",
    "    # assert that the two lists must be of equal size\n",
    "    pred_size = pred_list.size()[0]\n",
    "    lab_size = label_list.size()[0]\n",
    "    assert pred_size == lab_size\n",
    "    \n",
    "    sentence_loss = 0.0\n",
    "    \n",
    "    # sum over all the tokens in the sentence\n",
    "    for i in range(pred_size):\n",
    "        sentence_loss += distance_loss(pred_list[i], label_list[i], include_r)\n",
    "        \n",
    "    return sentence_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example = torch.tensor([2.8, 45, 1.4, 0, 3.5])\n",
    "example2 = torch.tensor([0.0,0.0,0.0,0.0,0.0])\n",
    "geometric_loss(example, example2, include_r=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l = torch.tensor([[2.8, 45, 1.4, 0, 3.5], [2124, 90, 1000, 14, 0.5]])\n",
    "l.view(-1)\n",
    "l.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(torch.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# <span style=\"color:red\"> WSD as Classification Task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> WSD as Classification Task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderModel(nn.Module):\n",
    "\n",
    "    def __init__(self, weights_matrix:np.ndarray, target_matrix: np.ndarray,\n",
    "                 ntoken: int, out_features:int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.weights_matrix = weights_matrix\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(self.weights_matrix, True)\n",
    "        \n",
    "        # Multi-head attention mechanism is included in TransformerEncoderLayer\n",
    "        # d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=<function relu>, \n",
    "        # layer_norm_eps=1e-05, batch_first=False, norm_first=False, device=None, dtype=None\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout) # activation\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers, norm=None)\n",
    "        \n",
    "        \n",
    "#         padding_idx (int, optional)  If specified, the entries at padding_idx do not contribute to the gradient;\n",
    "#         therefore, the embedding vector at padding_idx is not updated during training,\n",
    "#         i.e. it remains as a fixed pad. For a newly constructed Embedding, the embedding vector at\n",
    "#         padding_idx will default to all zeros, but can be updated to another value to be used as the padding vector.\n",
    "        self.emb = nn.Embedding(ntoken, d_model) \n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Linear layer: returns the last hidden state of the encoder \n",
    "        self.fc = nn.Linear(d_model, embedding_dim)\n",
    "        \n",
    "        # No! Here I am just redoing fully connected connections\n",
    "        # Linear Layer: affine transformation of last hidden layer into shape (1, embedding_dim)\n",
    "        #self.context_vec = nn.Linear(d_model, embedding_dim)\n",
    "        \n",
    "        #self.decoder = nn.Linear(d_model, ntoken)\n",
    "        \n",
    "        # Now, I need to have a Linear space that takes the whole/subset dataframe as input, extracts its spatial_context_vec,\n",
    "        # based on Glove-word-vector + spatial_point,\n",
    "        # then calculates softmax on this distribution\n",
    "        # choose the argmax\n",
    "        # get its spatial tags\n",
    "        # calculate distance loss between them\n",
    "        # do backprop! \n",
    "        # Nx300 into Nx227733: matmul product of two matrices Nx300 and 300x227733 --> Nx227733\n",
    "        # apply softmax to get the probabilities\n",
    "        # apply argmax to get the maximum indices\n",
    "        # use the indices to get the synset names as well as the mapping to coordinates\n",
    "        # into Nx5: mapping to the coordinates\n",
    "        \n",
    "        self.target_matrix = target_matrix\n",
    "        \n",
    "        #self.wn_embeddings = nn.Linear(1, target_matrix.shape[0])\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "#         weights_matrix = weights_matrix, \n",
    "#                                     ntoken= # false: 300,\n",
    "#                                     out_features=5,\n",
    "#                                     d_model=300,\n",
    "#                                     d_hid=200,\n",
    "#                                     nlayers=2,\n",
    "#                                     nhead=2,\n",
    "#                                     dropout=0.2\n",
    "        \n",
    "        \n",
    "        # -------------------------------------\n",
    "\n",
    "        # voc_size = len(text_field.vocab)\n",
    "        # print(\"voc_size: \", voc_size )\n",
    "        #\n",
    "        # # Embedding layer. If we're using pre-trained embeddings, copy them\n",
    "        # # into our embedding module.\n",
    "        # self.embedding = nn.Embedding(voc_size, 300)\n",
    "        # print(\"Embedding\", self.embedding)\n",
    "        # if text_field.vocab.vectors is not None:\n",
    "        #     self.embedding.weight = torch.nn.Parameter(TEXT.vocab.vectors)\n",
    "\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.emb.weight.data.uniform_(-initrange, initrange)\n",
    "        # self.decoder.bias.data.zero_()\n",
    "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        #self.output.bias.data.zero_()\n",
    "        #self.output.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        \n",
    "        #src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = torch.mul(self.emb(src), math.sqrt(self.d_model)) #? 1/sqrt!\n",
    "        print(\"Embedding\", src.shape)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        \n",
    "        src = self.pos_encoder(src)\n",
    "        print(\"Positional Encoding\", src.shape)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        \n",
    "        encoder_output = self.transformer_encoder(src) #, src_mask)\n",
    "        print(\"Encoder\", encoder_output.shape)\n",
    "        print(encoder_output)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        \n",
    "        linear_layer = self.fc(encoder_output)\n",
    "        print(\"Linear Layer\", linear_layer.shape)\n",
    "        print(linear_layer)\n",
    "        print('-' * 80)\n",
    "\n",
    "        # calculate the sum/weighted sum/ ?? on the linear layer to get the context vector of size (1, embd_dim)\n",
    "        context_vec = torch.sum(linear_layer, dim=1)\n",
    "        print(\"Final Context Vector\", context_vec.shape)\n",
    "        print(context_vec)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        # calculate the matrix that transforms a context vector into all wordnet embeddings\n",
    "        # expected: Nx223377x300\n",
    "#         wn_layer = self.wn_embeddings(context_vec)\n",
    "#         print(\"WordNet Embedding\", wn_layer.shape)\n",
    "#         print(wn_layer[0])\n",
    "#         print('-' * 80)\n",
    "        \n",
    "        # there must be a calculation here with the \n",
    "        # calculate dot product\n",
    "        # dim -1 is dim = 2 in 1[1[1[]],2[],..., N[]] \n",
    "        #trans_context_vec = torch.transpose(context_vec, 0, 1)\n",
    "        sense_matrix = torch.from_numpy(self.target_matrix).float()\n",
    "        print(\"sense mat\", sense_matrix.shape)\n",
    "        print(sense_matrix[0])\n",
    "        trans_sense_matrix = torch.transpose(sense_matrix, 0, 1)\n",
    "#         dot_prod = torch.sum(torch.matmul(context_vec, trans_sense_matrix), dim=-1)\n",
    "        dot_prod = torch.matmul(context_vec, trans_sense_matrix)\n",
    "\n",
    "        print(\"Dot Product\", dot_prod.shape)\n",
    "        print(dot_prod)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        \n",
    "        # calculate softmax\n",
    "        # expected Nx 227733\n",
    "        softmax = F.softmax(dot_prod)\n",
    "        print(\"Softmax\", softmax.shape)\n",
    "        print(softmax)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        # argmax\n",
    "        max_match = torch.argmax(softmax, dim=1) # dim 1 to indicate row\n",
    "        # expected Nx1\n",
    "        print(\"argmax\", max_match.shape)\n",
    "        print(max_match)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        # calculate softmax on all \n",
    "        \n",
    "        #dec = self.decoder(output)\n",
    "        #print(\"Decoder\")\n",
    "        #print(dec.shape)\n",
    "        #print(dec)\n",
    "        return max_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = defaultdict(list)\n",
    "\n",
    "VOCAB, weights_matrix = load_vocab(data, embed_size=300)\n",
    "\n",
    "# target_VOCAB\n",
    "# SPATIAL_TAGS\n",
    "\n",
    "n_epochs = 3\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    \n",
    "    model = TransformerEncoderModel(weights_matrix = weights_matrix, \n",
    "                                    target_matrix = tsense_matrix,\n",
    "                                    ntoken= len(VOCAB), #300,\n",
    "                                    out_features=5,\n",
    "                                    d_model=300,\n",
    "                                    d_hid=200,\n",
    "                                    nlayers=2,\n",
    "                                    nhead=2,\n",
    "                                    dropout=0.2)\n",
    "    model.to(device)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    #                       Optimizer\n",
    "    # ---------------------------------------------------------------------\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lr = 5.0  # learning rate\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "    # -------\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    loss_sum = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # for transformer\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "    # Training\n",
    "    for batch in training_generator:\n",
    "        \n",
    "        for local_batch, local_labels in batch:\n",
    "            \n",
    "            # Transform list(<string>) to Tensor(<Tensor>)\n",
    "            print(\"Input Sentence\")\n",
    "            print(local_batch)\n",
    "            input_words = local_batch\n",
    "            local_batch = numericalize(local_batch, VOCAB)\n",
    "            print(type(local_batch), local_batch)\n",
    "            \n",
    "            \n",
    "            # Transform List(<Tensor>) to Tensor(<Tensor>)\n",
    "            # I have labels of same length --> this should be no problem for Tensor\n",
    "            local_labels = torch.stack(local_labels)\n",
    "            print(\"Labels\")\n",
    "            print(type(local_labels), len(local_labels), type(local_labels[0]))\n",
    "            print(local_labels)\n",
    "            \n",
    "            # Transfer to GPU\n",
    "            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "            # Model computations\n",
    "            # out outputs the indices of wordnet database\n",
    "            out = model(local_batch)\n",
    "            print(type(out), out.shape)\n",
    "            # predicted synsets\n",
    "            word_synset_list = list(map(target_VOCAB.__getitem__, out))\n",
    "            print(\"Current Predictions\")\n",
    "            print(\"*\" * 100)\n",
    "            for i in range(len(input_words)):\n",
    "                print(\"<{}> predicted as {}\".format(input_words[i], word_synset_list[i]))\n",
    "            #print_pred = '\\n'.join('{} predicted as {}' for _, _ in zip(range(len(input_words)), range(len(word_synset_list)))).format(*input_words, *word_synset_list)\n",
    "            #print(print_pred)\n",
    "            #print(\"<{}> predicted as {}\".format(zip(input_words, word_synset_list)))\n",
    "            print(\"*\" * 100)\n",
    "            \n",
    "            tags = list(map(SPATIAL_TAGS.__getitem__, out))\n",
    "            # tags is a list of arrays of 5 parameters\n",
    "            print(\"Spatial Tags\", len(tags))\n",
    "            \n",
    "            ntokens = len(VOCAB)#300\n",
    "            #loss = criterion(out.view(-1, ntokens), local_labels)\n",
    "            # ignore prediction for the words that have no entry in the system? [0,0,0,0,0]\n",
    "            loss = distance_loss()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # I added this\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            # ---\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "\n",
    "            train_loss = loss_sum / len(local_batch)\n",
    "            history['train_loss'].append(train_loss)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "#         # Validation\n",
    "#         with torch.set_grad_enabled(False):\n",
    "#             for local_batch, local_labels in validation_generator:\n",
    "#                 # Transfer to GPU\n",
    "#                 local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "#                 # Model computations\n",
    "#                 [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X, y = training_set.__getitem__(0)\n",
    "for batch in training_generator:\n",
    "    print(len(batch))\n",
    "    for local_batch, local_label in batch:\n",
    "        print(local_batch, local_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# normalize each label\n",
    "sent_labels = torch.tensor([[8.5479e+04, 6.0380e+01, 1.2985e+05, 9.5740e+01, 5.0000e-01],\n",
    "                            [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
    "                            [1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01],\n",
    "                            [1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]])\n",
    "norm_sent_labels = torch.nn.functional.normalize(sent_labels, p=2.0, dim=1)\n",
    "norm_sent_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_labels\n",
    "# sent_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.transpose(sent_labels, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Output tensor\n",
    "out = torch.tensor([\n",
    "        [[-1.0549, -0.1433, -0.1394, -0.6256,  0.4749],\n",
    "         [-0.0305,  0.0031,  0.0382, -0.0017,  0.5000],\n",
    "         [ 0.5141, -0.0154, -0.3072,  0.4833,  0.3565],\n",
    "         [ 0.9379,  0.5004,  0.1174,  0.0816, -0.2704]],\n",
    "\n",
    "        [[-0.9528,  0.0359, -0.3091, -0.1796,  0.3259],\n",
    "         [ 0.2951,  0.0628, -0.1382, -0.0146,  0.2808],\n",
    "         [ 0.6864,  0.7282, -0.6748,  0.7032,  0.3163],\n",
    "         [ 1.1643,  0.3034,  0.2858,  0.0823, -0.5213]],\n",
    "\n",
    "        [[-0.3433,  0.7988, -0.9953, -0.2597,  0.6321],\n",
    "         [ 0.0388, -0.5481, -0.2017, -0.2700,  0.4406],\n",
    "         [ 0.9477,  0.4574, -0.2327,  0.0848,  0.3336],\n",
    "         [ 0.2500,  0.2523,  0.3734,  0.7667, -0.3530]],\n",
    "\n",
    "        [[-0.7934,  0.4223, -0.1750, -0.5049,  0.6555],\n",
    "         [ 0.0534, -0.6308, -0.7725, -0.3165,  0.3568],\n",
    "         [ 0.3805,  0.2004, -0.3675,  0.2600,  0.2836],\n",
    "         [-0.0449, -0.0765, -0.2250,  0.9507,  0.1856]]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nx300 [[1, ..., 300], ... , N[]] * [[1, ..., 300], ..., 227733[]]\n",
    "# from 4x5 to 4x20x5\n",
    "N = 1\n",
    "n = 5\n",
    "tensor = torch.rand(20, n) \n",
    "print(tensor.shape)\n",
    "print(tensor)\n",
    "\n",
    "trans = torch.transpose(sent_labels, 0, 1)\n",
    "print(trans.shape)\n",
    "print(trans)\n",
    "torch.matmul(tensor, trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.transpose(sent_labels, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Output tensor\n",
    "out = torch.tensor([\n",
    "        [[-1.0549, -0.1433, -0.1394, -0.6256,  0.4749],\n",
    "         [-0.0305,  0.0031,  0.0382, -0.0017,  0.5000],\n",
    "         [ 0.5141, -0.0154, -0.3072,  0.4833,  0.3565],\n",
    "         [ 0.9379,  0.5004,  0.1174,  0.0816, -0.2704]],\n",
    "\n",
    "        [[-0.9528,  0.0359, -0.3091, -0.1796,  0.3259],\n",
    "         [ 0.2951,  0.0628, -0.1382, -0.0146,  0.2808],\n",
    "         [ 0.6864,  0.7282, -0.6748,  0.7032,  0.3163],\n",
    "         [ 1.1643,  0.3034,  0.2858,  0.0823, -0.5213]],\n",
    "\n",
    "        [[-0.3433,  0.7988, -0.9953, -0.2597,  0.6321],\n",
    "         [ 0.0388, -0.5481, -0.2017, -0.2700,  0.4406],\n",
    "         [ 0.9477,  0.4574, -0.2327,  0.0848,  0.3336],\n",
    "         [ 0.2500,  0.2523,  0.3734,  0.7667, -0.3530]],\n",
    "\n",
    "        [[-0.7934,  0.4223, -0.1750, -0.5049,  0.6555],\n",
    "         [ 0.0534, -0.6308, -0.7725, -0.3165,  0.3568],\n",
    "         [ 0.3805,  0.2004, -0.3675,  0.2600,  0.2836],\n",
    "         [-0.0449, -0.0765, -0.2250,  0.9507,  0.1856]]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try normal sum over columns \n",
    "out_sum = torch.sum(out, dim=1)\n",
    "out_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "norm_sent_labels - out_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = defaultdict(list)\n",
    "\n",
    "VOCAB, weights_matrix = load_vocab(data, embed_size=300)\n",
    "\n",
    "n_epochs = 3\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    \n",
    "    model = TransformerEncoderModel(weights_matrix = weights_matrix, \n",
    "                                    ntoken=300,\n",
    "                                    out_features=5,\n",
    "                                    d_model=300,\n",
    "                                    d_hid=200,\n",
    "                                    nlayers=2,\n",
    "                                    nhead=2,\n",
    "                                    dropout=0.2)\n",
    "    model.to(device)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    #                       Optimizer\n",
    "    # ---------------------------------------------------------------------\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lr = 5.0  # learning rate\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "    # -------\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    loss_sum = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # for transformer\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "    # Training\n",
    "    for batch in training_generator:\n",
    "        \n",
    "        for local_batch, local_labels in batch:\n",
    "            \n",
    "            # Transform list(<string>) to Tensor(<Tensor>)\n",
    "            print(\"Input Sentence\")\n",
    "            print(local_batch)\n",
    "            local_batch = numericalize(local_batch, VOCAB)\n",
    "            print(type(local_batch), local_batch)\n",
    "            \n",
    "            \n",
    "            # Transform List(<Tensor>) to Tensor(<Tensor>)\n",
    "            # I have labels of same length --> this should be no problem for Tensor\n",
    "            local_labels = torch.stack(local_labels)\n",
    "            print(\"Labels\")\n",
    "            print(type(local_labels), len(local_labels), type(local_labels[0]))\n",
    "            print(local_labels)\n",
    "            \n",
    "            # Transfer to GPU\n",
    "            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "            # Model computations\n",
    "            out = model(local_batch)\n",
    "            ntokens = 300\n",
    "            loss = criterion(out.view(-1, ntokens), local_labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # I added this\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            # ---\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "\n",
    "            train_loss = loss_sum / len(local_batch)\n",
    "            history['train_loss'].append(train_loss)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "#         # Validation\n",
    "#         with torch.set_grad_enabled(False):\n",
    "#             for local_batch, local_labels in validation_generator:\n",
    "#                 # Transfer to GPU\n",
    "#                 local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "#                 # Model computations\n",
    "#                 [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Output tensor\n",
    "out = torch.tensor([\n",
    "        [[-1.0549, -0.1433, -0.1394, -0.6256,  0.4749],\n",
    "         [-0.0305,  0.0031,  0.0382, -0.0017,  0.5000],\n",
    "         [ 0.5141, -0.0154, -0.3072,  0.4833,  0.3565],\n",
    "         [ 0.9379,  0.5004,  0.1174,  0.0816, -0.2704]],\n",
    "\n",
    "        [[-0.9528,  0.0359, -0.3091, -0.1796,  0.3259],\n",
    "         [ 0.2951,  0.0628, -0.1382, -0.0146,  0.2808],\n",
    "         [ 0.6864,  0.7282, -0.6748,  0.7032,  0.3163],\n",
    "         [ 1.1643,  0.3034,  0.2858,  0.0823, -0.5213]],\n",
    "\n",
    "        [[-0.3433,  0.7988, -0.9953, -0.2597,  0.6321],\n",
    "         [ 0.0388, -0.5481, -0.2017, -0.2700,  0.4406],\n",
    "         [ 0.9477,  0.4574, -0.2327,  0.0848,  0.3336],\n",
    "         [ 0.2500,  0.2523,  0.3734,  0.7667, -0.3530]],\n",
    "\n",
    "        [[-0.7934,  0.4223, -0.1750, -0.5049,  0.6555],\n",
    "         [ 0.0534, -0.6308, -0.7725, -0.3165,  0.3568],\n",
    "         [ 0.3805,  0.2004, -0.3675,  0.2600,  0.2836],\n",
    "         [-0.0449, -0.0765, -0.2250,  0.9507,  0.1856]]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try normal sum over columns \n",
    "out_sum = torch.sum(out, dim=1)\n",
    "out_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "norm_sent_labels - out_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}