{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available kernels:\n",
      "  python3    C:\\Users\\HP\\Anaconda3\\envs\\Ball4WSD\\share\\jupyter\\kernels\\python3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TerminalIPythonApp] WARNING | Subcommand `ipython kernelspec` is deprecated and will be removed in future versions.\n",
      "[TerminalIPythonApp] WARNING | You likely want to use `jupyter kernelspec` in the future\n",
      "[ListKernelSpecs] WARNING | Config option `kernel_spec_manager_class` not recognized by `ListKernelSpecs`.\n"
     ]
    }
   ],
   "source": [
    "# check \n",
    "!ipython kernelspec list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aiohttp', '3.8.0']\n",
      "['aiosignal', '1.2.0']\n",
      "['antlr4-python3-runtime @ file:///D:/bld/antlr-python-runtime_1602352319407/work']\n",
      "['anyio', '3.3.4']\n",
      "['argcomplete', '1.12.3']\n",
      "['argon2-cffi @ file:///opt/conda/conda-bld/argon2-cffi_1645000214183/work']\n",
      "['argon2-cffi-bindings @ file:///C:/ci/argon2-cffi-bindings_1644569848815/work']\n",
      "['async-timeout', '4.0.0']\n",
      "['asynctest', '0.13.0']\n",
      "['attrs @ file:///opt/conda/conda-bld/attrs_1642510447205/work']\n",
      "['autobahn', '21.3.1']\n",
      "['backcall @ file:///home/ktietz/src/ci/backcall_1611930011877/work']\n",
      "['bcolz', '1.2.1']\n",
      "['bert-embedding', '1.0.1']\n",
      "['bleach @ file:///opt/conda/conda-bld/bleach_1641577558959/work']\n",
      "['bokeh @ file:///C:/ci/bokeh_1638363000110/work']\n",
      "['Bottleneck', '1.3.2']\n",
      "['bpemb', '0.3.3']\n",
      "['cached-property', '1.5.2']\n",
      "['certifi', '2021.10.8']\n",
      "['cffi @ file:///C:/ci/cffi_1613247308275/work']\n",
      "['chardet', '3.0.4']\n",
      "['charset-normalizer', '2.0.7']\n",
      "['click', '8.0.3']\n",
      "['cloudpickle @ file:///tmp/build/80754af9/cloudpickle_1632508026186/work']\n",
      "['colorama', '0.4.4']\n",
      "['cryptography', '35.0.0']\n",
      "['cycler @ file:///tmp/build/80754af9/cycler_1637851556182/work']\n",
      "['Cython', '0.29.14']\n",
      "['cytoolz', '0.11.0']\n",
      "['dask', '2021.10.0']\n",
      "['dataclasses', '0.6']\n",
      "['debugpy @ file:///C:/ci/debugpy_1637091911212/work']\n",
      "['decorator @ file:///opt/conda/conda-bld/decorator_1643638310831/work']\n",
      "['defusedxml @ file:///tmp/build/80754af9/defusedxml_1615228127516/work']\n",
      "['Deprecated', '1.2.12']\n",
      "['distributed @ file:///C:/ci/distributed_1635950227219/work']\n",
      "['editdistance', '0.6.0']\n",
      "['entrypoints', '0.3']\n",
      "['fairseq @ file:///D:/bld/fairseq_1609882179141/work']\n",
      "['filelock', '3.3.1']\n",
      "['flair', '0.8.0.post1']\n",
      "['fonttools', '4.25.0']\n",
      "['frozenlist', '1.2.0']\n",
      "['fsspec @ file:///opt/conda/conda-bld/fsspec_1642510437511/work']\n",
      "['ftfy', '6.0.1']\n",
      "['future', '0.18.2']\n",
      "['gdown', '3.12.2']\n",
      "['gensim', '3.8.3']\n",
      "['gluonnlp', '0.6.0']\n",
      "['graphviz', '0.8.4']\n",
      "['h5py', '3.6.0']\n",
      "['HeapDict @ file:///Users/ktietz/demo/mc3/conda-bld/heapdict_1630598515714/work']\n",
      "['huggingface-hub', '0.0.19']\n",
      "['hydra-core @ file:///home/conda/feedstock_root/build_artifacts/hydra-core_1629433216282/work']\n",
      "['hyperlink', '21.0.0']\n",
      "['hyperopt', '0.2.5']\n",
      "['idna', '3.3']\n",
      "['importlib-metadata', '4.8.1']\n",
      "['importlib-resources', '5.4.0']\n",
      "['ipykernel @ file:///C:/ci/ipykernel_1647000985174/work/dist/ipykernel-6.9.1-py3-none-any.whl']\n",
      "['ipython @ file:///C:/ci/ipython_1643800131373/work']\n",
      "['ipython-genutils @ file:///tmp/build/80754af9/ipython_genutils_1606773439826/work']\n",
      "['ipywidgets', '7.6.5']\n",
      "['Janome', '0.4.1']\n",
      "['jedi @ file:///C:/ci/jedi_1644297241925/work']\n",
      "['Jinja2 @ file:///tmp/build/80754af9/jinja2_1635780242639/work']\n",
      "['joblib', '1.1.0']\n",
      "['jsonschema', '4.2.0']\n",
      "['jupyter', '1.0.0']\n",
      "['jupyter-client @ file:///opt/conda/conda-bld/jupyter_client_1643638337975/work']\n",
      "['jupyter-console', '6.4.0']\n",
      "['jupyter-core @ file:///C:/ci/jupyter_core_1646976467633/work']\n",
      "['jupyter-server', '1.11.2']\n",
      "['jupyter-server-proxy', '3.1.0']\n",
      "['jupyterlab-pygments @ file:///tmp/build/80754af9/jupyterlab_pygments_1601490720602/work']\n",
      "['jupyterlab-widgets', '1.0.2']\n",
      "['kiwisolver @ file:///C:/ci/kiwisolver_1612282618948/work']\n",
      "['konoha', '4.6.4']\n",
      "['langdetect', '1.0.8']\n",
      "['lda-classification', '0.0.29']\n",
      "['locket', '0.2.1']\n",
      "['lxml', '4.6.3']\n",
      "['MarkupSafe @ file:///C:/ci/markupsafe_1621528383308/work']\n",
      "['matplotlib', '3.3.3']\n",
      "['matplotlib-inline', '0.1.3']\n",
      "['mistune @ file:///C:/ci/mistune_1594373272338/work']\n",
      "['mkl-fft', '1.3.0']\n",
      "['mkl-service', '2.3.0']\n",
      "['mock @ file:///tmp/build/80754af9/mock_1607622725907/work']\n",
      "['mpld3', '0.3']\n",
      "['msgpack @ file:///C:/ci/msgpack-python_1612287191162/work']\n",
      "['multidict', '5.2.0']\n",
      "['munkres', '1.1.4']\n",
      "['mxnet', '1.4.0']\n",
      "['nb-conda', '2.2.1']\n",
      "['nb-conda-kernels @ file:///C:/ci/nb_conda_kernels_1606832727237/work']\n",
      "['nbclient @ file:///tmp/build/80754af9/nbclient_1645431659072/work']\n",
      "['nbconvert', '6.2.0']\n",
      "['nbformat @ file:///tmp/build/80754af9/nbformat_1617383369282/work']\n",
      "['nest-asyncio @ file:///tmp/build/80754af9/nest-asyncio_1613680548246/work']\n",
      "['networkx', '2.5.1']\n",
      "['nltk @ file:///tmp/build/80754af9/nltk_1618327084230/work']\n",
      "['notebook @ file:///C:/ci/notebook_1645002740769/work']\n",
      "['numexpr @ file:///C:/ci/numexpr_1614798720432/work']\n",
      "['numpy', '1.21.2']\n",
      "['olefile', '0.46']\n",
      "['omegaconf @ file:///D:/bld/omegaconf_1636817949024/work']\n",
      "['overrides', '3.1.0']\n",
      "['packaging', '21.0']\n",
      "['pandas @ file:///C:/ci/pandas_1627570311072/work']\n",
      "['pandocfilters @ file:///opt/conda/conda-bld/pandocfilters_1643405455980/work']\n",
      "['parso @ file:///opt/conda/conda-bld/parso_1641458642106/work']\n",
      "['partd @ file:///tmp/build/80754af9/partd_1618000087440/work']\n",
      "['pickleshare @ file:///tmp/build/80754af9/pickleshare_1606932040724/work']\n",
      "['Pillow', '8.1.0']\n",
      "['portalocker', '2.4.0']\n",
      "['prometheus-client @ file:///opt/conda/conda-bld/prometheus_client_1643788673601/work']\n",
      "['prompt-toolkit', '3.0.22']\n",
      "['psutil @ file:///C:/ci/psutil_1612298033174/work']\n",
      "['py-babelnet', '0.0.2']\n",
      "['py-cpuinfo', '7.0.0']\n",
      "['pyarrow', '6.0.1']\n",
      "['pycparser @ file:///tmp/build/80754af9/pycparser_1594388511720/work']\n",
      "['Pygments @ file:///opt/conda/conda-bld/pygments_1644249106324/work']\n",
      "['pyparsing', '2.4.7']\n",
      "['pyrsistent @ file:///C:/ci/pyrsistent_1636093257833/work']\n",
      "['PySocks', '1.7.1']\n",
      "['python-dateutil @ file:///tmp/build/80754af9/python-dateutil_1626374649649/work']\n",
      "['pytorch-nlp', '0.5.0']\n",
      "['pytz @ file:///tmp/build/80754af9/pytz_1612215392582/work']\n",
      "['pywin32', '302']\n",
      "['pywinpty @ file:///C:/ci_310/pywinpty_1644230983541/work/target/wheels/pywinpty-2.0.2-cp37-none-win_amd64.whl']\n",
      "['PyYAML', '6.0']\n",
      "['pyzmq @ file:///C:/ci/pyzmq_1638435182681/work']\n",
      "['qtconsole', '5.1.1']\n",
      "['QtPy', '1.11.2']\n",
      "['regex', '2021.10.8']\n",
      "['requests', '2.26.0']\n",
      "['sacrebleu @ file:///tmp/build/80754af9/sacrebleu_1634578602645/work']\n",
      "['sacremoses', '0.0.46']\n",
      "['scikit-learn', '0.23.2']\n",
      "['scipy @ file:///C:/bld/scipy_1637806996411/work']\n",
      "['seaborn @ file:///tmp/build/80754af9/seaborn_1629307859561/work']\n",
      "['segtok', '1.5.10']\n",
      "['Send2Trash @ file:///tmp/build/80754af9/send2trash_1632406701022/work']\n",
      "['sentencepiece', '0.1.95']\n",
      "['simpervisor', '0.4']\n",
      "['six', '1.16.0']\n",
      "['smart-open', '4.1.0']\n",
      "['sniffio', '1.2.0']\n",
      "['sortedcontainers @ file:///tmp/build/80754af9/sortedcontainers_1623949099177/work']\n",
      "['spacy', '2.3.5']\n",
      "['sqlitedict', '1.7.0']\n",
      "['tables', '3.6.1']\n",
      "['tabulate', '0.8.9']\n",
      "['tblib @ file:///Users/ktietz/demo/mc3/conda-bld/tblib_1629402031467/work']\n",
      "['terminado @ file:///C:/ci/terminado_1644322782754/work']\n",
      "['testpath @ file:///tmp/build/80754af9/testpath_1624638946665/work']\n",
      "['thinc', '7.4.5']\n",
      "['threadpoolctl', '2.1.0']\n",
      "['tokenizers', '0.10.3']\n",
      "['tomotopy', '0.9.1']\n",
      "['toolz @ file:///tmp/build/80754af9/toolz_1636545406491/work']\n",
      "['torch', '1.10.1']\n",
      "['torchtext', '0.11.1']\n",
      "['tornado @ file:///C:/ci/tornado_1606935947090/work']\n",
      "['tqdm', '4.62.3']\n",
      "['traitlets @ file:///tmp/build/80754af9/traitlets_1636710298902/work']\n",
      "['transformers', '4.11.3']\n",
      "['txaio', '21.2.1']\n",
      "['typing', '3.6.6']\n",
      "['typing-extensions', '3.10.0.2']\n",
      "['urllib3', '1.26.7']\n",
      "['vpython', '7.6.2']\n",
      "['wcwidth @ file:///Users/ktietz/demo/mc3/conda-bld/wcwidth_1629357192024/work']\n",
      "['webcolors', '1.3']\n",
      "['webencodings', '0.5.1']\n",
      "['websocket-client', '1.2.1']\n",
      "['widgetsnbextension', '3.5.2']\n",
      "['wincertstore', '0.2']\n",
      "['wrapt', '1.12.1']\n",
      "['yarl', '1.7.2']\n",
      "['z3-solver', '4.8.13.0']\n",
      "['zict', '2.0.0']\n",
      "['zipp', '3.6.0']\n"
     ]
    }
   ],
   "source": [
    "import types\n",
    "\n",
    "def imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            yield val.__name__\n",
    "\n",
    "excludes = ['builtins', 'types', 'sys']\n",
    "\n",
    "imported_modules = [module for module in imports() if module not in excludes]\n",
    "\n",
    "clean_modules = []\n",
    "\n",
    "for module in imported_modules:\n",
    "\n",
    "    sep = '.'  # to handle 'matplotlib.pyplot' cases\n",
    "    rest = module.split(sep, 1)[0]\n",
    "    clean_modules.append(rest)\n",
    "\n",
    "changed_imported_modules = list(set(clean_modules))  # drop duplicates\n",
    "\n",
    "pip_modules = !pip freeze  # you could also use `!conda list` with anaconda\n",
    "\n",
    "for module in pip_modules:\n",
    "    print(module.split('=='))\n",
    "    \n",
    "    #if name in changed_imported_modules:\n",
    "     #   print(name + '\\t' + version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install kiwisolver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import math\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import bcolz\n",
    "\n",
    "import time\n",
    "import random\n",
    "import functools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.10 (default, Feb 26 2021, 13:06:18) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"../data/test_transformer/\"\n",
    "# split the dataset into training, validation and testing\n",
    "train_path = \"train.csv\"\n",
    "validate_path = \"validate.csv\"\n",
    "test_path = \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "tensor([1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02])\n"
     ]
    }
   ],
   "source": [
    "def to_tensor(string_list):\n",
    "    l_str = []\n",
    "    for ele in string_list:\n",
    "        if ele[0] == \"[\":\n",
    "            l_str.append(ele[1:])\n",
    "        else:\n",
    "            if ele[-1] == \"]\":\n",
    "                l_str.append(ele[:-1])\n",
    "            else:\n",
    "                l_str.append(ele)\n",
    "\n",
    "    str_vec = \" \".join(l_str)\n",
    "    torch_labels = torch.tensor(list(map(float, str_vec.split(' '))), dtype=torch.float32)\n",
    "    return torch_labels\n",
    "\n",
    "t = to_tensor(['[142676.0', '107.17', '71890.08', '0.0', '106.5]'])\n",
    "print(type(t), type(t[0]))\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_data(file):\n",
    "    \"\"\"\n",
    "    reads the stem word and the spatial tag of each token in the .csv file\n",
    "    :param corpus_file:\n",
    "    :param datafields:\n",
    "    :return: List of training data of the form [[tokenized_sentence-1, spatial_tensors],\n",
    "                                                [tokenized_sentence-1, spatial_tensors], ...]\n",
    "    \"\"\"\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        examples = []\n",
    "        words = []\n",
    "        lemmas = []\n",
    "        synset_offset = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                examples.append([lemmas, synset_offset, labels])\n",
    "                words = []\n",
    "                lemmas = []\n",
    "                synset_offset = []\n",
    "                labels = []\n",
    "            else:\n",
    "                columns = line.split()\n",
    "                words.append(columns[0])\n",
    "                lemmas.append(columns[1])\n",
    "                synset_offset.append(columns[-6])\n",
    "                lab = to_tensor(columns[-5:])\n",
    "                labels.append(lab)\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['have',\n",
       "   'the',\n",
       "   'necessary',\n",
       "   'means',\n",
       "   'or',\n",
       "   'skill',\n",
       "   'or',\n",
       "   'know-how',\n",
       "   'or',\n",
       "   'authority',\n",
       "   'to',\n",
       "   'do',\n",
       "   'something'],\n",
       "  ['no-synset',\n",
       "   'no-synset',\n",
       "   'necessary.a.01',\n",
       "   'means.n.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'know-how.n.01',\n",
       "   'no-synset',\n",
       "   'authority.n.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.1174e+05, 9.8310e+01, 9.8013e+04, 0.0000e+00, 1.8500e+01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([3.7587e+04, 1.0434e+02, 1.9497e+05, 0.0000e+00, 7.5000e+00]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['not',\n",
       "   'have',\n",
       "   'the',\n",
       "   'necessary',\n",
       "   'means',\n",
       "   'or',\n",
       "   'skill',\n",
       "   'or',\n",
       "   'know-how'],\n",
       "  ['not.r.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'necessary.a.01',\n",
       "   'means.n.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'know-how.n.01'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.1174e+05, 9.8310e+01, 9.8013e+04, 0.0000e+00, 1.8500e+01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02])]],\n",
       " [['have',\n",
       "   'or',\n",
       "   'be',\n",
       "   'many%3|more%3|much%3|more%4|much%4',\n",
       "   'than',\n",
       "   'normal',\n",
       "   'or',\n",
       "   'necessary',\n",
       "   ':'],\n",
       "  ['no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'normal.a.01',\n",
       "   'no-synset',\n",
       "   'necessary.a.01',\n",
       "   'no-synset'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.2872e+05, 2.0550e+01, 5.4535e+04, 7.8460e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['involve',\n",
       "   'deductive_reasoning',\n",
       "   'from',\n",
       "   'a',\n",
       "   'general',\n",
       "   'principle',\n",
       "   'to',\n",
       "   'a',\n",
       "   'necessary',\n",
       "   'effect'],\n",
       "  ['no-synset',\n",
       "   'deduction.n.04',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'general.a.01',\n",
       "   'principle.n.03',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'necessary.a.01',\n",
       "   'no-synset'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([4.9319e+04, 1.0515e+02, 1.6306e+05, 0.0000e+00, 1.5000e+00]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.1143e+05, 3.0800e+01, 8.3244e+04, 7.8460e+01, 5.0000e-01]),\n",
       "   tensor([1.1814e+05, 1.0754e+02, 9.5664e+04, 0.0000e+00, 1.4500e+01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['not', 'support', 'by', 'fact'],\n",
       "  ['not.r.01', 'corroborate.v.03', 'no-synset', 'no-synset'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.4906e+04, 1.0194e+02, 1.1614e+05, 1.8000e+02, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['not', 'support', 'by', 'fact'],\n",
       "  ['not.r.01', 'confirm.v.01', 'no-synset', 'no-synset'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.4906e+04, 1.0194e+02, 1.1483e+05, 1.8000e+02, 2.8500e+01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['lack', 'necessary', 'physical_ability', 'or', 'mental_ability'],\n",
       "  ['miss.v.06',\n",
       "   'necessary.a.01',\n",
       "   'physical_ability.n.01',\n",
       "   'no-synset',\n",
       "   'capacity.n.08'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([5.1520e+03, 1.1805e+02, 2.2714e+05, 0.0000e+00, 2.5000e+00]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([8.6015e+04, 1.0743e+02, 1.2885e+05, 0.0000e+00, 1.5000e+00])]],\n",
       " [['the', 'necessary', 'consequence', 'of', 'one', 'action'],\n",
       "  ['no-synset',\n",
       "   'necessary.s.02',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 7.6603e+04, 9.5740e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['have',\n",
       "   'every',\n",
       "   'necessary',\n",
       "   'or',\n",
       "   'normal',\n",
       "   'part',\n",
       "   'or',\n",
       "   'component',\n",
       "   'or',\n",
       "   'step'],\n",
       "  ['no-synset',\n",
       "   'no-synset',\n",
       "   'necessary.a.01',\n",
       "   'no-synset',\n",
       "   'normal.a.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.2872e+05, 2.0550e+01, 5.4535e+04, 7.8460e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['have',\n",
       "   'or',\n",
       "   'display',\n",
       "   'all',\n",
       "   'the',\n",
       "   'characteristic',\n",
       "   'necessary',\n",
       "   'for',\n",
       "   'completeness'],\n",
       "  ['no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'feature.n.01',\n",
       "   'necessary.a.01',\n",
       "   'no-synset',\n",
       "   'completeness.n.01'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([5.0349e+04, 1.0389e+02, 1.6340e+05, 0.0000e+00, 2.4500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.6165e+04, 1.1673e+02, 2.1134e+05, 0.0000e+00, 3.5000e+00])]],\n",
       " [['have',\n",
       "   'or',\n",
       "   'display',\n",
       "   'all',\n",
       "   'the',\n",
       "   'characteristic',\n",
       "   'necessary',\n",
       "   'for',\n",
       "   'completeness'],\n",
       "  ['no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'characteristic.n.02',\n",
       "   'necessary.a.01',\n",
       "   'no-synset',\n",
       "   'completeness.n.01'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([5.0349e+04, 1.0389e+02, 1.8358e+05, 0.0000e+00, 3.9500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.6165e+04, 1.1673e+02, 2.1134e+05, 0.0000e+00, 3.5000e+00])]],\n",
       " [['(',\n",
       "   'of',\n",
       "   'a',\n",
       "   'boat',\n",
       "   'or',\n",
       "   'vessel',\n",
       "   ')',\n",
       "   'furnished',\n",
       "   'with',\n",
       "   'necessary',\n",
       "   'official_document',\n",
       "   'specify',\n",
       "   'ownership',\n",
       "   'etc.'],\n",
       "  ['no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'boat.n.01',\n",
       "   'no-synset',\n",
       "   'vessel.n.02',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'necessary.a.01',\n",
       "   'legal_document.n.01',\n",
       "   'no-synset',\n",
       "   'ownership.n.03',\n",
       "   'and_so_forth.r.01'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([3.4035e+04, 7.8060e+01, 1.7478e+05, 0.0000e+00, 6.4500e+01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([6.9658e+04, 7.3010e+01, 1.3975e+05, 0.0000e+00, 1.8950e+02]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([7.7181e+04, 1.1327e+02, 1.4597e+05, 0.0000e+00, 2.8850e+02]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([6.2580e+03, 1.0677e+02, 2.2125e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([8.7896e+04, 1.7910e+02, 6.2122e+04, 0.0000e+00, 5.0000e-01])]],\n",
       " [['lack',\n",
       "   'necessary',\n",
       "   'document',\n",
       "   '(',\n",
       "   'as',\n",
       "   'for',\n",
       "   'e.g.',\n",
       "   'permission',\n",
       "   'to',\n",
       "   'live',\n",
       "   'or',\n",
       "   'work',\n",
       "   'in',\n",
       "   'a',\n",
       "   'country',\n",
       "   ')'],\n",
       "  ['miss.v.06',\n",
       "   'necessary.a.01',\n",
       "   'document.n.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'for_example.r.01',\n",
       "   'license.n.04',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'country.n.02',\n",
       "   'no-synset'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([6.4798e+04, 1.0888e+02, 1.5864e+05, 0.0000e+00, 3.9050e+02]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([6.6105e+04, 1.7943e+02, 8.3903e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.5885e+04, 1.0822e+02, 1.6395e+05, 0.0000e+00, 1.3500e+01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.4370e+03, 9.3970e+01, 2.0489e+05, 0.0000e+00, 2.2750e+02]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['lack', 'necessary', 'force', 'for', 'effectiveness'],\n",
       "  ['miss.v.06',\n",
       "   'necessary.a.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'effectiveness.n.01'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3004e+05, 1.1761e+02, 1.0131e+05, 0.0000e+00, 2.5000e+00])]],\n",
       " [['use',\n",
       "   'the',\n",
       "   'minimum',\n",
       "   'of',\n",
       "   'time',\n",
       "   'or',\n",
       "   'resource',\n",
       "   'necessary',\n",
       "   'for',\n",
       "   'effectiveness'],\n",
       "  ['no-synset',\n",
       "   'no-synset',\n",
       "   'minimum.n.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'resource.n.01',\n",
       "   'necessary.a.01',\n",
       "   'no-synset',\n",
       "   'effectiveness.n.01'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([8.8864e+04, 7.5350e+01, 1.4785e+05, 5.3130e+01, 1.5000e+00]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.8408e+04, 1.0795e+02, 1.8866e+05, 0.0000e+00, 1.2500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3004e+05, 1.1761e+02, 1.0131e+05, 0.0000e+00, 2.5000e+00])]],\n",
       " [['possible', 'but', 'not', 'necessary'],\n",
       "  ['possible.a.01', 'no-synset', 'not.r.01', 'necessary.a.01'],\n",
       "  [tensor([8.5479e+04, 6.0380e+01, 1.2985e+05, 9.5740e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " [['leave', 'to', 'personal', 'choice'],\n",
       "  ['no-synset', 'no-synset', 'personal.a.01', 'choice.n.02'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([2.7140e+04, 1.9770e+01, 1.2471e+05, 2.5840e+01, 5.0000e-01]),\n",
       "   tensor([1.4028e+05, 7.7850e+01, 1.0613e+05, 6.0000e+01, 5.9500e+01])]],\n",
       " [['morally', 'bind', 'or', 'necessary'],\n",
       "  ['morally.r.01', 'no-synset', 'no-synset', 'necessary.a.01'],\n",
       "  [tensor([2.4772e+04, 1.7990e+02, 1.2524e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " [['urgently', 'need'],\n",
       "  ['urgently.r.01', 'need.v.03'],\n",
       "  [tensor([2.6208e+04, 1.7894e+02, 1.2382e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([8.9570e+03, 1.0177e+02, 7.9864e+04, 1.8000e+02, 5.0000e-01])]],\n",
       " [['absolutely', 'necessary'],\n",
       "  ['absolutely.r.02', 'necessary.a.01'],\n",
       "  [tensor([8.1604e+04, 1.7865e+02, 6.8438e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = parse_data(path + train_path)\n",
    "# validation_data = parse_data(path + validate_path)\n",
    "# testing_data = parse_data(path + test_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data: [lemma, synset offset, spatial label]\n",
    "# remove all the words, where synset offset is 'no-synset'\n",
    "\n",
    "def clean_untagged(data):\n",
    "    original_data = data\n",
    "    for entry in data:\n",
    "        \n",
    "        idx = [i for i, syn in enumerate(entry[1]) if syn == 'no-synset']\n",
    "\n",
    "        # remove those from the data\n",
    "        for s in reversed(idx):\n",
    "            del entry[0][s]\n",
    "            del entry[1][s]\n",
    "            del entry[2][s]\n",
    "\n",
    "    return original_data, data\n",
    "\n",
    "orig, data = clean_untagged(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['necessary', 'means', 'know-how', 'authority'],\n",
       "  ['necessary.a.01', 'means.n.01', 'know-how.n.01', 'authority.n.01'],\n",
       "  [tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.1174e+05, 9.8310e+01, 9.8013e+04, 0.0000e+00, 1.8500e+01]),\n",
       "   tensor([1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02]),\n",
       "   tensor([3.7587e+04, 1.0434e+02, 1.9497e+05, 0.0000e+00, 7.5000e+00])]],\n",
       " [['not', 'necessary', 'means', 'know-how'],\n",
       "  ['not.r.01', 'necessary.a.01', 'means.n.01', 'know-how.n.01'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.1174e+05, 9.8310e+01, 9.8013e+04, 0.0000e+00, 1.8500e+01]),\n",
       "   tensor([1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02])]],\n",
       " [['normal', 'necessary'],\n",
       "  ['normal.a.01', 'necessary.a.01'],\n",
       "  [tensor([1.2872e+05, 2.0550e+01, 5.4535e+04, 7.8460e+01, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " [['deductive_reasoning', 'general', 'principle', 'necessary'],\n",
       "  ['deduction.n.04', 'general.a.01', 'principle.n.03', 'necessary.a.01'],\n",
       "  [tensor([4.9319e+04, 1.0515e+02, 1.6306e+05, 0.0000e+00, 1.5000e+00]),\n",
       "   tensor([1.1143e+05, 3.0800e+01, 8.3244e+04, 7.8460e+01, 5.0000e-01]),\n",
       "   tensor([1.1814e+05, 1.0754e+02, 9.5664e+04, 0.0000e+00, 1.4500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " [['not', 'support'],\n",
       "  ['not.r.01', 'corroborate.v.03'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.4906e+04, 1.0194e+02, 1.1614e+05, 1.8000e+02, 5.0000e-01])]],\n",
       " [['not', 'support'],\n",
       "  ['not.r.01', 'confirm.v.01'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.4906e+04, 1.0194e+02, 1.1483e+05, 1.8000e+02, 2.8500e+01])]],\n",
       " [['lack', 'necessary', 'physical_ability', 'mental_ability'],\n",
       "  ['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([5.1520e+03, 1.1805e+02, 2.2714e+05, 0.0000e+00, 2.5000e+00]),\n",
       "   tensor([8.6015e+04, 1.0743e+02, 1.2885e+05, 0.0000e+00, 1.5000e+00])]],\n",
       " [['necessary'],\n",
       "  ['necessary.s.02'],\n",
       "  [tensor([1.3514e+05, 2.5010e+01, 7.6603e+04, 9.5740e+01, 5.0000e-01])]],\n",
       " [['necessary', 'normal'],\n",
       "  ['necessary.a.01', 'normal.a.01'],\n",
       "  [tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.2872e+05, 2.0550e+01, 5.4535e+04, 7.8460e+01, 5.0000e-01])]],\n",
       " [['characteristic', 'necessary', 'completeness'],\n",
       "  ['feature.n.01', 'necessary.a.01', 'completeness.n.01'],\n",
       "  [tensor([5.0349e+04, 1.0389e+02, 1.6340e+05, 0.0000e+00, 2.4500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.6165e+04, 1.1673e+02, 2.1134e+05, 0.0000e+00, 3.5000e+00])]],\n",
       " [['characteristic', 'necessary', 'completeness'],\n",
       "  ['characteristic.n.02', 'necessary.a.01', 'completeness.n.01'],\n",
       "  [tensor([5.0349e+04, 1.0389e+02, 1.8358e+05, 0.0000e+00, 3.9500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.6165e+04, 1.1673e+02, 2.1134e+05, 0.0000e+00, 3.5000e+00])]],\n",
       " [['boat', 'vessel', 'necessary', 'official_document', 'ownership', 'etc.'],\n",
       "  ['boat.n.01',\n",
       "   'vessel.n.02',\n",
       "   'necessary.a.01',\n",
       "   'legal_document.n.01',\n",
       "   'ownership.n.03',\n",
       "   'and_so_forth.r.01'],\n",
       "  [tensor([3.4035e+04, 7.8060e+01, 1.7478e+05, 0.0000e+00, 6.4500e+01]),\n",
       "   tensor([6.9658e+04, 7.3010e+01, 1.3975e+05, 0.0000e+00, 1.8950e+02]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([7.7181e+04, 1.1327e+02, 1.4597e+05, 0.0000e+00, 2.8850e+02]),\n",
       "   tensor([6.2580e+03, 1.0677e+02, 2.2125e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([8.7896e+04, 1.7910e+02, 6.2122e+04, 0.0000e+00, 5.0000e-01])]],\n",
       " [['lack', 'necessary', 'document', 'e.g.', 'permission', 'country'],\n",
       "  ['miss.v.06',\n",
       "   'necessary.a.01',\n",
       "   'document.n.01',\n",
       "   'for_example.r.01',\n",
       "   'license.n.04',\n",
       "   'country.n.02'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([6.4798e+04, 1.0888e+02, 1.5864e+05, 0.0000e+00, 3.9050e+02]),\n",
       "   tensor([6.6105e+04, 1.7943e+02, 8.3903e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.5885e+04, 1.0822e+02, 1.6395e+05, 0.0000e+00, 1.3500e+01]),\n",
       "   tensor([1.4370e+03, 9.3970e+01, 2.0489e+05, 0.0000e+00, 2.2750e+02])]],\n",
       " [['lack', 'necessary', 'effectiveness'],\n",
       "  ['miss.v.06', 'necessary.a.01', 'effectiveness.n.01'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.3004e+05, 1.1761e+02, 1.0131e+05, 0.0000e+00, 2.5000e+00])]],\n",
       " [['minimum', 'resource', 'necessary', 'effectiveness'],\n",
       "  ['minimum.n.01', 'resource.n.01', 'necessary.a.01', 'effectiveness.n.01'],\n",
       "  [tensor([8.8864e+04, 7.5350e+01, 1.4785e+05, 5.3130e+01, 1.5000e+00]),\n",
       "   tensor([1.8408e+04, 1.0795e+02, 1.8866e+05, 0.0000e+00, 1.2500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.3004e+05, 1.1761e+02, 1.0131e+05, 0.0000e+00, 2.5000e+00])]],\n",
       " [['possible', 'not', 'necessary'],\n",
       "  ['possible.a.01', 'not.r.01', 'necessary.a.01'],\n",
       "  [tensor([8.5479e+04, 6.0380e+01, 1.2985e+05, 9.5740e+01, 5.0000e-01]),\n",
       "   tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " [['personal', 'choice'],\n",
       "  ['personal.a.01', 'choice.n.02'],\n",
       "  [tensor([2.7140e+04, 1.9770e+01, 1.2471e+05, 2.5840e+01, 5.0000e-01]),\n",
       "   tensor([1.4028e+05, 7.7850e+01, 1.0613e+05, 6.0000e+01, 5.9500e+01])]],\n",
       " [['morally', 'necessary'],\n",
       "  ['morally.r.01', 'necessary.a.01'],\n",
       "  [tensor([2.4772e+04, 1.7990e+02, 1.2524e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " [['urgently', 'need'],\n",
       "  ['urgently.r.01', 'need.v.03'],\n",
       "  [tensor([2.6208e+04, 1.7894e+02, 1.2382e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([8.9570e+03, 1.0177e+02, 7.9864e+04, 1.8000e+02, 5.0000e-01])]],\n",
       " [['absolutely', 'necessary'],\n",
       "  ['absolutely.r.02', 'necessary.a.01'],\n",
       "  [tensor([8.1604e+04, 1.7865e+02, 6.8438e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def data_id(data):\n",
    "\n",
    "    # data_collector = {\"0\": [[], []], \"1\": [[],[]], ...}\n",
    "    data_collector = {}\n",
    "    for i, instance in enumerate(data):\n",
    "        data_collector[str(i)] = instance\n",
    "\n",
    "    return data_collector\n",
    "\n",
    "# in my case, this data must be shuffled before continuing!\n",
    "datasetID = data_id(data)\n",
    "# datasetID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': [['necessary', 'means', 'know-how', 'authority'],\n",
       "  ['necessary.a.01', 'means.n.01', 'know-how.n.01', 'authority.n.01'],\n",
       "  [tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.1174e+05, 9.8310e+01, 9.8013e+04, 0.0000e+00, 1.8500e+01]),\n",
       "   tensor([1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02]),\n",
       "   tensor([3.7587e+04, 1.0434e+02, 1.9497e+05, 0.0000e+00, 7.5000e+00])]],\n",
       " '1': [['not', 'necessary', 'means', 'know-how'],\n",
       "  ['not.r.01', 'necessary.a.01', 'means.n.01', 'know-how.n.01'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.1174e+05, 9.8310e+01, 9.8013e+04, 0.0000e+00, 1.8500e+01]),\n",
       "   tensor([1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02])]],\n",
       " '2': [['normal', 'necessary'],\n",
       "  ['normal.a.01', 'necessary.a.01'],\n",
       "  [tensor([1.2872e+05, 2.0550e+01, 5.4535e+04, 7.8460e+01, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " '3': [['deductive_reasoning', 'general', 'principle', 'necessary'],\n",
       "  ['deduction.n.04', 'general.a.01', 'principle.n.03', 'necessary.a.01'],\n",
       "  [tensor([4.9319e+04, 1.0515e+02, 1.6306e+05, 0.0000e+00, 1.5000e+00]),\n",
       "   tensor([1.1143e+05, 3.0800e+01, 8.3244e+04, 7.8460e+01, 5.0000e-01]),\n",
       "   tensor([1.1814e+05, 1.0754e+02, 9.5664e+04, 0.0000e+00, 1.4500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " '4': [['not', 'support'],\n",
       "  ['not.r.01', 'corroborate.v.03'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.4906e+04, 1.0194e+02, 1.1614e+05, 1.8000e+02, 5.0000e-01])]],\n",
       " '5': [['not', 'support'],\n",
       "  ['not.r.01', 'confirm.v.01'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.4906e+04, 1.0194e+02, 1.1483e+05, 1.8000e+02, 2.8500e+01])]],\n",
       " '6': [['lack', 'necessary', 'physical_ability', 'mental_ability'],\n",
       "  ['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([5.1520e+03, 1.1805e+02, 2.2714e+05, 0.0000e+00, 2.5000e+00]),\n",
       "   tensor([8.6015e+04, 1.0743e+02, 1.2885e+05, 0.0000e+00, 1.5000e+00])]],\n",
       " '7': [['necessary'],\n",
       "  ['necessary.s.02'],\n",
       "  [tensor([1.3514e+05, 2.5010e+01, 7.6603e+04, 9.5740e+01, 5.0000e-01])]],\n",
       " '8': [['necessary', 'normal'],\n",
       "  ['necessary.a.01', 'normal.a.01'],\n",
       "  [tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.2872e+05, 2.0550e+01, 5.4535e+04, 7.8460e+01, 5.0000e-01])]],\n",
       " '9': [['characteristic', 'necessary', 'completeness'],\n",
       "  ['feature.n.01', 'necessary.a.01', 'completeness.n.01'],\n",
       "  [tensor([5.0349e+04, 1.0389e+02, 1.6340e+05, 0.0000e+00, 2.4500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.6165e+04, 1.1673e+02, 2.1134e+05, 0.0000e+00, 3.5000e+00])]],\n",
       " '10': [['characteristic', 'necessary', 'completeness'],\n",
       "  ['characteristic.n.02', 'necessary.a.01', 'completeness.n.01'],\n",
       "  [tensor([5.0349e+04, 1.0389e+02, 1.8358e+05, 0.0000e+00, 3.9500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.6165e+04, 1.1673e+02, 2.1134e+05, 0.0000e+00, 3.5000e+00])]],\n",
       " '11': [['boat',\n",
       "   'vessel',\n",
       "   'necessary',\n",
       "   'official_document',\n",
       "   'ownership',\n",
       "   'etc.'],\n",
       "  ['boat.n.01',\n",
       "   'vessel.n.02',\n",
       "   'necessary.a.01',\n",
       "   'legal_document.n.01',\n",
       "   'ownership.n.03',\n",
       "   'and_so_forth.r.01'],\n",
       "  [tensor([3.4035e+04, 7.8060e+01, 1.7478e+05, 0.0000e+00, 6.4500e+01]),\n",
       "   tensor([6.9658e+04, 7.3010e+01, 1.3975e+05, 0.0000e+00, 1.8950e+02]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([7.7181e+04, 1.1327e+02, 1.4597e+05, 0.0000e+00, 2.8850e+02]),\n",
       "   tensor([6.2580e+03, 1.0677e+02, 2.2125e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([8.7896e+04, 1.7910e+02, 6.2122e+04, 0.0000e+00, 5.0000e-01])]],\n",
       " '12': [['lack', 'necessary', 'document', 'e.g.', 'permission', 'country'],\n",
       "  ['miss.v.06',\n",
       "   'necessary.a.01',\n",
       "   'document.n.01',\n",
       "   'for_example.r.01',\n",
       "   'license.n.04',\n",
       "   'country.n.02'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([6.4798e+04, 1.0888e+02, 1.5864e+05, 0.0000e+00, 3.9050e+02]),\n",
       "   tensor([6.6105e+04, 1.7943e+02, 8.3903e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.5885e+04, 1.0822e+02, 1.6395e+05, 0.0000e+00, 1.3500e+01]),\n",
       "   tensor([1.4370e+03, 9.3970e+01, 2.0489e+05, 0.0000e+00, 2.2750e+02])]],\n",
       " '13': [['lack', 'necessary', 'effectiveness'],\n",
       "  ['miss.v.06', 'necessary.a.01', 'effectiveness.n.01'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.3004e+05, 1.1761e+02, 1.0131e+05, 0.0000e+00, 2.5000e+00])]],\n",
       " '14': [['minimum', 'resource', 'necessary', 'effectiveness'],\n",
       "  ['minimum.n.01', 'resource.n.01', 'necessary.a.01', 'effectiveness.n.01'],\n",
       "  [tensor([8.8864e+04, 7.5350e+01, 1.4785e+05, 5.3130e+01, 1.5000e+00]),\n",
       "   tensor([1.8408e+04, 1.0795e+02, 1.8866e+05, 0.0000e+00, 1.2500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.3004e+05, 1.1761e+02, 1.0131e+05, 0.0000e+00, 2.5000e+00])]],\n",
       " '15': [['possible', 'not', 'necessary'],\n",
       "  ['possible.a.01', 'not.r.01', 'necessary.a.01'],\n",
       "  [tensor([8.5479e+04, 6.0380e+01, 1.2985e+05, 9.5740e+01, 5.0000e-01]),\n",
       "   tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " '16': [['personal', 'choice'],\n",
       "  ['personal.a.01', 'choice.n.02'],\n",
       "  [tensor([2.7140e+04, 1.9770e+01, 1.2471e+05, 2.5840e+01, 5.0000e-01]),\n",
       "   tensor([1.4028e+05, 7.7850e+01, 1.0613e+05, 6.0000e+01, 5.9500e+01])]],\n",
       " '17': [['morally', 'necessary'],\n",
       "  ['morally.r.01', 'necessary.a.01'],\n",
       "  [tensor([2.4772e+04, 1.7990e+02, 1.2524e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " '18': [['urgently', 'need'],\n",
       "  ['urgently.r.01', 'need.v.03'],\n",
       "  [tensor([2.6208e+04, 1.7894e+02, 1.2382e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([8.9570e+03, 1.0177e+02, 7.9864e+04, 1.8000e+02, 5.0000e-01])]],\n",
       " '19': [['absolutely', 'necessary'],\n",
       "  ['absolutely.r.02', 'necessary.a.01'],\n",
       "  [tensor([8.1604e+04, 1.7865e+02, 6.8438e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create dict of key-synset\n",
    "train_val_syn = {}\n",
    "train_val_syn.fromkeys(datasetID.keys())\n",
    "for key, ele in datasetID.items():\n",
    "    train_val_syn[key] = ele[1]\n",
    "# train_val_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': ['necessary.a.01', 'means.n.01', 'know-how.n.01', 'authority.n.01'],\n",
       " '1': ['not.r.01', 'necessary.a.01', 'means.n.01', 'know-how.n.01'],\n",
       " '2': ['normal.a.01', 'necessary.a.01'],\n",
       " '3': ['deduction.n.04', 'general.a.01', 'principle.n.03', 'necessary.a.01'],\n",
       " '4': ['not.r.01', 'corroborate.v.03'],\n",
       " '5': ['not.r.01', 'confirm.v.01'],\n",
       " '6': ['miss.v.06',\n",
       "  'necessary.a.01',\n",
       "  'physical_ability.n.01',\n",
       "  'capacity.n.08'],\n",
       " '7': ['necessary.s.02'],\n",
       " '8': ['necessary.a.01', 'normal.a.01'],\n",
       " '9': ['feature.n.01', 'necessary.a.01', 'completeness.n.01'],\n",
       " '10': ['characteristic.n.02', 'necessary.a.01', 'completeness.n.01'],\n",
       " '11': ['boat.n.01',\n",
       "  'vessel.n.02',\n",
       "  'necessary.a.01',\n",
       "  'legal_document.n.01',\n",
       "  'ownership.n.03',\n",
       "  'and_so_forth.r.01'],\n",
       " '12': ['miss.v.06',\n",
       "  'necessary.a.01',\n",
       "  'document.n.01',\n",
       "  'for_example.r.01',\n",
       "  'license.n.04',\n",
       "  'country.n.02'],\n",
       " '13': ['miss.v.06', 'necessary.a.01', 'effectiveness.n.01'],\n",
       " '14': ['minimum.n.01',\n",
       "  'resource.n.01',\n",
       "  'necessary.a.01',\n",
       "  'effectiveness.n.01'],\n",
       " '15': ['possible.a.01', 'not.r.01', 'necessary.a.01'],\n",
       " '16': ['personal.a.01', 'choice.n.02'],\n",
       " '17': ['morally.r.01', 'necessary.a.01'],\n",
       " '18': ['urgently.r.01', 'need.v.03'],\n",
       " '19': ['absolutely.r.02', 'necessary.a.01']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# partition data in training/validation\n",
    "splittings = {}\n",
    "labels = {}\n",
    "labels.fromkeys(datasetID.keys())\n",
    "for key, ele in datasetID.items():\n",
    "    labels[key] = ele[2]\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_train = 10\n",
    "N_valid = 5\n",
    "N_test = 5\n",
    "# choose N training instances, randomly!\n",
    "splittings[\"train\"] = random.sample(list(datasetID), N_train)\n",
    "splittings[\"validate\"] = random.sample(list(set(datasetID) - set(splittings[\"train\"])), N_valid)\n",
    "# splittings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Saving and loading data to/from .pt\n",
    "# save\n",
    "#torch.save(datasetID, path + \"pwngc_id.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['necessary', 'means', 'know-how', 'authority']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = torch.load(path + \"pwngc_id.pt\")[\"0\"][0]\n",
    "li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Preparing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, list_ids, labels, train_val_syn):\n",
    "        self.labels = labels\n",
    "        self.list_ids = list_ids\n",
    "        self.train_val_syn = train_val_syn\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Total Number of samples.\"\n",
    "\n",
    "        return len(self.list_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Extracts one Example of data.\"\n",
    "\n",
    "        id = self.list_ids[index]\n",
    "\n",
    "        # data\n",
    "        X = torch.load(path + \"pwngc_id.pt\")[id][0]\n",
    "        y = self.labels[id]\n",
    "        tag_y = self.train_val_syn[id]\n",
    "\n",
    "        return X, y, tag_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Downloading GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I downloaded it from torchtext\n",
    "glove_path = \"./.vector_cache\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#import bcolz\n",
    "# # initial code from: https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n",
    "## list of words in GloVe\n",
    "#words = []\n",
    "## index of words in GloVe\n",
    "#idx = 0\n",
    "## assign an index to each word in GloVe to ease embedding while training\n",
    "## e.g. {\"Hi\":0, \"the\":1, ...}\n",
    "##word2idx = {}\n",
    "\n",
    "# #I downloaded it from torchtext\n",
    "#glove_path = \"./.vector_cache\"\n",
    "\n",
    "#vectors = bcolz.carray(np.zeros((1,300), dtype=np.float32), rootdir=f'{glove_path}/840B.300d.dat', mode='w')\n",
    "\n",
    "#glove = {}\n",
    "\n",
    "# with open(f'{glove_path}/glove.840B.300d.txt', 'rb') as f:\n",
    "#     for l in f:\n",
    "#         try:\n",
    "#             line = l.decode().split()\n",
    "#             word = line[0]\n",
    "            \n",
    "#             #word2idx[word] = idx\n",
    "#             #idx += 1\n",
    "#             vec = np.array(line[1:], dtype=np.float32)\n",
    "#             # print(vec)\n",
    "#             vectors.append(vec)\n",
    "#             words.append(word)\n",
    "#             #print(word)\n",
    "#             idx += 1\n",
    "#             print(idx)\n",
    "#             glove[word] = vec\n",
    "#             #idx += 1\n",
    "#         except:\n",
    "#             f.__next__()\n",
    "#             #words = [\". . . . .\", \". . .\", \"at name@domain.com\"]\n",
    "#             #splits = [len(word.split()) for word in words]\n",
    "#             #for i, j in enumerate(splits):\n",
    "#             #    if line[0: j] == words[i].split() and isinstance(line[j], np.float32):\n",
    "#             #        words.append(words[i])\n",
    "#              #       word2idx[words[i]] = idx\n",
    "#               #      #idx += 1\n",
    "#                #     vec = np.array(line[j:], dtype=np.float32)\n",
    "#                 #    vectors.append(vec)\n",
    "#                  #   idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(vectors.shape)\n",
    "# wordy = [\". . . . .\", \". . .\", \"at name@domain.com\"]\n",
    "# w = wordy[2].split()\n",
    "# print(w)\n",
    "# #word2idx['.\\xa0.\\xa0.']\n",
    "#\n",
    "# print(\". . .\" in words)\n",
    "# print(\". . . . .\" in words) # False, because it transforms '.' to float, then checks if it is a float!\n",
    "# print(\"at name@domain.com\" in words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(len(words))\n",
    "# print(len(vectors))\n",
    "# print(len(glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# From their website: https://nlp.stanford.edu/projects/glove/\n",
    "# Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download)\n",
    "# vectors = bcolz.carray(vectors[1:], rootdir=f'{glove_path}/840B.300.dat', mode='w')\n",
    "# vectors.flush()\n",
    "# pickle.dump(words, open(f'{glove_path}/840B.300_words.pkl', 'wb'))\n",
    "# #pickle.dump(word2idx, open(f'{glove_path}/840B.300_idx.pkl', 'wb'))\n",
    "# pickle.dump(glove, open(f'{glove_path}/840B.300_glove.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vectors = bcolz.open(f'{glove_path}/840B.300.dat')[:]\n",
    "# words = pickle.load(open(f'{glove_path}/840B.300_words.pkl', 'rb'))\n",
    "# #word2idx = pickle.load(open(f'{glove_path}/840B.300_idx.pkl', 'rb'))\n",
    "# #glove = {w: vectors[word2idx[w]] for w in words}\n",
    "glove = pickle.load(open(f'{glove_path}/840B.300_glove.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2195846\n"
     ]
    }
   ],
   "source": [
    "print(len(glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_vocab(data, embed_size=300):\n",
    "    \n",
    "    # insert all dataset vocabulary\n",
    "    dataset_vocab = []\n",
    "    if isinstance(data[0], str):\n",
    "        dataset_vocab = data\n",
    "    else:\n",
    "        for instance in data:            \n",
    "            dataset_vocab += instance[0]\n",
    "        \n",
    "    # print(len(dataset_vocab))\n",
    "    \n",
    "    # remove duplicates\n",
    "    target_vocab = set(dataset_vocab)\n",
    "    \n",
    "    # generate weights matrix using glove\n",
    "    matrix_len = len(target_vocab)\n",
    "    \n",
    "    weights_matrix = np.zeros((matrix_len, embed_size))\n",
    "    \n",
    "    words_found = 0\n",
    "\n",
    "    for i, word in enumerate(target_vocab):\n",
    "        #print(i, word)\n",
    "        try:\n",
    "            weights_matrix[i] = glove[word]\n",
    "            #print(weights_matrix[i])\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            weights_matrix[i] = np.random.normal(scale=0.6, size=(embed_size, ))\n",
    "            #print(weights_matrix[i])\n",
    "    #print(words_found)\n",
    "    \n",
    "    return target_vocab, weights_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preparing Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate the Dataset in dataloader\n",
    "# Define the model\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 5, #64,\n",
    "          'shuffle': True,\n",
    "          'collate_fn': lambda x: x,\n",
    "          'num_workers': 0}#6}\n",
    "max_epochs = 10 #100\n",
    "\n",
    "# Datasets\n",
    "# partition = # IDs\n",
    "# labels = # Labels\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(splittings['train'], labels, train_val_syn)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(splittings['validate'], labels, train_val_syn)\n",
    "validation_generator = torch.utils.data.DataLoader(validation_set, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "<class 'list'>\n",
      "['necessary', 'means', 'know-how', 'authority'] [tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]), tensor([1.1174e+05, 9.8310e+01, 9.8013e+04, 0.0000e+00, 1.8500e+01]), tensor([1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02]), tensor([3.7587e+04, 1.0434e+02, 1.9497e+05, 0.0000e+00, 7.5000e+00])] ['necessary.a.01', 'means.n.01', 'know-how.n.01', 'authority.n.01']\n",
      "5\n",
      "<class 'list'>\n",
      "['not', 'support'] [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]), tensor([4.4906e+04, 1.0194e+02, 1.1483e+05, 1.8000e+02, 2.8500e+01])] ['not.r.01', 'confirm.v.01']\n"
     ]
    }
   ],
   "source": [
    "for batch in training_generator:\n",
    "    print(len(batch))\n",
    "    print(type(batch))\n",
    "    for local_batch, local_label, syn in batch:\n",
    "        print(local_batch,local_label, syn)\n",
    "        break\n",
    "        #print(local_batch, local_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def numericalize(tokens_list, vocab):\n",
    "    \n",
    "    str2num = {word: index for index, word in enumerate(vocab)}\n",
    "    num_list = []\n",
    "    for token in tokens_list:\n",
    "        num_list.append(str2num[token])\n",
    "        \n",
    "    return torch.tensor(num_list, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Create the target vocab dataframe containing senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spatial_wordnet = pd.read_pickle(\"../data/wordnet_dataframes/SPATIAL_WORDNET.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spatial_wordnet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add a column denoting the word-sense coordinates based on [l0, alpha, l_i, beta_i, radius]\n",
    "\n",
    "def decode_params(spatial_params):\n",
    "    l0 = spatial_params[0]\n",
    "    alpha = spatial_params[1]\n",
    "    alpha_rad = alpha * np.pi / 180\n",
    "    l_i = spatial_params[2]\n",
    "    beta_i = spatial_params[3]\n",
    "    beta_i_rad = beta_i * np.pi / 180\n",
    "    r = spatial_params[4]\n",
    "    return l0, alpha, alpha_rad, l_i, beta_i, beta_i_rad, r\n",
    "\n",
    "\n",
    "def point_in_space(spatial_params):\n",
    "    l0, alpha, alpha_rad, l_i, beta_i, beta_i_rad, r = decode_params(spatial_params)\n",
    "    # np.cos() and np.sin() take angles in radian as params\n",
    "    center_pt = np.array([l0*np.cos(alpha_rad), l0 * np.sin(alpha_rad)])\n",
    "    sense_pt = center_pt + np.array([l_i * np.cos(alpha_rad + beta_i_rad),\n",
    "                                     l_i * np.sin(alpha_rad + beta_i_rad)])\n",
    "    return sense_pt, center_pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spatial_wordnet[\"sense_coo\"] = spatial_wordnet.apply(lambda row: point_in_space([row.l0, row.alpha, row.l_i, row.beta_i, row.radius])[0], axis=1)\n",
    "spatial_wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop the rows of verb_root, adjective_root, and adverb_root\n",
    "spatial_wordnet  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arr = list(spatial_wordnet.sense_coo)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ex = np.random.rand(300) \n",
    "ex[0] += arr[0]\n",
    "ex[1] += arr[1]\n",
    "print(ex)\n",
    "len(ex)\n",
    "#normalize\n",
    "no = ex/np.linalg.norm(ex)\n",
    "no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng()\n",
    "numbers = rng.choice(20, size=10, replace=False)\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cpp = list(zip(spatial_wordnet.l0, spatial_wordnet.alpha, spatial_wordnet.l_i, spatial_wordnet.beta_i, spatial_wordnet.radius))\n",
    "type(cpp[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now I need to transform this dataset to a target vocab\n",
    "# How is a target vocab defined?\n",
    "# each word/synset? has a number (it can be the index of in the dataframe)\n",
    "# each of word has a Glove vector\n",
    "# add the point to it\n",
    "# add the sense coordinate\n",
    "# normalize\n",
    "def embed_target_vocab(df, embed_size=300):\n",
    "    \n",
    "    # store (word, synset)-tuples in a list\n",
    "    word_synset = list(zip(df.word, df.synset))\n",
    "    \n",
    "    # store indices in a list to numericalize the vocabulary of wordnet\n",
    "    indices = list(df.index)\n",
    "    \n",
    "    senses_coo = list(df.sense_coo)\n",
    "    \n",
    "    spatial_tags = list(zip(df.l0, df.alpha, df.l_i, df.beta_i, df.radius))\n",
    "    \n",
    "    \n",
    "    # generate weights matrix using glove\n",
    "    matrix_len = len(word_synset)\n",
    "    \n",
    "    weights_matrix = np.zeros((matrix_len, embed_size))\n",
    "    \n",
    "    words_found = 0\n",
    "\n",
    "    for i, word in enumerate(word_synset):\n",
    "        #print(i, word)\n",
    "        try:\n",
    "            weights_matrix[i] = glove[word[0]]\n",
    "            print(\"found GLOVE :) \")\n",
    "            #print(weights_matrix[i])\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            weights_matrix[i] = np.random.normal(scale=0.6, size=(embed_size, ))\n",
    "            print(\"Random Vector\")\n",
    "            #print(weights_matrix[i])\n",
    "    #print(words_found)\n",
    "    \n",
    "    # add the parameters to the sense vector\n",
    "    # add them in a way, that no all of them are in the same part of the 300-dim Glove vector\n",
    "    # Why?\n",
    "    # because this will help learn some patterns in the backpropagation, instead of adding all vectors to the first two columns\n",
    "    # only\n",
    "    # the randomness for choosing the indices to add the coordinates could result in better diversity, and thus better learning\n",
    "    # choose radom integers without replacement\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    x_y_indices = rng.choice(embed_size, size=matrix_len*2, replace=True)\n",
    "    x_col = x_y_indices[:matrix_len]\n",
    "    y_col = x_y_indices[matrix_len:]\n",
    "    \n",
    "    \n",
    "    sense_matrix = np.copy(weights_matrix)\n",
    "    for i, coo in enumerate(senses_coo):\n",
    "        sense_matrix[i][x_col[i]] += coo[0]\n",
    "        sense_matrix[i][y_col[i]] += coo[1]\n",
    "        # normalize\n",
    "        sense_matrix[i] = sense_matrix[i] / np.linalg.norm(sense_matrix[i])\n",
    "    \n",
    "\n",
    "    return weights_matrix, sense_matrix, word_synset, spatial_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# glove_wordnet, sense_matrix, word_synset_VOCAB, spatial_tags = embed_target_vocab(spatial_wordnet, embed_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.save(\"target_sense_matrix_exp_01.npy\", sense_matrix)\n",
    "\n",
    "# # to load:\n",
    "# np.load(\"target_sense_matrix_exp_01.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.save(\"glove_wordnet_matrix_exp_01.npy\", glove_wordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.save(\"WORDNET_VOCAB_exp_01.npy\", word_synset_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.save(\"WORDNET_SPATIAL_TAGS_exp_01.npy\", spatial_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Choosing the argmax sense from softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227733, 300)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsense_matrix = np.load(\"target_sense_matrix_exp_01.npy\")\n",
    "tsense_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227733, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['.22-caliber', '.22_caliber.a.01'],\n",
       "       ['.22-calibre', '.22_caliber.a.01']], dtype='<U76')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_VOCAB = np.load(\"WORDNET_VOCAB_exp_01.npy\")\n",
    "print(target_VOCAB.shape)\n",
    "target_VOCAB[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227733, 5)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPATIAL_TAGS = np.load(\"WORDNET_SPATIAL_TAGS_exp_01.npy\")\n",
    "SPATIAL_TAGS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def inside_sphere(point, sphere_coo):\n",
    "\n",
    "    pt = point_in_space(point)\n",
    "    sphere_sense, sphere_center = point_in_space(sphere_coo)\n",
    "\n",
    "    sphere_rad = sphere_coo[-1] # in angles\n",
    "\n",
    "    contained = (pt[0] - sphere_sense[0])**2 + (pt[1] - sphere_sense[1])**2 <= sphere_rad**2\n",
    "\n",
    "    if contained:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def distance_loss(pred_pt, original_pt, include_r=False):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two sense points, including radii.\n",
    "    :param pred_pt:\n",
    "    :param original_pt:\n",
    "    :param include_r: if set to true, include radius in the distance. \n",
    "                      It gives more freedom/tolerance degrees to the loss function. \n",
    "                      Loss is satisfied once the predicted point is part of original point.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    r1 = pred_pt[-1]\n",
    "    r2 = original_pt[-1]\n",
    "    pred_sense, pred_center = point_in_space(pred_pt)\n",
    "    orig_sense, orig_center = point_in_space(original_pt)\n",
    "    \n",
    "    loss = np.linalg.norm(pred_sense - orig_sense)\n",
    "    tolerant_loss = r1 + loss - r2\n",
    "    if loss < 0:\n",
    "        tolerant_loss = 0\n",
    "    \n",
    "    if include_r:\n",
    "        return tolerant_loss\n",
    "    else:\n",
    "        return loss \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sphere_dist(pred_pt, original_pt):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two 2D spheres.\n",
    "    :param pred_pt:\n",
    "    :param original_pt:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pred_sense, pred_center = point_in_space(pred_pt)\n",
    "    pred_radius = pred_pt[-1]\n",
    "    orig_sense, orig_center = point_in_space(original_pt)\n",
    "    orig_radius = original_pt[-1]\n",
    "\n",
    "    return (pred_radius + orig_radius -\n",
    "            np.linalg.norm(pred_sense - orig_sense))\n",
    "\n",
    "def decode_prediction(spatial_params, df=\"SPATIAL_WORDNET.pickle\") -> [str]:\n",
    "    \"\"\"\n",
    "    Projects the predicted spatial parameters into the embedding space.\n",
    "    Returns the synsets in the vacinity of the projected point.\n",
    "    :param spatial_params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    synsets = [] # sort from specific to most general\n",
    "\n",
    "    sense_pt, center_pt = point_in_space(spatial_params)\n",
    "\n",
    "    spatial_df = pd.read_pickle(df)\n",
    "    # get the spheres, where the point/point+radius is contained/overlaping/near\n",
    "\n",
    "    # 1. check if the predicted point is contained in some sense\n",
    "    spatial_df[\"contained\"] = spatial_df.apply(lambda row:\n",
    "                                               inside_sphere(spatial_params,\n",
    "                                                             row[['l0', 'alpha', 'l_i', 'beta_i', 'radius']]))\n",
    "\n",
    "    # 2. For those synsets, which is the nearest synset point\n",
    "    #use distance() to calculate distance between centers\n",
    "\n",
    "    # 3. If None of the synsets apply to that word sense\n",
    "    # use sphere_dist to find the nearest sphere (most general synset), and assign it to that synset\n",
    "    # (this maybe good for rare senses)\n",
    "\n",
    "\n",
    "    return synsets\n",
    "\n",
    "def train_loss(tmp_pred, synset_params):\n",
    "    # Loss is the distance between the two spheres/containment of the word within that sphere\n",
    "    # radius acts as tolerance!\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    weights_matrix = torch.from_numpy(weights_matrix)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# emb, n, d = create_emb_layer(weights_matrix)\n",
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokens = torch.tensor([0,5,9], dtype=torch.long)\n",
    "# len(emb(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: np.ndarray, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self, weights_matrix:np.ndarray, \n",
    "                 ntoken: int, out_features:int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.weights_matrix = weights_matrix\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(self.weights_matrix, True)\n",
    "        \n",
    "        # Multi-head attention mechanism is included in TransformerEncoderLayer\n",
    "        # d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=<function relu>, \n",
    "        # layer_norm_eps=1e-05, batch_first=False, norm_first=False, device=None, dtype=None\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout) # activation\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers, norm=None)\n",
    "        \n",
    "        \n",
    "#         padding_idx (int, optional) – If specified, the entries at padding_idx do not contribute to the gradient;\n",
    "#         therefore, the embedding vector at padding_idx is not updated during training,\n",
    "#         i.e. it remains as a fixed “pad”. For a newly constructed Embedding, the embedding vector at\n",
    "#         padding_idx will default to all zeros, but can be updated to another value to be used as the padding vector.\n",
    "        self.emb = nn.Embedding(ntoken, d_model) \n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Linear layer: returns the last hidden state of the encoder \n",
    "        self.fc = nn.Linear(d_model, embedding_dim)\n",
    "        \n",
    "        # No! Here I am just redoing fully connected connections\n",
    "        # Linear Layer: affine transformation of last hidden layer into shape (1, embedding_dim)\n",
    "        #self.context_vec = nn.Linear(d_model, embedding_dim)\n",
    "        \n",
    "        #self.decoder = nn.Linear(d_model, ntoken)\n",
    "        \n",
    "        # Now, I need to have a Linear space that takes the whole/subset dataframe as input, extracts its spatial_context_vec,\n",
    "        # based on Glove-word-vector + spatial_point,\n",
    "        # then calculates softmax on this distribution\n",
    "        # choose the argmax\n",
    "        # get its spatial tags\n",
    "        # calculate distance loss between them\n",
    "        # do backprop! \n",
    "        # Nx300 into Nx227733: matmul product of two matrices Nx300 and 300x227733 --> Nx227733\n",
    "        # apply softmax to get the probabilities\n",
    "        # apply argmax to get the maximum indices\n",
    "        # use the indices to get the synset names as well as the mapping to coordinates\n",
    "        # into Nx5: mapping to the coordinates\n",
    "        \n",
    "        self.output = nn.Linear(embedding_dim, 5)\n",
    "        #self.wn_embeddings = nn.Linear(1, target_matrix.shape[0])\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "#         weights_matrix = weights_matrix, \n",
    "#                                     ntoken= # false: 300,\n",
    "#                                     out_features=5,\n",
    "#                                     d_model=300,\n",
    "#                                     d_hid=200,\n",
    "#                                     nlayers=2,\n",
    "#                                     nhead=2,\n",
    "#                                     dropout=0.2\n",
    "        \n",
    "        \n",
    "        # -------------------------------------\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        \"initialize weights using uniform distribution\"\n",
    "        initrange = 0.1\n",
    "        self.emb.weight.data.uniform_(-initrange, initrange)\n",
    "        # self.decoder.bias.data.zero_()\n",
    "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        #self.output.bias.data.zero_()\n",
    "        #self.output.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        \n",
    "        #src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = torch.mul(self.emb(src), math.sqrt(self.d_model)) #? 1/sqrt!\n",
    "#         print(\"Embedding\", src.shape)\n",
    "#         print('-' * 80)\n",
    "        \n",
    "        \n",
    "        src = self.pos_encoder(src)\n",
    "#         print(\"Positional Encoding\", src.shape)\n",
    "#         print('-' * 80)\n",
    "        \n",
    "        \n",
    "        encoder_output = self.transformer_encoder(src) #, src_mask)\n",
    "#         print(\"Encoder\", encoder_output.shape)\n",
    "#         # print(encoder_output)\n",
    "#         print('-' * 80)\n",
    "        \n",
    "        \n",
    "        linear_layer = self.fc(encoder_output)\n",
    "#         print(\"Linear Layer\", linear_layer.shape)\n",
    "#         # print(linear_layer)\n",
    "#         print('-' * 80)\n",
    "\n",
    "        # calculate the sum/weighted sum/ ?? on the linear layer to get the context vector of size (1, embd_dim)\n",
    "        context_vec = torch.sum(linear_layer, dim=1)\n",
    "#         print(\"Final Context Vector\", context_vec.shape)\n",
    "#         # print(context_vec)\n",
    "#         print('-' * 80)\n",
    "        \n",
    "        # regression output\n",
    "        coordinates = self.output(context_vec)\n",
    "#         print(\"Coordinates from Context Vector\", coordinates.size())\n",
    "#         # print(coordinates)\n",
    "#         print('-'*80)\n",
    "        return coordinates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Geometric Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def coo2point(coo):\n",
    "    # print(coo)\n",
    "    l0 = coo[0]\n",
    "    alpha = coo[1]\n",
    "    alpha_rad = alpha * math.pi / 180\n",
    "    l_i = coo[2]\n",
    "    beta_i = coo[3]\n",
    "    beta_i_rad = beta_i * math.pi / 180\n",
    "    r = coo[4]\n",
    "    \n",
    "    # np.cos() and np.sin() take angles in radian as params\n",
    "    center_pt = torch.tensor([l0 * math.cos(alpha_rad), l0 * math.sin(alpha_rad)], dtype=torch.float64, requires_grad=True)\n",
    "    sense_pt = center_pt + torch.tensor([l_i * math.cos(alpha_rad + beta_i_rad),\n",
    "                                     l_i * math.sin(alpha_rad + beta_i_rad)], dtype=torch.float64, requires_grad=True)\n",
    "    return sense_pt, center_pt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def distance_loss(pred_pt, original_pt, include_r=False, pt_sphere=False):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two sense points, including radii.\n",
    "    :param pred_pt:\n",
    "    :param original_pt:\n",
    "    :param include_r: if set to true, include radius in the distance. \n",
    "                      It gives more freedom/tolerance degrees to the loss function. \n",
    "                      Loss is satisfied once the predicted point is part of original point.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "        \n",
    "    # original_pt = torch.from_numpy(original_pt)\n",
    "    # print(\"original point\", type(original_pt), original_pt)\n",
    "    \n",
    "    r1 = pred_pt[-1]\n",
    "    r2 = original_pt[-1]\n",
    "\n",
    "    pred_sense, pred_center = coo2point(pred_pt)\n",
    "    orig_sense, orig_center = coo2point(original_pt)\n",
    "    \n",
    "    \n",
    "    loss = torch.linalg.norm(torch.sub(pred_sense, orig_sense)) - r2\n",
    "    \n",
    "    # very strong assumption for the words that are not sense-tagged\n",
    "    # If I want more tolerance, I could neglect those tokens from the beginning\n",
    "    if torch.all(torch.eq(original_pt, torch.zeros(original_pt.size(0))), dim=0):\n",
    "        return loss\n",
    "    \n",
    "    if pt_sphere:\n",
    "        dist = torch.linalg.norm(torch.sub(pred_sense, orig_sense)) + r2\n",
    "        return dist\n",
    "\n",
    "    \n",
    "    if include_r:\n",
    "        \n",
    "        tolerant_loss = r1 + loss - r2\n",
    "    \n",
    "        if tolerant_loss < 0:\n",
    "            tolerant_loss = 0.0\n",
    "        \n",
    "#         if r1 > r2: #case the predicted radius is bigger than actual one\n",
    "#             tolerant_loss = torch.abs(torch.sub(r1, r2))\n",
    "           \n",
    "        return tolerant_loss\n",
    "    \n",
    "    else:\n",
    "        return loss \n",
    "   \n",
    "\n",
    "\n",
    "def geometric_loss(pred_list, label_list, include_r=False):\n",
    "    \n",
    "    # assert that the two lists must be of equal size\n",
    "    pred_size = pred_list.size()[0]\n",
    "    lab_size = label_list.size()[0]\n",
    "    assert pred_size == lab_size\n",
    "    \n",
    "    sentence_loss = 0.0\n",
    "    \n",
    "    # sum over all the tokens in the sentence\n",
    "    for i in range(pred_size):\n",
    "        sentence_loss += distance_loss(pred_list[i], label_list[i], include_r)\n",
    "        \n",
    "    return sentence_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sense Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_contained(pred, sphere_coo, compare_spheres=False):\n",
    "\n",
    "    pt, word = coo2point(pred)\n",
    "    sphere_sense, sphere_center = coo2point(sphere_coo)\n",
    "\n",
    "    pt_rad = pred[-1]\n",
    "    sphere_rad = sphere_coo[-1] # in angles\n",
    "    \n",
    "    \n",
    "    \n",
    "    if compare_spheres == False:\n",
    "        contained = (pt[0] - sphere_sense[0])**2 + (pt[1] - sphere_sense[1])**2 <= sphere_rad**2\n",
    "    else:\n",
    "        contained = pt_rad + torch.linalg.norm(pt - sphere_sense) - sphere_rad <= 0\n",
    "\n",
    "    if contained:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "def vicinity_matrix(spatial_params, target_vocab: np.ndarray, spatial_tags: np.ndarray, k=5):#, include_sphere=True, include_r=True) -> [str]:\n",
    "    \"\"\"\n",
    "    Projects the predicted spatial parameters into the embedding space.\n",
    "    Returns the synsets in the vicinity of the projected point.\n",
    "    :param spatial_params:\n",
    "    :return: Vicinity matrix, synsets dict\n",
    "    \"\"\"\n",
    "    N = len(spatial_tags)\n",
    "    \n",
    "    #convert spatial_tags to tensor\n",
    "    spatial_tags = torch.from_numpy(spatial_tags)\n",
    "    \n",
    "    synsets = {} # sort from most specific to most general\n",
    "    \n",
    "    indices = {}\n",
    "\n",
    "    sense_pt, center_pt = coo2point(spatial_params)\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------------------------\n",
    "    # Prepare distance and containment calculations\n",
    "    # ----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # distance calculations\n",
    "    dist_spheres = torch.empty(N) \n",
    "    dist_pt_sphere = torch.empty(N) \n",
    "    dist_pts = torch.empty(N)\n",
    "    \n",
    "    for i, tag in enumerate(spatial_tags):\n",
    "        dist_spheres[i] = distance_loss(spatial_params, tag, include_r=True)\n",
    "        dist_pt_sphere[i] = distance_loss(spatial_params, tag, pt_sphere=True)\n",
    "        dist_pts[i] = distance_loss(spatial_params, tag, include_r=False)\n",
    "    \n",
    "    # containment calculations\n",
    "    full_contained = torch.empty(N) \n",
    "    part_contained = torch.empty(N)\n",
    "    disconnected = torch.empty(N) # handles points only\n",
    "    \n",
    "    for j, tag in enumerate(spatial_tags):\n",
    "        full_contained[j] = is_contained(spatial_params, tag, compare_spheres=True)\n",
    "        part_contained[j] = distance_loss(spatial_params, tag, include_r=True) > 0\n",
    "        disconnected[j] = ~ is_contained(spatial_params, tag, compare_spheres=True) # reverse the True <----> False\n",
    "    \n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Initialize the Vicinity Matrix\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    # row=3, col=3, topk=2, 2 indicates the column of indices and the distances\n",
    "    vicinity_matrix = torch.zeros((3,3, k, 2))\n",
    "    \n",
    "    ####################################################################################################################\n",
    "    # # Full contained + min dist between sense points\n",
    "    ####################################################################################################################\n",
    "    \n",
    "#     print(\"True elements\")\n",
    "    true_indices1 = (full_contained == True).nonzero(as_tuple=True)[0]\n",
    "#     print(true_indices1)\n",
    "    \n",
    "    if true_indices1.size(0) != 0:\n",
    "        dist1 = torch.index_select(dist_pts, 0, true_indices1)\n",
    "#         print(\"dist1\", dist1)\n",
    "#         print(\"k = \", k)\n",
    "        # sort in ascending order\n",
    "        # select top k \n",
    "        sort_dist1, sort_indices = torch.topk(dist1, k, largest=False)  \n",
    "#         print(\"SORTING\", sort_dist1, sort_indices)\n",
    "        synsets1 = np.take(target_vocab, sort_indices, 0)\n",
    "        synsets[\"A\"] = [synsets1, sort_dist1]\n",
    "        indices[\"A\"] = sort_indices\n",
    "        # index, distance (without synsets because this would result in conflicts for torch.tensor that do not support str)\n",
    "        vicinity_matrix[2][0] = torch.stack((sort_indices, sort_dist1), dim=1)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    ####################################################################################################################\n",
    "    # # Partially contained + min dist between sense points\n",
    "    ####################################################################################################################\n",
    "    true_indices2 = (part_contained == True).nonzero(as_tuple=True)[0]\n",
    "#     print(\"True Indices 2\", true_indices2)\n",
    "    \n",
    "    if true_indices2.size(0) != 0:\n",
    "        dist1 = torch.index_select(dist_pts, 0, true_indices2)\n",
    "        # sort in ascending order\n",
    "        # select top k \n",
    "        sort_dist2, sort_indices2 = torch.topk(dist1, k, largest=False)     \n",
    "        synsets2 = np.take(target_vocab, sort_indices2, 0)\n",
    "#         print(\"synset 2\", synsets2)\n",
    "        synsets[\"B\"] = [synsets2, sort_dist2]\n",
    "        indices[\"B\"] = sort_indices2\n",
    "        # index, distance (without synsets because this would result in conflicts for torch.tensor that do not support str)\n",
    "        vicinity_matrix[2][1] = torch.stack((sort_indices2, sort_dist2), dim=1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # # Disconnected + min dist between spheres/point2sphere/sense points ---> acts as Nearest neighbor\n",
    "    ####################################################################################################################\n",
    "    # get indices, where disconnected is true\n",
    "    true_indices3 = (disconnected == True).nonzero(as_tuple=True)[0]\n",
    "#     print(\"True Indices 3\", true_indices3)\n",
    "\n",
    "    if true_indices3.size(0) != 0:\n",
    "        # get the distances at those indices\n",
    "        dist_spheres3 = torch.index_select(dist_spheres, 0, true_indices3)\n",
    "        dist_pt_sphere3 = torch.index_select(dist_pt_sphere, 0, true_indices3)\n",
    "        dist_pts3 = torch.index_select(dist_pts, 0, true_indices3)\n",
    "\n",
    "        # sort-select top k minimum distances\n",
    "        sort_dist_spheres3, sort_sph_indices3 = torch.topk(dist_spheres3, k, largest=False)\n",
    "        sort_dist_pt_sphere3, sort_pt_sph_indices3 = torch.topk(dist_pt_sphere3, k, largest=False)\n",
    "        sort_dist_pts3, sort_pts_indices3 = torch.topk(dist_pts3, k, largest=False)\n",
    "\n",
    "        # get their corresponding synsets\n",
    "        synsets30 = np.take(target_vocab, sort_sph_indices3, 0)\n",
    "        #print(\"synset30\", synsets30)\n",
    "        synsets[\"C\"] = [synsets30, sort_dist_spheres3]\n",
    "        indices[\"C\"] = sort_sph_indices3\n",
    "        \n",
    "        synsets31 = np.take(target_vocab, sort_pt_sph_indices3, 0)\n",
    "        synsets[\"D\"] = [synsets31, sort_dist_pt_sphere3]\n",
    "        indices[\"D\"] = sort_pt_sph_indices3\n",
    "        \n",
    "        synsets32 = np.take(target_vocab, sort_pts_indices3, 0)\n",
    "        synsets[\"E\"] = [synsets32, sort_dist_pts3]\n",
    "        indices[\"E\"] = sort_pts_indices3\n",
    "        \n",
    "        # insert them into the vicinity matrix    \n",
    "        vicinity_matrix[0][3] = torch.stack((sort_sph_indices3, sort_dist_spheres3), dim=1)\n",
    "        vicinity_matrix[1][3] = torch.stack((sort_pt_sph_indices3, sort_dist_pt_sphere3), dim=1)\n",
    "        vicinity_matrix[2][3] = torch.stack((sort_pts_indices3, sort_dist_pts3), dim=1)  \n",
    "    \n",
    "\n",
    "\n",
    "#     # get the spheres, where the point/point+radius is contained/overlaping/near\n",
    "\n",
    "#     # 1. check if the predicted point is contained in some sense\n",
    "#     contained = torch.empty(N)\n",
    "    \n",
    "#     for i, tag in enumerate(spatial_tags):\n",
    "#         contained[i] = is_contained(spatial_params, tag, compare_spheres=include_sphere)\n",
    "    \n",
    "#     # 2. For those synsets, which is the nearest synset point\n",
    "#     #use distance() to calculate distance between centers\n",
    "#     distances = torch.empty(N)\n",
    "#     for i, tag in enumerate(spatial_tags):\n",
    "#         distances[i] = distance_loss(spatial_params, tag, include_r=include_r)\n",
    "    \n",
    "#     # sort dist--> indices\n",
    "#     # check if for those distances the containment is true\n",
    "#     # if true: choose the one having min_dist as sense\n",
    "#     # top k senses must be stored in a dict \n",
    "    \n",
    "#     # check if for those distances the containment is false, then, only the radius is falsly predicted (not priority now)\n",
    "#     # if false and min_dist: choose it as potential sense\n",
    "    \n",
    "    \n",
    "\n",
    "#     # 3. If None of the synsets apply to that word sense\n",
    "#     # use sphere_dist to find the nearest sphere (most general synset), and assign it to that synset\n",
    "#     # (this maybe good for rare senses)\n",
    "#     # acts as a second chance\n",
    "#     rare_contained = torch.empty(N)\n",
    "#     rare_distances = torch.empty(N)\n",
    "#     for i, tag in enumerate(spatial_tags):\n",
    "#         rare_contained[i] = is_contained(spatial_params, tag, compare_spheres=False) #only consider sense point\n",
    "#         rare_distances[i] = distance_loss(spatial_params, tag, include_r=False)\n",
    "\n",
    "\n",
    "    return indices, vicinity_matrix, synsets\n",
    "\n",
    "def decode_key(key, mtx):\n",
    "    if key == \"A\":\n",
    "        return mtx[2, 0]\n",
    "    if key == \"B\":\n",
    "        return mtx[2, 1]\n",
    "    if key == \"C\":\n",
    "        return mtx[0, 2]\n",
    "    if key == \"D\":\n",
    "        return mtx[1, 2]\n",
    "    if key == \"E\":\n",
    "        return mtx[2, 2]\n",
    "    \n",
    "\n",
    "def label_in_vicinity(vicinity_matrix, vicinity_synsets, target_vocab, spatial_tags, true_label):\n",
    "    \n",
    "    checked_synsets = []\n",
    "    contained = []\n",
    "    checks = 0\n",
    "    predicted = []\n",
    "    distances = []\n",
    "    \n",
    "    in_vicinity = False\n",
    "    associated_syn = []\n",
    "    \n",
    "    # true label is either one of the possibilities [word, synset] or a randomly chosen one\n",
    "    \n",
    "    # induce subset of word-synset name \n",
    "    \n",
    "    #spatial_tags = torch.from_numpy(spatial_tags)\n",
    "    #idx_label = (spatial_tags == true_label).nonzero(as_tuple=True)[0]\n",
    "    # transform to numpy to \n",
    "    true_label = np.array(true_label, dtype=np.float64)\n",
    "    # keep spatial tag an np.ndarray\n",
    "    rounded_l = np.round(true_label, decimals=2)\n",
    "    \n",
    "    if np.all(rounded_l == np.zeros(5)): #true_label): #torch.all(torch.eq(rounded_l, true_label)):\n",
    "        in_vicinity = False #True\n",
    "        associated_syn.append('no-synset')\n",
    "        return in_vicinity, associated_syn\n",
    "    \n",
    "    try:\n",
    "        # detecting the true label from the spatial_tags\n",
    "        idx = [[np.array_equal(rounded_l, tag) for tag in spatial_tags].index(True)]\n",
    "#         print(\"Found {} matching word-synset tags.\".format(len(idx)))\n",
    "        word_synset = target_vocab[idx] #list of list \n",
    "#         print(\"Matching word-synset\", word_synset)\n",
    "        # check if word_synset is within the vicinity matrix\n",
    "        if len(word_synset) != 0:\n",
    "            for e in word_synset:\n",
    "                for key, val in vicinity_synsets.items():\n",
    "#                     print(\"Searching in vicinity ... \")\n",
    "\n",
    "#                     print(\"Checking if true label is in vicinity ...\")\n",
    "                    checked_synsets.append(e)\n",
    "                    is_there = e[1] in val[:, 1]\n",
    "                    checks += 1\n",
    "                    contained.append(is_there)\n",
    "                    \n",
    "#                     print(\"1\")\n",
    "#                     print(checked_synsets)\n",
    "#                     print(checks)\n",
    "#                     print(contained)\n",
    "                    \n",
    "                    if is_there:\n",
    "#                         print(\"The main true label <{}> is in the vacinity of the predicted tag.\".format(e))\n",
    "                        idx_e = np.where(val[:, 1] == e[1])\n",
    "                        predicted.append(val[idx_e])\n",
    "#                         print(\"Predicted 1: \", predicted)\n",
    "                        distances.append(decode_key(key, vicinity_matrix)[idx_e][1])\n",
    "#                         print(\"Distances 1: \", distances)\n",
    "                    else:\n",
    "#                         print(\"The main true label is not in vicinity ... \")\n",
    "                        distances.append('no-distance')\n",
    "#                         print(\"Searching if alternative true label synsets are in vicinity ... \")\n",
    "                    # induce all the word-synset tuples that have same synset as true label.\n",
    "                    # This double check is necessary since I choose the spatial tags in the training data randomly sometimes.\n",
    "                    # get indices of all word-synsets sharing same synset (not same word)\n",
    "                    ix = np.where(target_vocab == [_, e[1]])[0] # add [0] to indicate only the row index, not the column\n",
    "#                     print(\"Indices \", ix)\n",
    "                    if len(ix) != 0:\n",
    "                        pos_syn = target_vocab[ix]\n",
    "                        \n",
    "#                         print(\"Possible synsets: \", pos_syn)\n",
    "#                         print(target_vocab[:10])\n",
    "                        for t in pos_syn:\n",
    "                            checks += 1\n",
    "                            checked_synsets.append(t)\n",
    "                            is_near = t[1] in val[:, -1]\n",
    "                            contained.append(is_near)\n",
    "#                             print(\"2\")\n",
    "#                             print(checked_synsets)\n",
    "#                             print(checks)\n",
    "#                             print(contained)\n",
    "                            if is_near == True:                                    \n",
    "#                                 print(\"... The word-synset <{}> is in the vicinity of the predicted tag.\".format(t))\n",
    "                                idx_t = np.where(val[:, -1] == t[1])\n",
    "                                predicted.append(val[idx_t])\n",
    "#                                 print(\"Predicted 2: \", predicted)\n",
    "                                distances.append(decode_key(key, vicinity_matrix)[idx_t][1])\n",
    "#                                 print(\"Distances 2: \", distances)\n",
    "                            else:\n",
    "                                distances.append('no-distance')\n",
    "                    else: \n",
    "                        print(\"... There are no other possibilites for word-synset <{}>\".format(e))\n",
    "                            \n",
    "        else:\n",
    "            print(\"Cannot find the suitable synset of this spatial tag!\")\n",
    "\n",
    "        \n",
    "    except ValueError as ve:\n",
    "        print(ve)\n",
    "#         print(\"Found no index for the true label. Something went wrong ...\")\n",
    "#         print(\"Comparing <true label = {}> with <rounded label = {}>\".format(true_label, rounded_l))\n",
    "    \n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Statistics\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "#     print(\"~\" * 80)\n",
    "#     print(\"Statistics\")\n",
    "#     print(\"~\" * 80)\n",
    "    \n",
    "#     print(\"Predicted Spatial Tag = \", spatial_params)\n",
    "#     print(\"Checked Spatial Tag(s) ; contained? ; Predicted ; distances = ({}):\".format(len(checked_synsets)))\n",
    "    for s, c, p, d in zip(checked_synsets, contained, predicted, distances):\n",
    "        print(s, \";\", c, \";\", \"\\n\", p, \";\", d)\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "#     print(\"True Spatial Tag(s) is in vicinity of predicted tag: \", contained)\n",
    "    contained_idx = np.where(np.array(contained) == True)\n",
    "    \n",
    "#     print(\"contained_idx\", contained_idx)\n",
    "#     print(\"checked_idx\", np.array(checked_synsets)[contained_idx])\n",
    "#     print(\"slice\", np.array(checked_synsets)[:, 1])\n",
    "#     print(\"check_slice\", np.array(checked_synsets)[:, 1][contained_idx])\n",
    "\n",
    "    if len(contained_idx[0]) > 0:\n",
    "#         print()\n",
    "#         print(contained_idx)\n",
    "        only_syn = set(np.array(checked_synsets)[contained_idx])#[:, 1])\n",
    "        associated_syn.append(only_syn)\n",
    "#         print(\"True Sense Tag(s) = ({}) --> \".format(len(only_syn)), only_syn)\n",
    "#         print(\"Prediction is correct!\")\n",
    "        in_vicinity = True\n",
    "#         print(\"Distance(predicted_sense, nearest_true_sense) = ({}): \".format(len(np.array(predicted)[contained_idx])))\n",
    "#         for p, d in zip(np.array(predicted), distances):\n",
    "#               print(p, d)\n",
    "              \n",
    "    else:\n",
    "#         print(\"Prediction is false ..\")\n",
    "#         print(\"All synsets in the vicinity of the predicted tag are not true senses ..\")\n",
    "#         print(\"Please check manually if the synsets in the vicinity are generalizations of the true labels.\")\n",
    "        in_vicinity = False\n",
    "        associated_syn.append(\"no-synset\")\n",
    "    \n",
    "    \n",
    "    return in_vicinity, associated_syn\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s=np.array([1,4,2,8,3,2])\n",
    "contained_idx = np.where(s == 9)\n",
    "print(type(contained_idx), contained_idx[0])\n",
    "print(len(contained_idx[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    'Counts the parameters of the model to allow comparision between different models.'\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def spatial_stats(pred, original, zeros_tags=True):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 6. Training/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I need to split all training data beforehand\n",
    "\n",
    "class RegTagger:\n",
    "    \n",
    "    def __init__(self, use_cuda, device):\n",
    "        self.use_cuda = use_cuda\n",
    "        self.device = device\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "        \n",
    "    def train(self, batch_size: int, num_workers: int, max_epochs: int, \n",
    "              splittings: dict, labels: list, train_val_syn: list, data: list, embed_size: int,\n",
    "              target_vocab: list, spatial_tags: list,\n",
    "              k=5,\n",
    "              d_model=300, d_hid=200, nlayers=2, nhead=2, dropout=0.2,\n",
    "              lr=5.0, gamma=0.95,\n",
    "              shuffle=True):\n",
    "        \n",
    "        # create batches\n",
    "        \n",
    "        # parameters\n",
    "        params = {'batch_size': batch_size, #64,\n",
    "                  'shuffle': shuffle,\n",
    "                  'collate_fn': lambda x: x,\n",
    "                  'num_workers': num_workers} #6} #set 0 if training on Windows machine\n",
    "\n",
    "        # Training and validation data generators\n",
    "        training_set = Dataset(splittings['train'], labels, train_val_syn)\n",
    "        training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "\n",
    "        validation_set = Dataset(splittings['validate'], labels, train_val_syn)\n",
    "        validation_generator = torch.utils.data.DataLoader(validation_set, **params)\n",
    "\n",
    "        # -------------------------------------------------\n",
    "    \n",
    "        # history to store the losses\n",
    "        history = defaultdict(list)\n",
    "\n",
    "        VOCAB, weights_matrix = load_vocab(data, embed_size=embed_size)\n",
    "\n",
    "        # target_VOCAB\n",
    "        # SPATIAL_TAGS\n",
    "\n",
    "    \n",
    "\n",
    "        #######################################################################################################################\n",
    "        #        Count sentences and number of words in training and validation datasets to normalize the loss\n",
    "        #######################################################################################################################\n",
    "        nb_words_training = 0\n",
    "        nb_train_sentences = 0\n",
    "        nb_words_validation = 0\n",
    "\n",
    "        for batch in training_generator:\n",
    "            for sentence, label, syn in batch:\n",
    "                nb_train_sentences += 1\n",
    "                nb_words_training += len(sentence)\n",
    "\n",
    "        for batch in validation_generator:\n",
    "            for sentence, label, syn in batch:\n",
    "                nb_words_validation += len(sentence)\n",
    "\n",
    "#         print(\"Count results:\")\n",
    "#         print(\"nb_words_training = {}\".format(nb_words_training))\n",
    "#         print(\"nb_train_sentences = {}\".format(nb_train_sentences))\n",
    "#         print(\"nb_words_validation = {}\".format(nb_words_validation))\n",
    "\n",
    "#         print(params[\"batch_size\"])\n",
    "        n_batches = np.ceil(nb_train_sentences / batch_size)\n",
    "#         print(\"ceiling\", n_batches)\n",
    "\n",
    "        mean_words = nb_words_training / n_batches\n",
    "#         print(\"mean_words\", mean_words)\n",
    "\n",
    "\n",
    "        self.model = TransformerEncoderRegressor(weights_matrix = weights_matrix, \n",
    "                                            ntoken= len(VOCAB), #300,\n",
    "                                            out_features=5,\n",
    "                                            d_model=d_model,\n",
    "                                            d_hid=d_hid,\n",
    "                                            nlayers=nlayers,\n",
    "                                            nhead=nhead,\n",
    "                                            dropout=dropout)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        #                       Optimizer\n",
    "        # ---------------------------------------------------------------------\n",
    "        # criterion = nn.CrossEntropyLoss()\n",
    "        criterion = nn.MSELoss()\n",
    "#             lr = 5.0  # learning rate\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=gamma)\n",
    "        # -------\n",
    "\n",
    "\n",
    "        # Loop over epochs\n",
    "        for epoch in range(max_epochs):\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            loss_sum = 0\n",
    "\n",
    "            self.model.train()\n",
    "\n",
    "            # for transformer\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "            print(\"Training ...\")\n",
    "            # Training\n",
    "            for batch in training_generator:\n",
    "#                 print(\"New Batch for Training\")\n",
    "#                 print(\"#\" * 100)\n",
    "\n",
    "                for local_batch, local_labels, local_synsets in batch:\n",
    "\n",
    "                    # Transform list(<string>) to Tensor(<Tensor>)\n",
    "#                     print(\"Input Sentence:\")\n",
    "#                     print(local_batch)\n",
    "                    input_words = local_batch\n",
    "                    local_batch = numericalize(local_batch, VOCAB)\n",
    "#                     print(type(local_batch), local_batch)\n",
    "\n",
    "\n",
    "                    # Transform List(<Tensor>) to Tensor(<Tensor>)\n",
    "                    # I have labels of same length --> this should be no problem for Tensor\n",
    "                    local_labels = torch.stack(local_labels)\n",
    "#                     print(\"Labels:\")\n",
    "#                     print(local_synsets)\n",
    "#                     print(type(local_labels), len(local_labels), type(local_labels[0]))\n",
    "#                     print(local_labels)\n",
    "\n",
    "                    # Transfer to GPU\n",
    "                    local_batch, local_labels = local_batch.to(self.device), local_labels.to(self.device)\n",
    "\n",
    "                    # Model computations\n",
    "                    # out outputs the indices of wordnet database\n",
    "                    out = self.model(local_batch)\n",
    "#                     print(\"Model's Output\")\n",
    "#                     print(type(out), out.shape)\n",
    "                    # print(out)\n",
    "                    # predicted synsets\n",
    "#                     print(\"Current Predictions based on vacinity of prediction\")\n",
    "#                     print(\"*\" * 100)\n",
    "#                     print(\"*\" * 100)\n",
    "\n",
    "\n",
    "                    # ntokens = len(VOCAB)#300\n",
    "                    loss = geometric_loss(out, local_labels) / mean_words\n",
    "                    # criterion(out.view(-1), local_labels.view(-1))\n",
    "#                     print(\"Loss\")\n",
    "#                     print(type(loss), loss.size())\n",
    "#                     print(loss)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    # I added this\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "                    # ---\n",
    "                    optimizer.step()\n",
    "                    loss_sum += loss.item()\n",
    "#                     print(\"Loss Sum\", loss_sum)\n",
    "\n",
    "\n",
    "                    train_loss = loss_sum / len(local_batch)\n",
    "                    history['train_loss'].append(train_loss)\n",
    "#                     print(history)\n",
    "#                     print(len(history['train_loss']))\n",
    "\n",
    "\n",
    "            # Evaluate on the validation set.\n",
    "            # evaluate every 1 step:\n",
    "\n",
    "            print(\"Validation ...\")\n",
    "            vloss_sum = 0\n",
    "            if epoch % 1 == 0:\n",
    "\n",
    "                correct_sense = 0\n",
    "                sense_accuracy = 0\n",
    "\n",
    "                # set model to eval mode to ignore updating the weights of the model\n",
    "                self.model.eval()\n",
    "\n",
    "                # do not calculate gradients while evaluating\n",
    "                with torch.set_grad_enabled(False):\n",
    "\n",
    "                    for batch in validation_generator:\n",
    "                        print(\"New Batch for Validation\")\n",
    "                        print(\"#\" * 100)\n",
    "\n",
    "                        for local_batch, local_labels, local_synsets in batch:\n",
    "\n",
    "                            # Transform list(<string>) to Tensor(<Tensor>)\n",
    "                            print(\"Input Sentence\")\n",
    "                            print(local_batch)\n",
    "                            input_words = local_batch\n",
    "                            local_batch = numericalize(local_batch, VOCAB)\n",
    "        #                     print(type(local_batch), local_batch)\n",
    "\n",
    "\n",
    "                            # Transform List(<Tensor>) to Tensor(<Tensor>)\n",
    "                            # I have labels of same length --> this should be no problem for Tensor\n",
    "                            local_labels = torch.stack(local_labels)\n",
    "                            print(\"Labels:\")\n",
    "                            print(local_synsets)\n",
    "        #                     print(\"Labels\")\n",
    "        #                     print(type(local_labels), len(local_labels), type(local_labels[0]))\n",
    "        #                     print(local_labels)\n",
    "\n",
    "                            # Transfer to GPU\n",
    "                            local_batch, local_labels = local_batch.to(self.device), local_labels.to(self.device)\n",
    "\n",
    "                            # Model computations\n",
    "                            # out outputs the indices of wordnet database\n",
    "                            out = self.model(local_batch)\n",
    "\n",
    "                            # During validation and testing, I want to be less strict.\n",
    "                            # So, if a point resides within the label sphere, the sense is correctly identified.\n",
    "                            loss = geometric_loss(out, local_labels, include_r=True)\n",
    "\n",
    "                            vloss_sum += loss.item()                  \n",
    "\n",
    "                            validation_loss = vloss_sum / len(local_batch)\n",
    "                            history['validation_loss'].append(validation_loss)\n",
    "#                             print(history)\n",
    "#                             print(len(history['validation_loss']))\n",
    "\n",
    "                            correct_sense_batch = 0\n",
    "#                             print(\"Initializing the corrext sense batch = {}\".format(correct_sense_batch))\n",
    "\n",
    "                            true_pred = []\n",
    "                            predicted_synsets = []\n",
    "\n",
    "                            for i, word_tag in enumerate(out):\n",
    "#                                 print(\"i = \", i)\n",
    "#                                 print(\"+\"*150)\n",
    "#                                 print(\"word_tag = \", word_tag.size())\n",
    "#                                 print(word_tag)\n",
    "#                                 print(\"+\"*150)\n",
    "\n",
    "                                vindices, vmat, vsyn = vicinity_matrix(spatial_params=word_tag,\n",
    "                                                               target_vocab=target_vocab[:100],\n",
    "                                                               spatial_tags=spatial_tags[:100], k=k)\n",
    "#                                 print(\"Vicinity Matrix-Synsets: {}\".format(vsyn))\n",
    "\n",
    "\n",
    "                                in_vic, pred_syn = label_in_vicinity(vicinity_matrix=vmat, vicinity_synsets=vsyn,\n",
    "                                                           target_vocab=target_vocab[:100], \n",
    "                                                           spatial_tags=spatial_tags[:100], true_label=local_labels[i])\n",
    "                                \n",
    "                                true_pred.append(in_vic)\n",
    "                                predicted_synsets.append(pred_syn)\n",
    "                                \n",
    "#                                 print(\"In Vicinity? --> {}\".format(in_vic))\n",
    "#                                 print(\"Predicted synsets --> {}\".format(pred_syn))\n",
    "\n",
    "                                if in_vic==True:\n",
    "                                    correct_sense += 1\n",
    "                                    correct_sense_batch += 1\n",
    "\n",
    "                            print(true_pred)\n",
    "                            print(predicted_synsets)\n",
    "                        \n",
    "                            batch_acc = correct_sense_batch / len(local_batch)\n",
    "                            history[\"sense_accuracy\"].append(batch_acc)\n",
    "#                             print(\"correct sense batch ({}) / local_batch ({}) = {}\".format(correct_sense_batch, len(local_batch), batch_acc))\n",
    "\n",
    "\n",
    "                        t1 = time.time()\n",
    "                        print(f'Epoch {epoch}: train loss = {train_loss:.4f}, batch accuracy: {batch_acc:.4f}, time = {t1-t0:.4f}')\n",
    "\n",
    "\n",
    "                sense_accuracy = correct_sense / nb_words_validation\n",
    "\n",
    "                print(\"The sense accuracy on the validation set is {} %\".format(sense_accuracy * 100))\n",
    "                \n",
    "        # **************************************************************************************************************\n",
    "        # Plot Histogram \n",
    "        # **************************************************************************************************************\n",
    "        data1 = history[\"train_loss\"] \n",
    "        data2 = history[\"sense_accuracy\"]\n",
    "\n",
    "        fig, ax1 = plt.subplots()\n",
    "\n",
    "        color = 'tab:red'\n",
    "        ax1.set_xlabel('time (s)')\n",
    "        ax1.set_ylabel('loss', color=color)\n",
    "        ax1.plot(data1, color=color)\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "        color = 'tab:blue'\n",
    "        ax2.set_ylabel('accuracy', color=color)  # we already handled the x-label with ax1\n",
    "        ax2.plot(data2, color=color)\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        plt.show()\n",
    "\n",
    "        return history\n",
    "    \n",
    "    \n",
    "    # assuming the sentence is already splitted into tokens, e.g. ['fall', 'in', 'catastrophes']\n",
    "    def test(self, testing_data, labels, test_syn, data, batch_size, num_workers, target_vocab, spatial_tags, k=5, shuffle=True):\n",
    "        \n",
    "        # parameters\n",
    "        params = {'batch_size': batch_size, #64,\n",
    "                  'shuffle': shuffle,\n",
    "                  'collate_fn': lambda x: x,\n",
    "                  'num_workers': num_workers} #6} #set 0 if training on Windows machine\n",
    "\n",
    "        # Training and validation data generators\n",
    "        testing_set = Dataset(testing_data, labels, test_syn)\n",
    "        testing_generator = torch.utils.data.DataLoader(testing_set, **params)\n",
    "        \n",
    "        # ------\n",
    "        # Count words in sentence to calculate accuracy\n",
    "        # ------\n",
    "        nb_words_testing = 0\n",
    "\n",
    "        for batch in testing_generator:\n",
    "            for sentence, label, syn in batch:\n",
    "                nb_words_testing += len(sentence)\n",
    "                \n",
    "        # --------------------------\n",
    "        VOCAB, weights_matrix = load_vocab(data, embed_size=embed_size)\n",
    "\n",
    "\n",
    "        # ---------------------------  \n",
    "        # testing\n",
    "        # ---------------------------\n",
    "        correct_sense = 0\n",
    "        sense_accuracy = 0\n",
    "        \n",
    "        t0 = time.time()\n",
    "\n",
    "        # set model to eval mode to ignore updating the weights of the model\n",
    "        self.model.eval()\n",
    "\n",
    "        # do not calculate gradients while evaluating\n",
    "        with torch.set_grad_enabled(False):\n",
    "\n",
    "            for batch in testing_generator:\n",
    "                print(\"Batches for testing\")\n",
    "                print(\"#\" * 100)\n",
    "\n",
    "                for local_batch, local_labels, local_synsets in batch:\n",
    "\n",
    "                    # Transform list(<string>) to Tensor(<Tensor>)\n",
    "                    print(\"Input Sentence\")\n",
    "                    print(local_batch)\n",
    "                    input_words = local_batch\n",
    "                    local_batch = numericalize(local_batch, VOCAB)\n",
    "#                     print(type(local_batch), local_batch)\n",
    "\n",
    "\n",
    "                    # Transform List(<Tensor>) to Tensor(<Tensor>)\n",
    "                    # I have labels of same length --> this should be no problem for Tensor\n",
    "                    local_labels = torch.stack(local_labels)\n",
    "                    print(\"Labels:\")\n",
    "                    print(local_synsets)\n",
    "#                     print(\"Labels\")\n",
    "#                     print(type(local_labels), len(local_labels), type(local_labels[0]))\n",
    "#                     print(local_labels)\n",
    "\n",
    "                    # Transfer to GPU\n",
    "                    local_batch, local_labels = local_batch.to(self.device), local_labels.to(self.device)\n",
    "\n",
    "                    # Model computations\n",
    "                    # out outputs the indices of wordnet database\n",
    "                    out = self.model(local_batch)\n",
    "\n",
    "                    # During validation and testing, I want to be less strict.\n",
    "                    # So, if a point resides within the label sphere, the sense is correctly identified.\n",
    "                    loss = geometric_loss(out, local_labels, include_r=True)\n",
    "\n",
    "                    vloss_sum += loss.item()                  \n",
    "\n",
    "                    validation_loss = vloss_sum / len(local_batch)\n",
    "                    history['testing_loss'].append(validation_loss)\n",
    "#                             print(history)\n",
    "#                             print(len(history['validation_loss']))\n",
    "\n",
    "                    correct_sense_batch = 0\n",
    "#                             print(\"Initializing the corrext sense batch = {}\".format(correct_sense_batch))\n",
    "\n",
    "                    true_pred = []\n",
    "                    predicted_synsets = []\n",
    "\n",
    "                    for i, word_tag in enumerate(out):\n",
    "\n",
    "                        vindices, vmat, vsyn = vicinity_matrix(spatial_params=word_tag,\n",
    "                                                       target_vocab=target_vocab,\n",
    "                                                       spatial_tags=spatial_tags, k=k)\n",
    "#                                 print(\"Vicinity Matrix-Synsets: {}\".format(vsyn))\n",
    "\n",
    "        \n",
    "                        in_vic, pred_syn = label_in_vicinity(vicinity_matrix=vmat, vicinity_synsets=vsyn,\n",
    "                                                   target_vocab=target_vocab, \n",
    "                                                   spatial_tags=spatial_tags, true_label=local_labels[i])\n",
    "        \n",
    "                        print(\"In Vicinity? --> {}\".format(in_vic))\n",
    "                        print(\"Predicted synsets --> {}\".format(pred_syn))\n",
    "            \n",
    "                        true_pred.append(in_vic)\n",
    "                        predicted_synsets.append(pred_syn)\n",
    "                        \n",
    "\n",
    "                        if in_vic:\n",
    "                            correct_sense += 1\n",
    "                            correct_sense_batch += 1\n",
    "\n",
    "                    print(true_pred)\n",
    "                    print(predicted_synsets)\n",
    "                    \n",
    "                    batch_acc = correct_sense_batch / len(local_batch)\n",
    "                    history[\"sense_accuracy\"].append(batch_acc)\n",
    "#                             print(\"correct sense batch ({}) / local_batch ({}) = {}\".format(correct_sense_batch, len(local_batch), batch_acc))\n",
    "\n",
    "                    \n",
    "                t1 = time.time()\n",
    "                print(f'batch accuracy: {batch_acc:.4f}, time = {t1-t0:.4f}')\n",
    "\n",
    "\n",
    "        sense_accuracy = correct_sense / nb_words_validation\n",
    "\n",
    "        print(\"The sense accuracy on the testing set is {} %\".format(sense_accuracy * 100))\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    def tag(self, sentence, embed_size, target_vocab, spatial_tags, k):\n",
    "        print(\"Initial Input: \", sentence)\n",
    "        \n",
    "        if isinstance(sentence, str):\n",
    "            # preprocess the sentence, such that the lemmatized sentence is returned\n",
    "            lemm_sentence = preprocess(sentence)\n",
    "            tokens = list(map(lambda x: x[0], lemm_sentence))\n",
    "            \n",
    "        if isinstance(sentence, list):\n",
    "            lst2str = \" \".join(sentence)\n",
    "            lemm_sentence = preprocess(lst2str)\n",
    "            tokens = list(map(lambda x: x[0], lemm_sentence))\n",
    "            \n",
    "            \n",
    "        \n",
    "        N = len(tokens)\n",
    "        tags = '?' * N\n",
    "        print(\"Lemmatized Sentence: \", tokens)\n",
    "        \n",
    "        #print(tags)\n",
    "        \n",
    "        data = tokens\n",
    "        \n",
    "        # words embeddings\n",
    "        vocab, wmat = load_vocab(data, embed_size)\n",
    "        \n",
    "        # numericalize words\n",
    "        num_data = numericalize(data, vocab)\n",
    "        \n",
    "        num_data = num_data.to(self.device)\n",
    "        \n",
    "        out = self.model(num_data)\n",
    "        \n",
    "        distances = []\n",
    "        predicted_synsets = []\n",
    "\n",
    "        for i, word_tag in enumerate(out):\n",
    "\n",
    "            vindices, vmat, vsyn = vicinity_matrix(spatial_params=word_tag,\n",
    "                                           target_vocab=target_vocab,\n",
    "                                           spatial_tags=spatial_tags, k=k)\n",
    "#                                 print(\"Vicinity Matrix-Synsets: {}\".format(vsyn))\n",
    "            \n",
    "            predicted_synsets.append(vsyn)\n",
    "    \n",
    "            distances.append(vmat)\n",
    "            #distances.append(decode_key(vsyn.keys(), vmat))\n",
    "            \n",
    "        for i in range(N):\n",
    "            print(data[i], \"\\t\", tags[i], \"\\t\", predicted_synsets[i].items())\n",
    "            print()\n",
    "            \n",
    "        \n",
    "        return predicted_synsets\n",
    "\n",
    "        \n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from bert_embedding import BertEmbedding\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk import pos_tag, WordNetLemmatizer\n",
    "from pprint import pprint\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "# set of english stop words U set of punctuation\n",
    "EN_STOPWORDS_PUNCT = set(stopwords.words('english')).union(set(string.punctuation))\n",
    "WN_LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def tags4wn(tag):\n",
    "    \"\"\"Penn Treebank tags: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "    Converts PennTreeBank tags to WN tags, e.g. n, a, v\"\"\"\n",
    "    tag_conversion = {\"NN\": \"n\", # noun\n",
    "                      \"JJ\": \"a\", # adjective\n",
    "                      \"VB\": \"v\", # verb\n",
    "                      \"RB\": \"r\"} # adverb\n",
    "    # there are still many more tags\n",
    "    try:\n",
    "        # return the WN tags\n",
    "        return tag_conversion[tag[:2]]\n",
    "    except:\n",
    "        # if no tag is found, treat the word as a noun\n",
    "        return \"n\"\n",
    "        # I think that in our case it is better to consider them all\n",
    "        # return None\n",
    "\n",
    "def preprocess(sentence):\n",
    "    \"\"\"Preprocesses a raw input sentence and return a list of each word with its POS tag.\"\"\"\n",
    "    # Tokenization\n",
    "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "    # lowercase all words\n",
    "    lower = [word.lower() for word in tokenized_sentence]\n",
    "    # print(lower)\n",
    "    # delete stop words and punctuation\n",
    "    clean_sentence = [word for word in lower if word not in EN_STOPWORDS_PUNCT]\n",
    "    # print(clean_sentence)\n",
    "    # use wordNet Lemmatizer to do POS and then lemmatize\n",
    "    pos_tagging = pos_tag(clean_sentence)\n",
    "    # print(pos_tagging)\n",
    "    # Lemmatize\n",
    "    lemmatized_sentence = [(WN_LEMMATIZER.lemmatize(word, pos=tags4wn(tag)), tags4wn(tag)) for word, tag in pos_tagging]\n",
    "    # print(lemmatized_sentence)\n",
    "\n",
    "    return lemmatized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('frog', 'n'), ('jumping', 'n')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frog = preprocess(\"The frog is jumping.\")\n",
    "frog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frog', 'jumping']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: x[0], frog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Validation ...\n",
      "New Batch for Validation\n",
      "####################################################################################################\n",
      "Input Sentence\n",
      "['not', 'support']\n",
      "Labels:\n",
      "['not.r.01', 'corroborate.v.03']\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary']\n",
      "Labels:\n",
      "['necessary.s.02']\n",
      "True is not in list\n",
      "[False]\n",
      "[['no-synset']]\n",
      "Input Sentence\n",
      "['necessary', 'normal']\n",
      "Labels:\n",
      "['necessary.a.01', 'normal.a.01']\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'effectiveness']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'effectiveness.n.01']\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'physical_ability', 'mental_ability']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08']\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Epoch 0: train loss = 59756.5335, batch accuracy: 0.0000, time = 3.4122\n",
      "The sense accuracy on the validation set is 0.0 %\n",
      "Training ...\n",
      "Validation ...\n",
      "New Batch for Validation\n",
      "####################################################################################################\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'effectiveness']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'effectiveness.n.01']\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'physical_ability', 'mental_ability']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08']\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary', 'normal']\n",
      "Labels:\n",
      "['necessary.a.01', 'normal.a.01']\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary']\n",
      "Labels:\n",
      "['necessary.s.02']\n",
      "True is not in list\n",
      "[False]\n",
      "[['no-synset']]\n",
      "Input Sentence\n",
      "['not', 'support']\n",
      "Labels:\n",
      "['not.r.01', 'corroborate.v.03']\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Epoch 1: train loss = 89634.7734, batch accuracy: 0.0000, time = 3.5037\n",
      "The sense accuracy on the validation set is 0.0 %\n",
      "Training ...\n",
      "Validation ...\n",
      "New Batch for Validation\n",
      "####################################################################################################\n",
      "Input Sentence\n",
      "['necessary', 'normal']\n",
      "Labels:\n",
      "['necessary.a.01', 'normal.a.01']\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'physical_ability', 'mental_ability']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08']\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'effectiveness']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'effectiveness.n.01']\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary']\n",
      "Labels:\n",
      "['necessary.s.02']\n",
      "True is not in list\n",
      "[False]\n",
      "[['no-synset']]\n",
      "Input Sentence\n",
      "['not', 'support']\n",
      "Labels:\n",
      "['not.r.01', 'corroborate.v.03']\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Epoch 2: train loss = 179269.6112, batch accuracy: 0.0000, time = 3.2672\n",
      "The sense accuracy on the validation set is 0.0 %\n",
      "Training ...\n",
      "Validation ...\n",
      "New Batch for Validation\n",
      "####################################################################################################\n",
      "Input Sentence\n",
      "['necessary']\n",
      "Labels:\n",
      "['necessary.s.02']\n",
      "True is not in list\n",
      "[False]\n",
      "[['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'effectiveness']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'effectiveness.n.01']\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary', 'normal']\n",
      "Labels:\n",
      "['necessary.a.01', 'normal.a.01']\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['not', 'support']\n",
      "Labels:\n",
      "['not.r.01', 'corroborate.v.03']\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'physical_ability', 'mental_ability']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08']\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Epoch 3: train loss = 89634.7633, batch accuracy: 0.0000, time = 3.8226\n",
      "The sense accuracy on the validation set is 0.0 %\n",
      "Training ...\n",
      "Validation ...\n",
      "New Batch for Validation\n",
      "####################################################################################################\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'physical_ability', 'mental_ability']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08']\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'effectiveness']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'effectiveness.n.01']\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary', 'normal']\n",
      "Labels:\n",
      "['necessary.a.01', 'normal.a.01']\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['not', 'support']\n",
      "Labels:\n",
      "['not.r.01', 'corroborate.v.03']\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary']\n",
      "Labels:\n",
      "['necessary.s.02']\n",
      "True is not in list\n",
      "[False]\n",
      "[['no-synset']]\n",
      "Epoch 4: train loss = 89634.7566, batch accuracy: 0.0000, time = 3.3364\n",
      "The sense accuracy on the validation set is 0.0 %\n",
      "Training ...\n",
      "Validation ...\n",
      "New Batch for Validation\n",
      "####################################################################################################\n",
      "Input Sentence\n",
      "['necessary']\n",
      "Labels:\n",
      "['necessary.s.02']\n",
      "True is not in list\n",
      "[False]\n",
      "[['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'effectiveness']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'effectiveness.n.01']\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['not', 'support']\n",
      "Labels:\n",
      "['not.r.01', 'corroborate.v.03']\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'physical_ability', 'mental_ability']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08']\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary', 'normal']\n",
      "Labels:\n",
      "['necessary.a.01', 'normal.a.01']\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Epoch 5: train loss = 179269.5483, batch accuracy: 0.0000, time = 3.1490\n",
      "The sense accuracy on the validation set is 0.0 %\n",
      "Training ...\n",
      "Validation ...\n",
      "New Batch for Validation\n",
      "####################################################################################################\n",
      "Input Sentence\n",
      "['not', 'support']\n",
      "Labels:\n",
      "['not.r.01', 'corroborate.v.03']\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary']\n",
      "Labels:\n",
      "['necessary.s.02']\n",
      "True is not in list\n",
      "[False]\n",
      "[['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'physical_ability', 'mental_ability']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary', 'normal']\n",
      "Labels:\n",
      "['necessary.a.01', 'normal.a.01']\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'effectiveness']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'effectiveness.n.01']\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Epoch 6: train loss = 89634.7753, batch accuracy: 0.0000, time = 3.2490\n",
      "The sense accuracy on the validation set is 0.0 %\n",
      "Training ...\n",
      "Validation ...\n",
      "New Batch for Validation\n",
      "####################################################################################################\n",
      "Input Sentence\n",
      "['necessary']\n",
      "Labels:\n",
      "['necessary.s.02']\n",
      "True is not in list\n",
      "[False]\n",
      "[['no-synset']]\n",
      "Input Sentence\n",
      "['not', 'support']\n",
      "Labels:\n",
      "['not.r.01', 'corroborate.v.03']\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'effectiveness']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'effectiveness.n.01']\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'physical_ability', 'mental_ability']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08']\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary', 'normal']\n",
      "Labels:\n",
      "['necessary.a.01', 'normal.a.01']\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Epoch 7: train loss = 89634.8082, batch accuracy: 0.0000, time = 3.1773\n",
      "The sense accuracy on the validation set is 0.0 %\n",
      "Training ...\n",
      "Validation ...\n",
      "New Batch for Validation\n",
      "####################################################################################################\n",
      "Input Sentence\n",
      "['not', 'support']\n",
      "Labels:\n",
      "['not.r.01', 'corroborate.v.03']\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary']\n",
      "Labels:\n",
      "['necessary.s.02']\n",
      "True is not in list\n",
      "[False]\n",
      "[['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'effectiveness']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'effectiveness.n.01']\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'physical_ability', 'mental_ability']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08']\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary', 'normal']\n",
      "Labels:\n",
      "['necessary.a.01', 'normal.a.01']\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Epoch 8: train loss = 89634.7774, batch accuracy: 0.0000, time = 3.1676\n",
      "The sense accuracy on the validation set is 0.0 %\n",
      "Training ...\n",
      "Validation ...\n",
      "New Batch for Validation\n",
      "####################################################################################################\n",
      "Input Sentence\n",
      "['necessary', 'normal']\n",
      "Labels:\n",
      "['necessary.a.01', 'normal.a.01']\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'physical_ability', 'mental_ability']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'physical_ability.n.01', 'capacity.n.08']\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['not', 'support']\n",
      "Labels:\n",
      "['not.r.01', 'corroborate.v.03']\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False]\n",
      "[['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['lack', 'necessary', 'effectiveness']\n",
      "Labels:\n",
      "['miss.v.06', 'necessary.a.01', 'effectiveness.n.01']\n",
      "True is not in list\n",
      "True is not in list\n",
      "True is not in list\n",
      "[False, False, False]\n",
      "[['no-synset'], ['no-synset'], ['no-synset']]\n",
      "Input Sentence\n",
      "['necessary']\n",
      "Labels:\n",
      "['necessary.s.02']\n",
      "True is not in list\n",
      "[False]\n",
      "[['no-synset']]\n",
      "Epoch 9: train loss = 89634.7864, batch accuracy: 0.0000, time = 3.2095\n",
      "The sense accuracy on the validation set is 0.0 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 389,
       "width": 568
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'train_loss': [5787.735087683378,\n",
       "              12418.015133682728,\n",
       "              27276.675181845345,\n",
       "              31001.93762744211,\n",
       "              41611.81551635931,\n",
       "              63338.382213062134,\n",
       "              57077.868009737926,\n",
       "              122027.00174585986,\n",
       "              50920.926375279094,\n",
       "              59756.53346902559,\n",
       "              10609.866082731829,\n",
       "              22002.446615497214,\n",
       "              26025.97754685994,\n",
       "              26186.255720191508,\n",
       "              86430.02287822487,\n",
       "              92217.78634688594,\n",
       "              72197.82556159604,\n",
       "              46344.176098999764,\n",
       "              80060.69750341632,\n",
       "              89634.77340727761,\n",
       "              10245.265890548695,\n",
       "              31209.846185163147,\n",
       "              32931.53119074653,\n",
       "              43475.96569677192,\n",
       "              92739.67648314685,\n",
       "              69682.42955094138,\n",
       "              43676.8256847894,\n",
       "              76125.10431503912,\n",
       "              85699.17859405595,\n",
       "              179269.61115479196,\n",
       "              8835.595960196692,\n",
       "              23863.267270393775,\n",
       "              26154.110768469498,\n",
       "              60164.20475923692,\n",
       "              55667.59093761478,\n",
       "              65191.734281121484,\n",
       "              97641.61213344021,\n",
       "              152250.15201988837,\n",
       "              160121.40387687058,\n",
       "              89634.76328831803,\n",
       "              10719.296744722267,\n",
       "              18649.353096976174,\n",
       "              45169.958407817445,\n",
       "              23892.245956197363,\n",
       "              45412.44728078428,\n",
       "              96612.63050534464,\n",
       "              57830.45554266918,\n",
       "              48798.89759072837,\n",
       "              105453.77247460569,\n",
       "              89634.75657015707,\n",
       "              7871.24838394305,\n",
       "              13459.763185404305,\n",
       "              19218.446240118734,\n",
       "              39437.53786055287,\n",
       "              49981.97251049913,\n",
       "              42156.9074316517,\n",
       "              95033.12128448465,\n",
       "              102889.09118858277,\n",
       "              86740.89775965526,\n",
       "              179269.54829265442,\n",
       "              10609.868539773326,\n",
       "              29090.978998917548,\n",
       "              34878.740094797315,\n",
       "              27983.794412163887,\n",
       "              45167.68803461306,\n",
       "              43399.90025311393,\n",
       "              68585.84273461069,\n",
       "              44538.2015082447,\n",
       "              53373.80500889916,\n",
       "              89634.77530553551,\n",
       "              7855.974732905467,\n",
       "              16501.84807176956,\n",
       "              40874.94513195841,\n",
       "              30981.896384459065,\n",
       "              29490.209615183154,\n",
       "              53809.38273936879,\n",
       "              82465.14805633227,\n",
       "              51477.8489035361,\n",
       "              160221.30177973225,\n",
       "              89634.80820203545,\n",
       "              7871.245545054884,\n",
       "              13658.978830517834,\n",
       "              13388.597202084522,\n",
       "              29656.975996361143,\n",
       "              40266.84290628358,\n",
       "              64408.426326926005,\n",
       "              58850.76725408005,\n",
       "              86323.64617884482,\n",
       "              53407.0949099944,\n",
       "              89634.77737818623,\n",
       "              10245.271114267567,\n",
       "              24892.055740546144,\n",
       "              55571.86352849017,\n",
       "              63443.109868966065,\n",
       "              50151.38240833388,\n",
       "              60870.69129209806,\n",
       "              56262.8849222733,\n",
       "              46344.18597434529,\n",
       "              80060.71472937992,\n",
       "              89634.78641069373],\n",
       "             'validation_loss': [110634.59393115007,\n",
       "              369798.3920096524,\n",
       "              334452.7912282148,\n",
       "              372842.69977785554,\n",
       "              445987.25971813203,\n",
       "              149874.17229237905,\n",
       "              278760.86410402466,\n",
       "              707075.323431438,\n",
       "              1562679.851010228,\n",
       "              891974.5194362641,\n",
       "              149553.5952233886,\n",
       "              241132.0324964347,\n",
       "              471383.54895429197,\n",
       "              1562679.851010228,\n",
       "              891974.5194362641,\n",
       "              148529.20414735225,\n",
       "              199383.90700816314,\n",
       "              448629.45573563327,\n",
       "              559264.0496667833,\n",
       "              445987.25971813203,\n",
       "              166355.23488474038,\n",
       "              371681.15213869954,\n",
       "              707075.323431438,\n",
       "              817709.917362588,\n",
       "              1783949.0388725284,\n",
       "              148529.20414735225,\n",
       "              199383.90700816314,\n",
       "              409710.4544433948,\n",
       "              371210.4621064378,\n",
       "              891974.5194362642,\n",
       "              110634.59393115007,\n",
       "              369798.3920096524,\n",
       "              258804.8328871535,\n",
       "              667163.2609976956,\n",
       "              594649.6796241761,\n",
       "              148529.20414735225,\n",
       "              184899.1960048262,\n",
       "              273140.3029622632,\n",
       "              371210.4621064378,\n",
       "              891974.5194362642,\n",
       "              110634.59393115007,\n",
       "              369798.3920096524,\n",
       "              273140.3029622632,\n",
       "              371210.4621064378,\n",
       "              891974.5194362642,\n",
       "              149553.5952233886,\n",
       "              241132.0324964347,\n",
       "              592898.6589240194,\n",
       "              545139.944908392,\n",
       "              1783949.0388725284],\n",
       "             'sense_accuracy': [0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0,\n",
       "              0.0]})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "#torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "T = RegTagger(use_cuda=use_cuda, device=device)\n",
    "T.train(batch_size=5, num_workers=0, max_epochs=10,\n",
    "        splittings=splittings, labels=labels, train_val_syn=train_val_syn, data=data, embed_size=300,\n",
    "        target_vocab=target_VOCAB, spatial_tags=SPATIAL_TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentence = ['Hundred', 'babies', 'are', 'one', 'years', 'old', '.']\n",
    "sentag = T.tag(sentence, 300, target_VOCAB[:100], SPATIAL_TAGS[:100], 5)\n",
    "# sentag"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hundred', 'babies', 'are', 'one', 'years', 'old', '.']\n",
      "['hundred', 'baby', 'one', 'year', 'old']\n",
      "Hundred \t ? \t dict_items([('B', [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144843.0469, 147138.3594, 147406.2188, 149038.9531, 149611.6406],\n",
      "       grad_fn=<TopkBackward0>)])])\n",
      "\n",
      "babies \t ? \t dict_items([('B', [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.8281, 147138.2031, 147406.0312, 149038.7500, 149611.4219],\n",
      "       grad_fn=<TopkBackward0>)])])\n",
      "\n",
      "are \t ? \t dict_items([('B', [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144842.3281, 147137.8281, 147405.6094, 149038.2812, 149610.9219],\n",
      "       grad_fn=<TopkBackward0>)])])\n",
      "\n",
      "one \t ? \t dict_items([('B', [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144841.8125, 147137.4688, 147405.2031, 149037.7969, 149610.4062],\n",
      "       grad_fn=<TopkBackward0>)])])\n",
      "\n",
      "years \t ? \t dict_items([('B', [array([['tenth', 'tenth.s.01'],\n",
      "       ['1', 'one.s.01'],\n",
      "       ['cv', 'one_hundred_five.s.01'],\n",
      "       ['k', 'thousand.s.01'],\n",
      "       ['x', 'ten.s.01']], dtype='<U76'), tensor([144841.5625, 147137.2812, 147404.9844, 149037.5625, 149610.1562],\n",
      "       grad_fn=<TopkBackward0>)])])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = ['Hundred', 'babies', 'are', 'one', 'years', 'old', '.']\n",
    "sentag = T.tag(sentence, 300, target_VOCAB[:100], SPATIAL_TAGS[:100], 5)\n",
    "sentag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence = ['Hundred', 'babies', 'are', 'one', 'years', 'old', '.']\n",
    "sentag = T.tag(sentence, 300, target_VOCAB[:100], SPATIAL_TAGS[:100], 5)\n",
    "sentag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ['Hundred', 'babies', 'are', 'one', 'years', 'old', '.']\n",
    "sentag = T.tag(sentence, 300, target_VOCAB[:100], SPATIAL_TAGS[:100], 5)\n",
    "sentag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k = sentag[0][0].keys()\n",
    "d = sentag[1][0]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d.fromkeys(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, val in sentag[0].items():\n",
    "#     print(k)\n",
    "\n",
    "for di in sentag[0]:\n",
    "    print(type(di))\n",
    "    print(di.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.fromkeys(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, val in sentag[0].items():\n",
    "#     print(k)\n",
    "\n",
    "for di in sentag[0]:\n",
    "    print(type(di))\n",
    "    print(di.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.test(testing_data=splittings[], test_syn=, batch_size=5, num_workers=0, target_vocab=target_VOCAB, spatial_tag=SPATIAL_TAGS, k=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['train_loss'])\n",
    "# plt.plot(history['sense_accuracy'])\n",
    "# plt.legend(['training loss', 'validation accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['train_loss'])\n",
    "# plt.plot(history['sense_accuracy'])\n",
    "# plt.legend(['training loss', 'validation accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['validation_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['sense_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = history[\"train_loss\"] \n",
    "data2 = history[\"sense_accuracy\"]\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('time (s)')\n",
    "ax1.set_ylabel('loss', color=color)\n",
    "ax1.plot(data1, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(data2, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = np.array([7,7,3])\n",
    "a[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sense Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def is_contained(pred, sphere_coo, compare_spheres=False):\n",
    "\n",
    "    pt, word = coo2point(pred)\n",
    "    sphere_sense, sphere_center = coo2point(sphere_coo)\n",
    "\n",
    "    pt_rad = pred[-1]\n",
    "    sphere_rad = sphere_coo[-1] # in angles\n",
    "    \n",
    "    \n",
    "    \n",
    "    if compare_spheres == False:\n",
    "        contained = (pt[0] - sphere_sense[0])**2 + (pt[1] - sphere_sense[1])**2 <= sphere_rad**2\n",
    "    else:\n",
    "        contained = pt_rad + torch.linalg.norm(pt - sphere_sense) - sphere_rad <= 0\n",
    "\n",
    "    if contained:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "def vicinity_matrix(spatial_params, target_vocab: np.ndarray, spatial_tags: np.ndarray, k=5):#, include_sphere=True, include_r=True) -> [str]:\n",
    "    \"\"\"\n",
    "    Projects the predicted spatial parameters into the embedding space.\n",
    "    Returns the synsets in the vicinity of the projected point.\n",
    "    :param spatial_params:\n",
    "    :return: Vicinity matrix, synsets dict\n",
    "    \"\"\"\n",
    "    N = len(spatial_tags)\n",
    "    \n",
    "    #convert spatial_tags to tensor\n",
    "    spatial_tags = torch.from_numpy(spatial_tags)\n",
    "    \n",
    "    synsets = {} # sort from most specific to most general\n",
    "    \n",
    "    indices = {}\n",
    "\n",
    "    sense_pt, center_pt = coo2point(spatial_params)\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------------------------\n",
    "    # Prepare distance and containment calculations\n",
    "    # ----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # distance calculations\n",
    "    dist_spheres = torch.empty(N) \n",
    "    dist_pt_sphere = torch.empty(N) \n",
    "    dist_pts = torch.empty(N)\n",
    "    \n",
    "    for i, tag in enumerate(spatial_tags):\n",
    "        dist_spheres[i] = distance_loss(spatial_params, tag, include_r=True)\n",
    "        dist_pt_sphere[i] = distance_loss(spatial_params, tag, pt_sphere=True)\n",
    "        dist_pts[i] = distance_loss(spatial_params, tag, include_r=False)\n",
    "    \n",
    "    # containment calculations\n",
    "    full_contained = torch.empty(N) \n",
    "    part_contained = torch.empty(N)\n",
    "    disconnected = torch.empty(N) # handles points only\n",
    "    \n",
    "    for j, tag in enumerate(spatial_tags):\n",
    "        full_contained[j] = is_contained(spatial_params, tag, compare_spheres=True)\n",
    "        part_contained[j] = distance_loss(spatial_params, tag, include_r=True) > 0\n",
    "        disconnected[j] = ~ is_contained(spatial_params, tag, compare_spheres=True) # reverse the True <----> False\n",
    "    \n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Initialize the Vicinity Matrix\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    # row=3, col=3, topk=2, 2 indicates the column of indices and the distances\n",
    "    vicinity_matrix = torch.zeros((3,3, k, 2))\n",
    "    \n",
    "    ####################################################################################################################\n",
    "    # # Full contained + min dist between sense points\n",
    "    ####################################################################################################################\n",
    "    \n",
    "    print(\"True elements\")\n",
    "    true_indices1 = (full_contained == True).nonzero(as_tuple=True)[0]\n",
    "    print(true_indices1)\n",
    "    \n",
    "    if true_indices1.size(0) != 0:\n",
    "        dist1 = torch.index_select(dist_pts, 0, true_indices1)\n",
    "        print(\"dist1\", dist1)\n",
    "        print(\"k = \", k)\n",
    "        # sort in ascending order\n",
    "        # select top k \n",
    "        sort_dist1, sort_indices = torch.topk(dist1, k, largest=False)  \n",
    "        print(\"SORTING\", sort_dist1, sort_indices)\n",
    "        synsets1 = np.take(target_vocab, sort_indices, 0)\n",
    "        synsets[\"A\"] = synsets1\n",
    "        indices[\"A\"] = sort_indices\n",
    "        # index, distance (without synsets because this would result in conflicts for torch.tensor that do not support str)\n",
    "        vicinity_matrix[2][0] = torch.stack((sort_indices, sort_dist1), dim=1)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    ####################################################################################################################\n",
    "    # # Partially contained + min dist between sense points\n",
    "    ####################################################################################################################\n",
    "    true_indices2 = (part_contained == True).nonzero(as_tuple=True)[0]\n",
    "    print(\"True Indices 2\", true_indices2)\n",
    "    \n",
    "    if true_indices2.size(0) != 0:\n",
    "        dist1 = torch.index_select(dist_pts, 0, true_indices2)\n",
    "        # sort in ascending order\n",
    "        # select top k \n",
    "        sort_dist2, sort_indices2 = torch.topk(dist1, k, largest=False)     \n",
    "        synsets2 = np.take(target_vocab, sort_indices2, 0)\n",
    "        print(\"synset 2\", synsets2)\n",
    "        synsets[\"B\"] = synsets2\n",
    "        indices[\"B\"] = sort_indices2\n",
    "        # index, distance (without synsets because this would result in conflicts for torch.tensor that do not support str)\n",
    "        vicinity_matrix[2][1] = torch.stack((sort_indices2, sort_dist2), dim=1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # # Disconnected + min dist between spheres/point2sphere/sense points ---> acts as Nearest neighbor\n",
    "    ####################################################################################################################\n",
    "    # get indices, where disconnected is true\n",
    "    true_indices3 = (disconnected == True).nonzero(as_tuple=True)[0]\n",
    "    print(\"True Indices 3\", true_indices3)\n",
    "\n",
    "    if true_indices3.size(0) != 0:\n",
    "        # get the distances at those indices\n",
    "        dist_spheres3 = torch.index_select(dist_spheres, 0, true_indices3)\n",
    "        dist_pt_sphere3 = torch.index_select(dist_pt_sphere, 0, true_indices3)\n",
    "        dist_pts3 = torch.index_select(dist_pts, 0, true_indices3)\n",
    "\n",
    "        # sort-select top k minimum distances\n",
    "        sort_dist_spheres3, sort_sph_indices3 = torch.topk(dist_spheres3, k, largest=False)\n",
    "        sort_dist_pt_sphere3, sort_pt_sph_indices3 = torch.topk(dist_pt_sphere3, k, largest=False)\n",
    "        sort_dist_pts3, sort_pts_indices3 = torch.topk(dist_pts3, k, largest=False)\n",
    "\n",
    "        # get their corresponding synsets\n",
    "        synsets30 = np.take(target_vocab, sort_sph_indices3, 0)\n",
    "        #print(\"synset30\", synsets30)\n",
    "        synsets[\"C\"] = synsets30\n",
    "        indices[\"C\"] = sort_sph_indices3\n",
    "        \n",
    "        synsets31 = np.take(target_vocab, sort_pt_sph_indices3, 0)\n",
    "        synsets[\"D\"] = synsets31\n",
    "        indices[\"D\"] = sort_pt_sph_indices3\n",
    "        \n",
    "        synsets32 = np.take(target_vocab, sort_pts_indices3, 0)\n",
    "        synsets[\"E\"] = synsets32\n",
    "        indices[\"E\"] = sort_pts_indices3\n",
    "        \n",
    "        # insert them into the vicinity matrix    \n",
    "        vicinity_matrix[0][3] = torch.stack((sort_sph_indices3, sort_dist_spheres3), dim=1)\n",
    "        vicinity_matrix[1][3] = torch.stack((sort_pt_sph_indices3, sort_dist_pt_sphere3), dim=1)\n",
    "        vicinity_matrix[2][3] = torch.stack((sort_pts_indices3, sort_dist_pts3), dim=1)  \n",
    "    \n",
    "\n",
    "\n",
    "#     # get the spheres, where the point/point+radius is contained/overlaping/near\n",
    "\n",
    "#     # 1. check if the predicted point is contained in some sense\n",
    "#     contained = torch.empty(N)\n",
    "    \n",
    "#     for i, tag in enumerate(spatial_tags):\n",
    "#         contained[i] = is_contained(spatial_params, tag, compare_spheres=include_sphere)\n",
    "    \n",
    "#     # 2. For those synsets, which is the nearest synset point\n",
    "#     #use distance() to calculate distance between centers\n",
    "#     distances = torch.empty(N)\n",
    "#     for i, tag in enumerate(spatial_tags):\n",
    "#         distances[i] = distance_loss(spatial_params, tag, include_r=include_r)\n",
    "    \n",
    "#     # sort dist--> indices\n",
    "#     # check if for those distances the containment is true\n",
    "#     # if true: choose the one having min_dist as sense\n",
    "#     # top k senses must be stored in a dict \n",
    "    \n",
    "#     # check if for those distances the containment is false, then, only the radius is falsly predicted (not priority now)\n",
    "#     # if false and min_dist: choose it as potential sense\n",
    "    \n",
    "    \n",
    "\n",
    "#     # 3. If None of the synsets apply to that word sense\n",
    "#     # use sphere_dist to find the nearest sphere (most general synset), and assign it to that synset\n",
    "#     # (this maybe good for rare senses)\n",
    "#     # acts as a second chance\n",
    "#     rare_contained = torch.empty(N)\n",
    "#     rare_distances = torch.empty(N)\n",
    "#     for i, tag in enumerate(spatial_tags):\n",
    "#         rare_contained[i] = is_contained(spatial_params, tag, compare_spheres=False) #only consider sense point\n",
    "#         rare_distances[i] = distance_loss(spatial_params, tag, include_r=False)\n",
    "\n",
    "\n",
    "    return indices, vicinity_matrix, synsets\n",
    "\n",
    "def decode_key(key, mtx):\n",
    "    if key == \"A\":\n",
    "        return mtx[2, 0]\n",
    "    if key == \"B\":\n",
    "        return mtx[2, 1]\n",
    "    if key == \"C\":\n",
    "        return mtx[0, 2]\n",
    "    if key == \"D\":\n",
    "        return mtx[1, 2]\n",
    "    if key == \"E\":\n",
    "        return mtx[2, 2]\n",
    "    \n",
    "\n",
    "def label_in_vicinity(vicinity_matrix, vicinity_synsets, target_vocab, spatial_tags, true_label):\n",
    "    \n",
    "    checked_synsets = []\n",
    "    contained = []\n",
    "    checks = 0\n",
    "    predicted = []\n",
    "    distances = []\n",
    "    \n",
    "    in_vicinity = False\n",
    "    \n",
    "    # true label is either one of the possibilities [word, synset] or a randomly chosen one\n",
    "    \n",
    "    # induce subset of word-synset name \n",
    "    \n",
    "    #spatial_tags = torch.from_numpy(spatial_tags)\n",
    "    #idx_label = (spatial_tags == true_label).nonzero(as_tuple=True)[0]\n",
    "    # transform to numpy to \n",
    "    true_label = np.array(true_label, dtype=np.float64)\n",
    "    # keep spatial tag an np.ndarray\n",
    "    rounded_l = np.round(true_label, decimals=2)\n",
    "    try:\n",
    "        # detecting the true label from the spatial_tags\n",
    "        idx = [[np.array_equal(rounded_l, tag) for tag in spatial_tags].index(True)]\n",
    "        print(\"Found {} matching word-synset tags.\".format(len(idx)))\n",
    "        word_synset = target_vocab[idx] #list of list \n",
    "        print(\"Matching word-synset\", word_synset)\n",
    "        # check if word_synset is within the vicinity matrix\n",
    "        if len(word_synset) != 0:\n",
    "            for e in word_synset:\n",
    "                for key, val in vicinity_synsets.items():\n",
    "                    print(\"Searching in vicinity ... \")\n",
    "\n",
    "                    print(\"Checking if true label is in vicinity ...\")\n",
    "                    checked_synsets.append(e)\n",
    "                    is_there = e[1] in val[:, 1]\n",
    "                    checks += 1\n",
    "                    contained.append(is_there)\n",
    "                    \n",
    "#                     print(\"1\")\n",
    "#                     print(checked_synsets)\n",
    "#                     print(checks)\n",
    "#                     print(contained)\n",
    "                    \n",
    "                    if is_there:\n",
    "                        print(\"The main true label <{}> is in the vacinity of the predicted tag.\".format(e))\n",
    "                        idx_e = np.where(val[:, 1] == e[1])\n",
    "                        predicted.append(val[idx_e])\n",
    "#                         print(\"Predicted 1: \", predicted)\n",
    "                        distances.append(decode_key(key, vicinity_matrix)[idx_e][1])\n",
    "#                         print(\"Distances 1: \", distances)\n",
    "                    else:\n",
    "                        print(\"The main true label is not in vicinity ... \")\n",
    "                        distances.append(0.0)\n",
    "                        print(\"Searching if alternative true label synsets are in vicinity ... \")\n",
    "                    # induce all the word-synset tuples that have same synset as true label.\n",
    "                    # This double check is necessary since I choose the spatial tags in the training data randomly sometimes.\n",
    "                    # get indices of all word-synsets sharing same synset (not same word)\n",
    "                    ix = np.where(target_vocab == [_, e[1]])[0] # add [0] to indicate only the row index, not the column\n",
    "#                     print(\"Indices \", ix)\n",
    "                    if len(ix) != 0:\n",
    "                        pos_syn = target_vocab[ix]\n",
    "                        \n",
    "#                         print(\"Possible synsets: \", pos_syn)\n",
    "#                         print(target_vocab[:10])\n",
    "                        for t in pos_syn:\n",
    "                            checks += 1\n",
    "                            checked_synsets.append(t)\n",
    "                            is_near = t[1] in val[:, -1]\n",
    "                            contained.append(is_near)\n",
    "#                             print(\"2\")\n",
    "#                             print(checked_synsets)\n",
    "#                             print(checks)\n",
    "#                             print(contained)\n",
    "                            if is_near == True:                                    \n",
    "                                print(\"... The word-synset <{}> is in the vicinity of the predicted tag.\".format(t))\n",
    "                                idx_t = np.where(val[:, -1] == t[1])\n",
    "                                predicted.append(val[idx_t])\n",
    "#                                 print(\"Predicted 2: \", predicted)\n",
    "                                distances.append(decode_key(key, vicinity_matrix)[idx_t][1])\n",
    "#                                 print(\"Distances 2: \", distances)\n",
    "                            else:\n",
    "                                distances.append(0.0)\n",
    "                    else: \n",
    "                        print(\"... There are no other possibilites for word-synset <{}>\".format(e))\n",
    "                            \n",
    "        else:\n",
    "            print(\"Cannot find the suitable synset of this spatial tag!\")\n",
    "\n",
    "        \n",
    "    except ValueError as ve:\n",
    "        print(ve)\n",
    "        print(\"Found no index for the true label. Something went wrong ...\")\n",
    "        print(\"Comparing <true label = {}> with <rounded label = {}>\".format(true_label, rounded_l))\n",
    "    \n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Statistics\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    print(\"~\" * 80)\n",
    "    print(\"Statistics\")\n",
    "    print(\"~\" * 80)\n",
    "    \n",
    "#     print(\"Predicted Spatial Tag = \", spatial_params)\n",
    "    print(\"Checked Spatial Tag(s) ; contained? ; Predicted ; distances = ({}):\".format(len(checked_synsets)))\n",
    "    for s, c, p, d in zip(checked_synsets, contained, predicted, distances):\n",
    "        print(s, \";\", c, \";\", \"\\n\", p, \";\", d)\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "#     print(\"True Spatial Tag(s) is in vicinity of predicted tag: \", contained)\n",
    "    contained_idx = np.where(np.array(contained) == True)\n",
    "    \n",
    "#     print(\"contained_idx\", contained_idx)\n",
    "#     print(\"checked_idx\", np.array(checked_synsets)[contained_idx])\n",
    "#     print(\"slice\", np.array(checked_synsets)[:, 1])\n",
    "#     print(\"check_slice\", np.array(checked_synsets)[:, 1][contained_idx])\n",
    "\n",
    "    if len(contained_idx) != 0:\n",
    "        print()\n",
    "        only_syn = set(np.array(checked_synsets)[contained_idx][:, 1])\n",
    "        print(\"True Sense Tag(s) = ({}) ..\".format(len(only_syn)), only_syn)\n",
    "        print(\"Prediction is correct!\")\n",
    "        in_vicinity = True\n",
    "#         print(\"Distance(predicted_sense, nearest_true_sense) = ({}): \".format(len(np.array(predicted)[contained_idx])))\n",
    "#         for p, d in zip(np.array(predicted), distances):\n",
    "#               print(p, d)\n",
    "              \n",
    "    else:\n",
    "        print(\"Prediction is false ..\")\n",
    "        print(\"All synsets in the vicinity of the predicted tag are not true senses ..\")\n",
    "        print(\"Please check manually if the synsets in the vicinity are generalizations of the true labels.\")\n",
    "        in_vicinity = False\n",
    "    \n",
    "    \n",
    "    return checked_synsets, contained, checks, predicted, distances\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.where(target_VOCAB==[_,\"boat.n.01\"])[0]\n",
    "SPATIAL_TAGS[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_VOCAB[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_VOCAB[60])\n",
    "print(SPATIAL_TAGS[60])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([34035, 78.06, 174780, 0.0, 64.5]) # synset boat\n",
    "\n",
    "# I created this synset for thousand.n.01, at index 60, word is 'thou'\n",
    "sample_tag = torch.tensor([36670, 100.07, 180000.00, 0.0, 1.6])\n",
    "true_lab = torch.tensor(SPATIAL_TAGS[60])\n",
    "\n",
    "idx, mat, syn = vicinity_matrix(spatial_params=sample_tag, target_vocab=target_VOCAB[:100], spatial_tags=SPATIAL_TAGS[:100], k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e = np.array(['thou', 'thousand.n.01'])\n",
    "e = np.array([\"thou\", \"thousand.n.01\"])\n",
    "val = np.array([['k', 'thousand.n.01'], ['c', 'hundred.n.01'], ['i', 'one.n.01'], ['ks', 'thousand.n.01'], ['cs', 'hundred.n.01']])\n",
    "e in val \n",
    "idx_e = np.where(val == e)\n",
    "idx_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s =np.where(val[:, 1] == e[1])\n",
    "val[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = np.array([True, False, True])\n",
    "\n",
    "co[np.where(co==True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csyn, cont, ch, pred, dist = label_in_vicinity(vicinity_matrix=mat, vicinity_synsets=syn, \n",
    "                      target_vocab=target_VOCAB[:100], spatial_tags=SPATIAL_TAGS[:100], true_label=true_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "poss = []\n",
    "dis = []\n",
    "for p, d in zip(pred, dist):\n",
    "    print(\"There are {} true values in the vicinity of predicted tag.\".format(len(p)))\n",
    "    if len(p) > 0:\n",
    "        for i in range(len(p)):\n",
    "            is_in = p[i] in poss\n",
    "            if is_in:\n",
    "                continue\n",
    "            else:\n",
    "                poss.append(p[i])\n",
    "                dis.append(d[i])\n",
    "    print(poss)\n",
    "    print(dis)\n",
    "    \n",
    "#     if p in poss:\n",
    "#         continue\n",
    "#     else:\n",
    "#         poss.append(p)\n",
    "#     if d in dis:\n",
    "#         continue\n",
    "#     else:\n",
    "#         dis.append(d)\n",
    "        \n",
    "        \n",
    "for p,d in zip(poss, dis):\n",
    "    print(p, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spatial_params = torch.tensor([111740.0, 98.3, 980130.0, 0.0, 18.5], dtype=torch.float64)\n",
    "# df=spatial_wordnet\n",
    "# st = list(zip(df.l0, df.alpha, df.l_i, df.beta_i, df.radius))\n",
    "# df.loc[df['l0'] == 111740.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The problem here was that there is transforming from torch to numpy leads to failures in rounding\n",
    "s = np.array(data[0][2][2], dtype=np.float64)\n",
    "print(s)\n",
    "spatial_params = np.array([1.351440e+05, 2.501000e+01, 6.417603e+04, 9.000000e+01, 5.000000e-01])\n",
    "print(\"spatial_params\", spatial_params)\n",
    "# index = np.where(SPATIAL_TAGS==spatial_params)\n",
    "# len(index[0])\n",
    "# len(SPATIAL_TAGS)\n",
    "sr = np.round(s, decimals=2)\n",
    "pr = np.round(spatial_params, decimals=2)\n",
    "print(\"rounded spatial_params\", pr)\n",
    "idx = [np.array_equal(pr,x) for x in SPATIAL_TAGS].index(True)\n",
    "idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [13, 304]\n",
    "target_VOCAB[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(target_VOCAB == [_, 'dog.n.01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target_VOCAB[111115]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tp = data[0][2][2]\n",
    "print(\"torch tensor:\", tp)\n",
    "tpr = torch.round(tp)\n",
    "print(\"rounded:\", tpr)\n",
    "ttags = torch.from_numpy(SPATIAL_TAGS) \n",
    "print(ttags[50801])\n",
    "# torch.nonzero((ttags == tp).sum(dim=1) == ttags.size(1))\n",
    "torch.all(ttags == tpr)#, x=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#spatial_params = torch.tensor([143500, 6.8500, 7574.6, 0.0, 0.5])\n",
    "torch.nonzero((torch.from_numpy(SPATIAL_TAGS) == spatial_params).sum(dim=1) == torch.from_numpy(SPATIAL_TAGS).size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#spatial_params = torch.tensor([143500, 6.8500, 7574.6, 0.0, 0.5])\n",
    "torch.nonzero((torch.from_numpy(SPATIAL_TAGS) == spatial_params).sum(dim=1) == torch.from_numpy(SPATIAL_TAGS).size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.all(torch.from_numpy(SPATIAL_TAGS) == spatial_params, x=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.from_numpy(SPATIAL_TAGS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spatial_params = torch.tensor([111740.0, 98.3, 980130.0, 0.0, 18.5])\n",
    "c0 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[0]).nonzero(as_tuple=True)\n",
    "print(c0)\n",
    "c1 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[1]).nonzero(as_tuple=True)\n",
    "print(c1)\n",
    "c2 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[2]).nonzero(as_tuple=True)\n",
    "print(c2)\n",
    "c3 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[3]).nonzero(as_tuple=True)\n",
    "print(c3)\n",
    "c5 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[4]).nonzero(as_tuple=True)\n",
    "print(c5)\n",
    "# for i in c[1]:\n",
    "#     if i != 3:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.where(torch.from_numpy(SPATIAL_TAGS)== spatial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.where(torch.from_numpy(SPATIAL_TAGS)== spatial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = torch.tensor([34035, 78.06, 174780, 0.0, 64.5]) # synset boat\n",
    "\n",
    "idx, mat, syn = vicinity_matrix(spatial_params=params, target_vocab=target_VOCAB[:20], spatial_tags=SPATIAL_TAGS[:20], k=5)\n",
    "syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tp = data[0][2][2]\n",
    "print(\"torch tensor:\", tp)\n",
    "tpr = torch.round(tp)\n",
    "print(\"rounded:\", tpr)\n",
    "ttags = torch.from_numpy(SPATIAL_TAGS) \n",
    "print(ttags[50801])\n",
    "# torch.nonzero((ttags == tp).sum(dim=1) == ttags.size(1))\n",
    "torch.all(ttags == tpr)#, x=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#spatial_params = torch.tensor([143500, 6.8500, 7574.6, 0.0, 0.5])\n",
    "torch.nonzero((torch.from_numpy(SPATIAL_TAGS) == spatial_params).sum(dim=1) == torch.from_numpy(SPATIAL_TAGS).size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#spatial_params = torch.tensor([143500, 6.8500, 7574.6, 0.0, 0.5])\n",
    "torch.nonzero((torch.from_numpy(SPATIAL_TAGS) == spatial_params).sum(dim=1) == torch.from_numpy(SPATIAL_TAGS).size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.all(torch.from_numpy(SPATIAL_TAGS) == spatial_params, x=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.from_numpy(SPATIAL_TAGS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spatial_params = torch.tensor([111740.0, 98.3, 980130.0, 0.0, 18.5])\n",
    "c0 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[0]).nonzero(as_tuple=True)\n",
    "print(c0)\n",
    "c1 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[1]).nonzero(as_tuple=True)\n",
    "print(c1)\n",
    "c2 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[2]).nonzero(as_tuple=True)\n",
    "print(c2)\n",
    "c3 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[3]).nonzero(as_tuple=True)\n",
    "print(c3)\n",
    "c5 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[4]).nonzero(as_tuple=True)\n",
    "print(c5)\n",
    "# for i in c[1]:\n",
    "#     if i != 3:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.where(torch.from_numpy(SPATIAL_TAGS)== spatial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.where(torch.from_numpy(SPATIAL_TAGS)== spatial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = torch.tensor([34035, 78.06, 174780, 0.0, 64.5]) # synset boat\n",
    "\n",
    "idx, mat, syn = vicinity_matrix(spatial_params=params, target_vocab=target_VOCAB[:20], spatial_tags=SPATIAL_TAGS[:20], k=5)\n",
    "syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat[2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in syn.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.tensor([]).size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not torch.all(torch.eq(spatial_params, torch.zeros(spatial_params.size(0))), dim=0):\n",
    "    print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = torch.tensor([True, False, False, True, True])\n",
    "(t == True).nonzero(as_tuple=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts = torch.tensor([12, 89, -2, 0.2])\n",
    "torch.sort(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topk = torch.topk(ts, 3, largest=False)\n",
    "topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sy = np.array([\"a\", \"b\", \"c\", \"d\"])\n",
    "\n",
    "np.take(sy, topk[1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row = 3\n",
    "col = 3\n",
    "k = 3\n",
    "t = 2\n",
    "vicinity_matrix = torch.empty((row, col, k, t))\n",
    "print(vicinity_matrix)\n",
    "print(vicinity_matrix[2][0])\n",
    "# newt = torch.tensor(topk[1], topk[0])\n",
    "# print(newt)\n",
    "# vicinity_matrix[2, 0] = torch.stack((torch.topk[1], torch.topk[0]), dim=-1)\n",
    "print(vicinity_matrix[2][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Geometric Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write a custom loss function in pytorch for \n",
    "# I need a fct to transform 5 coordinates into 2\n",
    "# and another function to calculate the loss between 2 points, then the total loss is the loss of all losses in a sentence \n",
    "\n",
    "\n",
    "def coo2point(coo):\n",
    "    print(coo)\n",
    "    l0 = coo[0]\n",
    "    alpha = coo[1]\n",
    "    alpha_rad = alpha * math.pi / 180\n",
    "    l_i = coo[2]\n",
    "    beta_i = coo[3]\n",
    "    beta_i_rad = beta_i * math.pi / 180\n",
    "    r = coo[4]\n",
    "    \n",
    "    # np.cos() and np.sin() take angles in radian as params\n",
    "    center_pt = torch.tensor([l0 * math.cos(alpha_rad), l0 * math.sin(alpha_rad)], dtype=torch.float64, requires_grad=True)\n",
    "    sense_pt = center_pt + torch.tensor([l_i * math.cos(alpha_rad + beta_i_rad),\n",
    "                                     l_i * math.sin(alpha_rad + beta_i_rad)], dtype=torch.float64, requires_grad=True)\n",
    "    return sense_pt, center_pt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def distance_loss(pred_pt, original_pt, include_r=False, pt_sphere=False):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two sense points, including radii.\n",
    "    :param pred_pt:\n",
    "    :param original_pt:\n",
    "    :param include_r: if set to true, include radius in the distance. \n",
    "                      It gives more freedom/tolerance degrees to the loss function. \n",
    "                      Loss is satisfied once the predicted point is part of original point.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "        \n",
    "    # original_pt = torch.from_numpy(original_pt)\n",
    "    # print(\"original point\", type(original_pt), original_pt)\n",
    "    \n",
    "    r1 = pred_pt[-1]\n",
    "    r2 = original_pt[-1]\n",
    "\n",
    "    pred_sense, pred_center = coo2point(pred_pt)\n",
    "    orig_sense, orig_center = coo2point(original_pt)\n",
    "    \n",
    "    \n",
    "    loss = torch.linalg.norm(torch.sub(pred_sense, orig_sense)) - r2\n",
    "    \n",
    "    # very strong assumption for the words that are not sense-tagged\n",
    "    # If I want more tolerance, I could neglect those tokens from the beginning\n",
    "    if torch.all(torch.eq(original_pt, torch.zeros(original_pt.size(0))), dim=0):\n",
    "        return loss\n",
    "    \n",
    "    if pt_sphere:\n",
    "        dist = torch.linalg.norm(torch.sub(pred_sense, orig_sense)) + r2\n",
    "        return dist\n",
    "\n",
    "    \n",
    "    if include_r:\n",
    "        \n",
    "        tolerant_loss = r1 + loss - r2\n",
    "    \n",
    "        if tolerant_loss < 0:\n",
    "            tolerant_loss = 0.0\n",
    "        \n",
    "#         if r1 > r2: #case the predicted radius is bigger than actual one\n",
    "#             tolerant_loss = torch.abs(torch.sub(r1, r2))\n",
    "           \n",
    "        return tolerant_loss\n",
    "    \n",
    "    else:\n",
    "        return loss \n",
    "   \n",
    "\n",
    "\n",
    "def geometric_loss(pred_list, label_list, include_r=False):\n",
    "    \n",
    "    # assert that the two lists must be of equal size\n",
    "    pred_size = pred_list.size()[0]\n",
    "    lab_size = label_list.size()[0]\n",
    "    assert pred_size == lab_size\n",
    "    \n",
    "    sentence_loss = 0.0\n",
    "    \n",
    "    # sum over all the tokens in the sentence\n",
    "    for i in range(pred_size):\n",
    "        sentence_loss += distance_loss(pred_list[i], label_list[i], include_r)\n",
    "        \n",
    "    return sentence_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example = torch.tensor([2.8, 45, 1.4, 0, 3.5])\n",
    "example2 = torch.tensor([0.0,0.0,0.0,0.0,0.0])\n",
    "geometric_loss(example, example2, include_r=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example = torch.tensor([2.8, 45, 1.4, 0, 3.5])\n",
    "example2 = torch.tensor([0.0,0.0,0.0,0.0,0.0])\n",
    "geometric_loss(example, example2, include_r=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sy = np.array([\"a\", \"b\", \"c\", \"d\"])\n",
    "\n",
    "np.take(sy, topk[1], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write a custom loss function in pytorch for \n",
    "# I need a fct to transform 5 coordinates into 2\n",
    "# and another function to calculate the loss between 2 points, then the total loss is the loss of all losses in a sentence \n",
    "\n",
    "\n",
    "def coo2point(coo):\n",
    "    print(coo)\n",
    "    l0 = coo[0]\n",
    "    alpha = coo[1]\n",
    "    alpha_rad = alpha * math.pi / 180\n",
    "    l_i = coo[2]\n",
    "    beta_i = coo[3]\n",
    "    beta_i_rad = beta_i * math.pi / 180\n",
    "    r = coo[4]\n",
    "    \n",
    "    # np.cos() and np.sin() take angles in radian as params\n",
    "    center_pt = torch.tensor([l0 * math.cos(alpha_rad), l0 * math.sin(alpha_rad)], dtype=torch.float64, requires_grad=True)\n",
    "    sense_pt = center_pt + torch.tensor([l_i * math.cos(alpha_rad + beta_i_rad),\n",
    "                                     l_i * math.sin(alpha_rad + beta_i_rad)], dtype=torch.float64, requires_grad=True)\n",
    "    return sense_pt, center_pt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def distance_loss(pred_pt, original_pt, include_r=False, pt_sphere=False):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two sense points, including radii.\n",
    "    :param pred_pt:\n",
    "    :param original_pt:\n",
    "    :param include_r: if set to true, include radius in the distance. \n",
    "                      It gives more freedom/tolerance degrees to the loss function. \n",
    "                      Loss is satisfied once the predicted point is part of original point.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    r1 = pred_pt[-1]\n",
    "    r2 = original_pt[-1]\n",
    "\n",
    "    pred_sense, pred_center = coo2point(pred_pt)\n",
    "    orig_sense, orig_center = coo2point(original_pt)\n",
    "    \n",
    "    \n",
    "    loss = torch.linalg.norm(torch.sub(pred_sense, orig_sense)) - r2\n",
    "    \n",
    "    # very strong assumption for the words that are not sense-tagged\n",
    "    # If I want more tolerance, I could neglect those tokens from the beginning\n",
    "    if torch.equal(original_pt, torch.zeros(original_pt.size()[0])):\n",
    "        return loss\n",
    "    \n",
    "    if pt_sphere:\n",
    "        dist = torch.linalg.norm(torch.sub(pred_sense, orig_sense)) + r2\n",
    "        return dist\n",
    "\n",
    "    \n",
    "    if include_r:\n",
    "        \n",
    "        tolerant_loss = r1 + loss - r2\n",
    "    \n",
    "        if tolerant_loss < 0:\n",
    "            tolerant_loss = 0.0\n",
    "        \n",
    "#         if r1 > r2: #case the predicted radius is bigger than actual one\n",
    "#             tolerant_loss = torch.abs(torch.sub(r1, r2))\n",
    "           \n",
    "        return tolerant_loss\n",
    "    \n",
    "    else:\n",
    "        return loss \n",
    "   \n",
    "\n",
    "\n",
    "def geometric_loss(pred_list, label_list, include_r=False):\n",
    "    \n",
    "    # assert that the two lists must be of equal size\n",
    "    pred_size = pred_list.size()[0]\n",
    "    lab_size = label_list.size()[0]\n",
    "    assert pred_size == lab_size\n",
    "    \n",
    "    sentence_loss = 0.0\n",
    "    \n",
    "    # sum over all the tokens in the sentence\n",
    "    for i in range(pred_size):\n",
    "        sentence_loss += distance_loss(pred_list[i], label_list[i], include_r)\n",
    "        \n",
    "    return sentence_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example = torch.tensor([2.8, 45, 1.4, 0, 3.5])\n",
    "example2 = torch.tensor([0.0,0.0,0.0,0.0,0.0])\n",
    "geometric_loss(example, example2, include_r=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l = torch.tensor([[2.8, 45, 1.4, 0, 3.5], [2124, 90, 1000, 14, 0.5]])\n",
    "l.view(-1)\n",
    "l.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(torch.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# <span style=\"color:red\"> WSD as Classification Task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> WSD as Classification Task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderModel(nn.Module):\n",
    "\n",
    "    def __init__(self, weights_matrix:np.ndarray, target_matrix: np.ndarray,\n",
    "                 ntoken: int, out_features:int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.weights_matrix = weights_matrix\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(self.weights_matrix, True)\n",
    "        \n",
    "        # Multi-head attention mechanism is included in TransformerEncoderLayer\n",
    "        # d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=<function relu>, \n",
    "        # layer_norm_eps=1e-05, batch_first=False, norm_first=False, device=None, dtype=None\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout) # activation\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers, norm=None)\n",
    "        \n",
    "        \n",
    "#         padding_idx (int, optional) – If specified, the entries at padding_idx do not contribute to the gradient;\n",
    "#         therefore, the embedding vector at padding_idx is not updated during training,\n",
    "#         i.e. it remains as a fixed “pad”. For a newly constructed Embedding, the embedding vector at\n",
    "#         padding_idx will default to all zeros, but can be updated to another value to be used as the padding vector.\n",
    "        self.emb = nn.Embedding(ntoken, d_model) \n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Linear layer: returns the last hidden state of the encoder \n",
    "        self.fc = nn.Linear(d_model, embedding_dim)\n",
    "        \n",
    "        # No! Here I am just redoing fully connected connections\n",
    "        # Linear Layer: affine transformation of last hidden layer into shape (1, embedding_dim)\n",
    "        #self.context_vec = nn.Linear(d_model, embedding_dim)\n",
    "        \n",
    "        #self.decoder = nn.Linear(d_model, ntoken)\n",
    "        \n",
    "        # Now, I need to have a Linear space that takes the whole/subset dataframe as input, extracts its spatial_context_vec,\n",
    "        # based on Glove-word-vector + spatial_point,\n",
    "        # then calculates softmax on this distribution\n",
    "        # choose the argmax\n",
    "        # get its spatial tags\n",
    "        # calculate distance loss between them\n",
    "        # do backprop! \n",
    "        # Nx300 into Nx227733: matmul product of two matrices Nx300 and 300x227733 --> Nx227733\n",
    "        # apply softmax to get the probabilities\n",
    "        # apply argmax to get the maximum indices\n",
    "        # use the indices to get the synset names as well as the mapping to coordinates\n",
    "        # into Nx5: mapping to the coordinates\n",
    "        \n",
    "        self.target_matrix = target_matrix\n",
    "        \n",
    "        #self.wn_embeddings = nn.Linear(1, target_matrix.shape[0])\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "#         weights_matrix = weights_matrix, \n",
    "#                                     ntoken= # false: 300,\n",
    "#                                     out_features=5,\n",
    "#                                     d_model=300,\n",
    "#                                     d_hid=200,\n",
    "#                                     nlayers=2,\n",
    "#                                     nhead=2,\n",
    "#                                     dropout=0.2\n",
    "        \n",
    "        \n",
    "        # -------------------------------------\n",
    "\n",
    "        # voc_size = len(text_field.vocab)\n",
    "        # print(\"voc_size: \", voc_size )\n",
    "        #\n",
    "        # # Embedding layer. If we're using pre-trained embeddings, copy them\n",
    "        # # into our embedding module.\n",
    "        # self.embedding = nn.Embedding(voc_size, 300)\n",
    "        # print(\"Embedding\", self.embedding)\n",
    "        # if text_field.vocab.vectors is not None:\n",
    "        #     self.embedding.weight = torch.nn.Parameter(TEXT.vocab.vectors)\n",
    "\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.emb.weight.data.uniform_(-initrange, initrange)\n",
    "        # self.decoder.bias.data.zero_()\n",
    "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        #self.output.bias.data.zero_()\n",
    "        #self.output.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        \n",
    "        #src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = torch.mul(self.emb(src), math.sqrt(self.d_model)) #? 1/sqrt!\n",
    "        print(\"Embedding\", src.shape)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        \n",
    "        src = self.pos_encoder(src)\n",
    "        print(\"Positional Encoding\", src.shape)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        \n",
    "        encoder_output = self.transformer_encoder(src) #, src_mask)\n",
    "        print(\"Encoder\", encoder_output.shape)\n",
    "        print(encoder_output)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        \n",
    "        linear_layer = self.fc(encoder_output)\n",
    "        print(\"Linear Layer\", linear_layer.shape)\n",
    "        print(linear_layer)\n",
    "        print('-' * 80)\n",
    "\n",
    "        # calculate the sum/weighted sum/ ?? on the linear layer to get the context vector of size (1, embd_dim)\n",
    "        context_vec = torch.sum(linear_layer, dim=1)\n",
    "        print(\"Final Context Vector\", context_vec.shape)\n",
    "        print(context_vec)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        # calculate the matrix that transforms a context vector into all wordnet embeddings\n",
    "        # expected: Nx223377x300\n",
    "#         wn_layer = self.wn_embeddings(context_vec)\n",
    "#         print(\"WordNet Embedding\", wn_layer.shape)\n",
    "#         print(wn_layer[0])\n",
    "#         print('-' * 80)\n",
    "        \n",
    "        # there must be a calculation here with the \n",
    "        # calculate dot product\n",
    "        # dim -1 is dim = 2 in 1[1[1[]],2[],..., N[]] \n",
    "        #trans_context_vec = torch.transpose(context_vec, 0, 1)\n",
    "        sense_matrix = torch.from_numpy(self.target_matrix).float()\n",
    "        print(\"sense mat\", sense_matrix.shape)\n",
    "        print(sense_matrix[0])\n",
    "        trans_sense_matrix = torch.transpose(sense_matrix, 0, 1)\n",
    "#         dot_prod = torch.sum(torch.matmul(context_vec, trans_sense_matrix), dim=-1)\n",
    "        dot_prod = torch.matmul(context_vec, trans_sense_matrix)\n",
    "\n",
    "        print(\"Dot Product\", dot_prod.shape)\n",
    "        print(dot_prod)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        \n",
    "        # calculate softmax\n",
    "        # expected Nx 227733\n",
    "        softmax = F.softmax(dot_prod)\n",
    "        print(\"Softmax\", softmax.shape)\n",
    "        print(softmax)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        # argmax\n",
    "        max_match = torch.argmax(softmax, dim=1) # dim 1 to indicate row\n",
    "        # expected Nx1\n",
    "        print(\"argmax\", max_match.shape)\n",
    "        print(max_match)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        # calculate softmax on all \n",
    "        \n",
    "        #dec = self.decoder(output)\n",
    "        #print(\"Decoder\")\n",
    "        #print(dec.shape)\n",
    "        #print(dec)\n",
    "        return max_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = defaultdict(list)\n",
    "\n",
    "VOCAB, weights_matrix = load_vocab(data, embed_size=300)\n",
    "\n",
    "# target_VOCAB\n",
    "# SPATIAL_TAGS\n",
    "\n",
    "n_epochs = 3\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    \n",
    "    model = TransformerEncoderModel(weights_matrix = weights_matrix, \n",
    "                                    target_matrix = tsense_matrix,\n",
    "                                    ntoken= len(VOCAB), #300,\n",
    "                                    out_features=5,\n",
    "                                    d_model=300,\n",
    "                                    d_hid=200,\n",
    "                                    nlayers=2,\n",
    "                                    nhead=2,\n",
    "                                    dropout=0.2)\n",
    "    model.to(device)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    #                       Optimizer\n",
    "    # ---------------------------------------------------------------------\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lr = 5.0  # learning rate\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "    # -------\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    loss_sum = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # for transformer\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "    # Training\n",
    "    for batch in training_generator:\n",
    "        \n",
    "        for local_batch, local_labels in batch:\n",
    "            \n",
    "            # Transform list(<string>) to Tensor(<Tensor>)\n",
    "            print(\"Input Sentence\")\n",
    "            print(local_batch)\n",
    "            input_words = local_batch\n",
    "            local_batch = numericalize(local_batch, VOCAB)\n",
    "            print(type(local_batch), local_batch)\n",
    "            \n",
    "            \n",
    "            # Transform List(<Tensor>) to Tensor(<Tensor>)\n",
    "            # I have labels of same length --> this should be no problem for Tensor\n",
    "            local_labels = torch.stack(local_labels)\n",
    "            print(\"Labels\")\n",
    "            print(type(local_labels), len(local_labels), type(local_labels[0]))\n",
    "            print(local_labels)\n",
    "            \n",
    "            # Transfer to GPU\n",
    "            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "            # Model computations\n",
    "            # out outputs the indices of wordnet database\n",
    "            out = model(local_batch)\n",
    "            print(type(out), out.shape)\n",
    "            # predicted synsets\n",
    "            word_synset_list = list(map(target_VOCAB.__getitem__, out))\n",
    "            print(\"Current Predictions\")\n",
    "            print(\"*\" * 100)\n",
    "            for i in range(len(input_words)):\n",
    "                print(\"<{}> predicted as {}\".format(input_words[i], word_synset_list[i]))\n",
    "            #print_pred = '\\n'.join('{} predicted as {}' for _, _ in zip(range(len(input_words)), range(len(word_synset_list)))).format(*input_words, *word_synset_list)\n",
    "            #print(print_pred)\n",
    "            #print(\"<{}> predicted as {}\".format(zip(input_words, word_synset_list)))\n",
    "            print(\"*\" * 100)\n",
    "            \n",
    "            tags = list(map(SPATIAL_TAGS.__getitem__, out))\n",
    "            # tags is a list of arrays of 5 parameters\n",
    "            print(\"Spatial Tags\", len(tags))\n",
    "            \n",
    "            ntokens = len(VOCAB)#300\n",
    "            #loss = criterion(out.view(-1, ntokens), local_labels)\n",
    "            # ignore prediction for the words that have no entry in the system? [0,0,0,0,0]\n",
    "            loss = distance_loss()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # I added this\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            # ---\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "\n",
    "            train_loss = loss_sum / len(local_batch)\n",
    "            history['train_loss'].append(train_loss)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "#         # Validation\n",
    "#         with torch.set_grad_enabled(False):\n",
    "#             for local_batch, local_labels in validation_generator:\n",
    "#                 # Transfer to GPU\n",
    "#                 local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "#                 # Model computations\n",
    "#                 [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X, y = training_set.__getitem__(0)\n",
    "for batch in training_generator:\n",
    "    print(len(batch))\n",
    "    for local_batch, local_label in batch:\n",
    "        print(local_batch, local_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# normalize each label\n",
    "sent_labels = torch.tensor([[8.5479e+04, 6.0380e+01, 1.2985e+05, 9.5740e+01, 5.0000e-01],\n",
    "                            [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
    "                            [1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01],\n",
    "                            [1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]])\n",
    "norm_sent_labels = torch.nn.functional.normalize(sent_labels, p=2.0, dim=1)\n",
    "norm_sent_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_labels\n",
    "# sent_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.transpose(sent_labels, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Output tensor\n",
    "out = torch.tensor([\n",
    "        [[-1.0549, -0.1433, -0.1394, -0.6256,  0.4749],\n",
    "         [-0.0305,  0.0031,  0.0382, -0.0017,  0.5000],\n",
    "         [ 0.5141, -0.0154, -0.3072,  0.4833,  0.3565],\n",
    "         [ 0.9379,  0.5004,  0.1174,  0.0816, -0.2704]],\n",
    "\n",
    "        [[-0.9528,  0.0359, -0.3091, -0.1796,  0.3259],\n",
    "         [ 0.2951,  0.0628, -0.1382, -0.0146,  0.2808],\n",
    "         [ 0.6864,  0.7282, -0.6748,  0.7032,  0.3163],\n",
    "         [ 1.1643,  0.3034,  0.2858,  0.0823, -0.5213]],\n",
    "\n",
    "        [[-0.3433,  0.7988, -0.9953, -0.2597,  0.6321],\n",
    "         [ 0.0388, -0.5481, -0.2017, -0.2700,  0.4406],\n",
    "         [ 0.9477,  0.4574, -0.2327,  0.0848,  0.3336],\n",
    "         [ 0.2500,  0.2523,  0.3734,  0.7667, -0.3530]],\n",
    "\n",
    "        [[-0.7934,  0.4223, -0.1750, -0.5049,  0.6555],\n",
    "         [ 0.0534, -0.6308, -0.7725, -0.3165,  0.3568],\n",
    "         [ 0.3805,  0.2004, -0.3675,  0.2600,  0.2836],\n",
    "         [-0.0449, -0.0765, -0.2250,  0.9507,  0.1856]]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nx300 [[1, ..., 300], ... , N[]] * [[1, ..., 300], ..., 227733[]]\n",
    "# from 4x5 to 4x20x5\n",
    "N = 1\n",
    "n = 5\n",
    "tensor = torch.rand(20, n) \n",
    "print(tensor.shape)\n",
    "print(tensor)\n",
    "\n",
    "trans = torch.transpose(sent_labels, 0, 1)\n",
    "print(trans.shape)\n",
    "print(trans)\n",
    "torch.matmul(tensor, trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.transpose(sent_labels, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Output tensor\n",
    "out = torch.tensor([\n",
    "        [[-1.0549, -0.1433, -0.1394, -0.6256,  0.4749],\n",
    "         [-0.0305,  0.0031,  0.0382, -0.0017,  0.5000],\n",
    "         [ 0.5141, -0.0154, -0.3072,  0.4833,  0.3565],\n",
    "         [ 0.9379,  0.5004,  0.1174,  0.0816, -0.2704]],\n",
    "\n",
    "        [[-0.9528,  0.0359, -0.3091, -0.1796,  0.3259],\n",
    "         [ 0.2951,  0.0628, -0.1382, -0.0146,  0.2808],\n",
    "         [ 0.6864,  0.7282, -0.6748,  0.7032,  0.3163],\n",
    "         [ 1.1643,  0.3034,  0.2858,  0.0823, -0.5213]],\n",
    "\n",
    "        [[-0.3433,  0.7988, -0.9953, -0.2597,  0.6321],\n",
    "         [ 0.0388, -0.5481, -0.2017, -0.2700,  0.4406],\n",
    "         [ 0.9477,  0.4574, -0.2327,  0.0848,  0.3336],\n",
    "         [ 0.2500,  0.2523,  0.3734,  0.7667, -0.3530]],\n",
    "\n",
    "        [[-0.7934,  0.4223, -0.1750, -0.5049,  0.6555],\n",
    "         [ 0.0534, -0.6308, -0.7725, -0.3165,  0.3568],\n",
    "         [ 0.3805,  0.2004, -0.3675,  0.2600,  0.2836],\n",
    "         [-0.0449, -0.0765, -0.2250,  0.9507,  0.1856]]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try normal sum over columns \n",
    "out_sum = torch.sum(out, dim=1)\n",
    "out_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "norm_sent_labels - out_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = defaultdict(list)\n",
    "\n",
    "VOCAB, weights_matrix = load_vocab(data, embed_size=300)\n",
    "\n",
    "n_epochs = 3\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    \n",
    "    model = TransformerEncoderModel(weights_matrix = weights_matrix, \n",
    "                                    ntoken=300,\n",
    "                                    out_features=5,\n",
    "                                    d_model=300,\n",
    "                                    d_hid=200,\n",
    "                                    nlayers=2,\n",
    "                                    nhead=2,\n",
    "                                    dropout=0.2)\n",
    "    model.to(device)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    #                       Optimizer\n",
    "    # ---------------------------------------------------------------------\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lr = 5.0  # learning rate\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "    # -------\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    loss_sum = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # for transformer\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "    # Training\n",
    "    for batch in training_generator:\n",
    "        \n",
    "        for local_batch, local_labels in batch:\n",
    "            \n",
    "            # Transform list(<string>) to Tensor(<Tensor>)\n",
    "            print(\"Input Sentence\")\n",
    "            print(local_batch)\n",
    "            local_batch = numericalize(local_batch, VOCAB)\n",
    "            print(type(local_batch), local_batch)\n",
    "            \n",
    "            \n",
    "            # Transform List(<Tensor>) to Tensor(<Tensor>)\n",
    "            # I have labels of same length --> this should be no problem for Tensor\n",
    "            local_labels = torch.stack(local_labels)\n",
    "            print(\"Labels\")\n",
    "            print(type(local_labels), len(local_labels), type(local_labels[0]))\n",
    "            print(local_labels)\n",
    "            \n",
    "            # Transfer to GPU\n",
    "            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "            # Model computations\n",
    "            out = model(local_batch)\n",
    "            ntokens = 300\n",
    "            loss = criterion(out.view(-1, ntokens), local_labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # I added this\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            # ---\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "\n",
    "            train_loss = loss_sum / len(local_batch)\n",
    "            history['train_loss'].append(train_loss)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "#         # Validation\n",
    "#         with torch.set_grad_enabled(False):\n",
    "#             for local_batch, local_labels in validation_generator:\n",
    "#                 # Transfer to GPU\n",
    "#                 local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "#                 # Model computations\n",
    "#                 [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Output tensor\n",
    "out = torch.tensor([\n",
    "        [[-1.0549, -0.1433, -0.1394, -0.6256,  0.4749],\n",
    "         [-0.0305,  0.0031,  0.0382, -0.0017,  0.5000],\n",
    "         [ 0.5141, -0.0154, -0.3072,  0.4833,  0.3565],\n",
    "         [ 0.9379,  0.5004,  0.1174,  0.0816, -0.2704]],\n",
    "\n",
    "        [[-0.9528,  0.0359, -0.3091, -0.1796,  0.3259],\n",
    "         [ 0.2951,  0.0628, -0.1382, -0.0146,  0.2808],\n",
    "         [ 0.6864,  0.7282, -0.6748,  0.7032,  0.3163],\n",
    "         [ 1.1643,  0.3034,  0.2858,  0.0823, -0.5213]],\n",
    "\n",
    "        [[-0.3433,  0.7988, -0.9953, -0.2597,  0.6321],\n",
    "         [ 0.0388, -0.5481, -0.2017, -0.2700,  0.4406],\n",
    "         [ 0.9477,  0.4574, -0.2327,  0.0848,  0.3336],\n",
    "         [ 0.2500,  0.2523,  0.3734,  0.7667, -0.3530]],\n",
    "\n",
    "        [[-0.7934,  0.4223, -0.1750, -0.5049,  0.6555],\n",
    "         [ 0.0534, -0.6308, -0.7725, -0.3165,  0.3568],\n",
    "         [ 0.3805,  0.2004, -0.3675,  0.2600,  0.2836],\n",
    "         [-0.0449, -0.0765, -0.2250,  0.9507,  0.1856]]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try normal sum over columns \n",
    "out_sum = torch.sum(out, dim=1)\n",
    "out_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "norm_sent_labels - out_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}