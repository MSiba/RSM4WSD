{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import math\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import bcolz\n",
    "\n",
    "import time\n",
    "import random\n",
    "import functools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"../data/test_transformer/\"\n",
    "# split the dataset into training, validation and testing\n",
    "train_path = \"train.csv\"\n",
    "validate_path = \"validate.csv\"\n",
    "test_path = \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "tensor([1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02])\n"
     ]
    }
   ],
   "source": [
    "def to_tensor(string_list):\n",
    "    l_str = []\n",
    "    for ele in string_list:\n",
    "        if ele[0] == \"[\":\n",
    "            l_str.append(ele[1:])\n",
    "        else:\n",
    "            if ele[-1] == \"]\":\n",
    "                l_str.append(ele[:-1])\n",
    "            else:\n",
    "                l_str.append(ele)\n",
    "\n",
    "    str_vec = \" \".join(l_str)\n",
    "    torch_labels = torch.tensor(list(map(float, str_vec.split(' '))), dtype=torch.float32)\n",
    "    return torch_labels\n",
    "\n",
    "t = to_tensor(['[142676.0', '107.17', '71890.08', '0.0', '106.5]'])\n",
    "print(type(t), type(t[0]))\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_data(file):\n",
    "    \"\"\"\n",
    "    reads the stem word and the spatial tag of each token in the .csv file\n",
    "    :param corpus_file:\n",
    "    :param datafields:\n",
    "    :return: List of training data of the form [[tokenized_sentence-1, spatial_tensors],\n",
    "                                                [tokenized_sentence-1, spatial_tensors], ...]\n",
    "    \"\"\"\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        examples = []\n",
    "        words = []\n",
    "        lemmas = []\n",
    "        synset_offset = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                examples.append([lemmas, synset_offset, labels])\n",
    "                words = []\n",
    "                lemmas = []\n",
    "                synset_offset = []\n",
    "                labels = []\n",
    "            else:\n",
    "                columns = line.split()\n",
    "                words.append(columns[0])\n",
    "                lemmas.append(columns[1])\n",
    "                synset_offset.append(columns[-6])\n",
    "                lab = to_tensor(columns[-5:])\n",
    "                labels.append(lab)\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['have',\n",
       "   'the',\n",
       "   'necessary',\n",
       "   'means',\n",
       "   'or',\n",
       "   'skill',\n",
       "   'or',\n",
       "   'know-how',\n",
       "   'or',\n",
       "   'authority',\n",
       "   'to',\n",
       "   'do',\n",
       "   'something'],\n",
       "  ['no-synset',\n",
       "   'no-synset',\n",
       "   'necessary.a.01',\n",
       "   'means.n.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'know-how.n.01',\n",
       "   'no-synset',\n",
       "   'authority.n.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.1174e+05, 9.8310e+01, 9.8013e+04, 0.0000e+00, 1.8500e+01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([3.7587e+04, 1.0434e+02, 1.9497e+05, 0.0000e+00, 7.5000e+00]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['not',\n",
       "   'have',\n",
       "   'the',\n",
       "   'necessary',\n",
       "   'means',\n",
       "   'or',\n",
       "   'skill',\n",
       "   'or',\n",
       "   'know-how'],\n",
       "  ['not.r.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'necessary.a.01',\n",
       "   'means.n.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'know-how.n.01'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([1.1174e+05, 9.8310e+01, 9.8013e+04, 0.0000e+00, 1.8500e+01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02])]],\n",
       " [['have',\n",
       "   'or',\n",
       "   'be',\n",
       "   'many%3|more%3|much%3|more%4|much%4',\n",
       "   'than',\n",
       "   'normal',\n",
       "   'or',\n",
       "   'necessary',\n",
       "   ':'],\n",
       "  ['no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'normal.a.01',\n",
       "   'no-synset',\n",
       "   'necessary.a.01',\n",
       "   'no-synset'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.2872e+05, 2.0550e+01, 5.4535e+04, 7.8460e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['involve',\n",
       "   'deductive_reasoning',\n",
       "   'from',\n",
       "   'a',\n",
       "   'general',\n",
       "   'principle',\n",
       "   'to',\n",
       "   'a',\n",
       "   'necessary',\n",
       "   'effect'],\n",
       "  ['no-synset',\n",
       "   'deduction.n.04',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'general.a.01',\n",
       "   'principle.n.03',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'necessary.a.01',\n",
       "   'no-synset'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([4.9319e+04, 1.0515e+02, 1.6306e+05, 0.0000e+00, 1.5000e+00]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.1143e+05, 3.0800e+01, 8.3244e+04, 7.8460e+01, 5.0000e-01]),\n",
       "   tensor([1.1814e+05, 1.0754e+02, 9.5664e+04, 0.0000e+00, 1.4500e+01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['not', 'support', 'by', 'fact'],\n",
       "  ['not.r.01', 'corroborate.v.03', 'no-synset', 'no-synset'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.4906e+04, 1.0194e+02, 1.1614e+05, 1.8000e+02, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['not', 'support', 'by', 'fact'],\n",
       "  ['not.r.01', 'confirm.v.01', 'no-synset', 'no-synset'],\n",
       "  [tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.4906e+04, 1.0194e+02, 1.1483e+05, 1.8000e+02, 2.8500e+01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['lack', 'necessary', 'physical_ability', 'or', 'mental_ability'],\n",
       "  ['miss.v.06',\n",
       "   'necessary.a.01',\n",
       "   'physical_ability.n.01',\n",
       "   'no-synset',\n",
       "   'capacity.n.08'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([5.1520e+03, 1.1805e+02, 2.2714e+05, 0.0000e+00, 2.5000e+00]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([8.6015e+04, 1.0743e+02, 1.2885e+05, 0.0000e+00, 1.5000e+00])]],\n",
       " [['the', 'necessary', 'consequence', 'of', 'one', 'action'],\n",
       "  ['no-synset',\n",
       "   'necessary.s.02',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 7.6603e+04, 9.5740e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['have',\n",
       "   'every',\n",
       "   'necessary',\n",
       "   'or',\n",
       "   'normal',\n",
       "   'part',\n",
       "   'or',\n",
       "   'component',\n",
       "   'or',\n",
       "   'step'],\n",
       "  ['no-synset',\n",
       "   'no-synset',\n",
       "   'necessary.a.01',\n",
       "   'no-synset',\n",
       "   'normal.a.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.2872e+05, 2.0550e+01, 5.4535e+04, 7.8460e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['have',\n",
       "   'or',\n",
       "   'display',\n",
       "   'all',\n",
       "   'the',\n",
       "   'characteristic',\n",
       "   'necessary',\n",
       "   'for',\n",
       "   'completeness'],\n",
       "  ['no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'feature.n.01',\n",
       "   'necessary.a.01',\n",
       "   'no-synset',\n",
       "   'completeness.n.01'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([5.0349e+04, 1.0389e+02, 1.6340e+05, 0.0000e+00, 2.4500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.6165e+04, 1.1673e+02, 2.1134e+05, 0.0000e+00, 3.5000e+00])]],\n",
       " [['have',\n",
       "   'or',\n",
       "   'display',\n",
       "   'all',\n",
       "   'the',\n",
       "   'characteristic',\n",
       "   'necessary',\n",
       "   'for',\n",
       "   'completeness'],\n",
       "  ['no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'characteristic.n.02',\n",
       "   'necessary.a.01',\n",
       "   'no-synset',\n",
       "   'completeness.n.01'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([5.0349e+04, 1.0389e+02, 1.8358e+05, 0.0000e+00, 3.9500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.6165e+04, 1.1673e+02, 2.1134e+05, 0.0000e+00, 3.5000e+00])]],\n",
       " [['(',\n",
       "   'of',\n",
       "   'a',\n",
       "   'boat',\n",
       "   'or',\n",
       "   'vessel',\n",
       "   ')',\n",
       "   'furnished',\n",
       "   'with',\n",
       "   'necessary',\n",
       "   'official_document',\n",
       "   'specify',\n",
       "   'ownership',\n",
       "   'etc.'],\n",
       "  ['no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'boat.n.01',\n",
       "   'no-synset',\n",
       "   'vessel.n.02',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'necessary.a.01',\n",
       "   'legal_document.n.01',\n",
       "   'no-synset',\n",
       "   'ownership.n.03',\n",
       "   'and_so_forth.r.01'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([3.4035e+04, 7.8060e+01, 1.7478e+05, 0.0000e+00, 6.4500e+01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([6.9658e+04, 7.3010e+01, 1.3975e+05, 0.0000e+00, 1.8950e+02]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([7.7181e+04, 1.1327e+02, 1.4597e+05, 0.0000e+00, 2.8850e+02]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([6.2580e+03, 1.0677e+02, 2.2125e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([8.7896e+04, 1.7910e+02, 6.2122e+04, 0.0000e+00, 5.0000e-01])]],\n",
       " [['lack',\n",
       "   'necessary',\n",
       "   'document',\n",
       "   '(',\n",
       "   'as',\n",
       "   'for',\n",
       "   'e.g.',\n",
       "   'permission',\n",
       "   'to',\n",
       "   'live',\n",
       "   'or',\n",
       "   'work',\n",
       "   'in',\n",
       "   'a',\n",
       "   'country',\n",
       "   ')'],\n",
       "  ['miss.v.06',\n",
       "   'necessary.a.01',\n",
       "   'document.n.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'for_example.r.01',\n",
       "   'license.n.04',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'country.n.02',\n",
       "   'no-synset'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([6.4798e+04, 1.0888e+02, 1.5864e+05, 0.0000e+00, 3.9050e+02]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([6.6105e+04, 1.7943e+02, 8.3903e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([4.5885e+04, 1.0822e+02, 1.6395e+05, 0.0000e+00, 1.3500e+01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.4370e+03, 9.3970e+01, 2.0489e+05, 0.0000e+00, 2.2750e+02]),\n",
       "   tensor([0., 0., 0., 0., 0.])]],\n",
       " [['lack', 'necessary', 'force', 'for', 'effectiveness'],\n",
       "  ['miss.v.06',\n",
       "   'necessary.a.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'effectiveness.n.01'],\n",
       "  [tensor([6.0597e+04, 1.2401e+02, 1.2927e+05, 1.8000e+02, 2.5000e+00]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3004e+05, 1.1761e+02, 1.0131e+05, 0.0000e+00, 2.5000e+00])]],\n",
       " [['use',\n",
       "   'the',\n",
       "   'minimum',\n",
       "   'of',\n",
       "   'time',\n",
       "   'or',\n",
       "   'resource',\n",
       "   'necessary',\n",
       "   'for',\n",
       "   'effectiveness'],\n",
       "  ['no-synset',\n",
       "   'no-synset',\n",
       "   'minimum.n.01',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'no-synset',\n",
       "   'resource.n.01',\n",
       "   'necessary.a.01',\n",
       "   'no-synset',\n",
       "   'effectiveness.n.01'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([8.8864e+04, 7.5350e+01, 1.4785e+05, 5.3130e+01, 1.5000e+00]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.8408e+04, 1.0795e+02, 1.8866e+05, 0.0000e+00, 1.2500e+01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3004e+05, 1.1761e+02, 1.0131e+05, 0.0000e+00, 2.5000e+00])]],\n",
       " [['possible', 'but', 'not', 'necessary'],\n",
       "  ['possible.a.01', 'no-synset', 'not.r.01', 'necessary.a.01'],\n",
       "  [tensor([8.5479e+04, 6.0380e+01, 1.2985e+05, 9.5740e+01, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " [['leave', 'to', 'personal', 'choice'],\n",
       "  ['no-synset', 'no-synset', 'personal.a.01', 'choice.n.02'],\n",
       "  [tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([2.7140e+04, 1.9770e+01, 1.2471e+05, 2.5840e+01, 5.0000e-01]),\n",
       "   tensor([1.4028e+05, 7.7850e+01, 1.0613e+05, 6.0000e+01, 5.9500e+01])]],\n",
       " [['morally', 'bind', 'or', 'necessary'],\n",
       "  ['morally.r.01', 'no-synset', 'no-synset', 'necessary.a.01'],\n",
       "  [tensor([2.4772e+04, 1.7990e+02, 1.2524e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([0., 0., 0., 0., 0.]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]],\n",
       " [['urgently', 'need'],\n",
       "  ['urgently.r.01', 'need.v.03'],\n",
       "  [tensor([2.6208e+04, 1.7894e+02, 1.2382e+05, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([8.9570e+03, 1.0177e+02, 7.9864e+04, 1.8000e+02, 5.0000e-01])]],\n",
       " [['absolutely', 'necessary'],\n",
       "  ['absolutely.r.02', 'necessary.a.01'],\n",
       "  [tensor([8.1604e+04, 1.7865e+02, 6.8438e+04, 0.0000e+00, 5.0000e-01]),\n",
       "   tensor([1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01])]]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = parse_data(path + train_path)\n",
    "# validation_data = parse_data(path + validate_path)\n",
    "# testing_data = parse_data(path + test_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_id(data):\n",
    "\n",
    "    # data_collector = {\"0\": [[], []], \"1\": [[],[]], ...}\n",
    "    data_collector = {}\n",
    "    for i, instance in enumerate(data):\n",
    "        data_collector[str(i)] = instance\n",
    "\n",
    "    return data_collector\n",
    "\n",
    "# in my case, this data must be shuffled before continuing!\n",
    "datasetID = data_id(data)\n",
    "# datasetID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create dict of key-synset\n",
    "train_val_syn = {}\n",
    "train_val_syn.fromkeys(datasetID.keys())\n",
    "for key, ele in datasetID.items():\n",
    "    train_val_syn[key] = ele[1]\n",
    "# train_val_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# partition data in training/validation\n",
    "splittings = {}\n",
    "labels = {}\n",
    "labels.fromkeys(datasetID.keys())\n",
    "for key, ele in datasetID.items():\n",
    "    labels[key] = ele[2]\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_train = 10\n",
    "N_valid = 5\n",
    "N_test = 5\n",
    "# choose N training instances, randomly!\n",
    "splittings[\"train\"] = random.sample(list(datasetID), N_train)\n",
    "splittings[\"validate\"] = random.sample(list(set(datasetID) - set(splittings[\"train\"])), N_valid)\n",
    "# splittings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Saving and loading data to/from .pt\n",
    "# save\n",
    "# torch.save(datasetID, path + \"pwngc_id.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['have',\n",
       " 'the',\n",
       " 'necessary',\n",
       " 'means',\n",
       " 'or',\n",
       " 'skill',\n",
       " 'or',\n",
       " 'know-how',\n",
       " 'or',\n",
       " 'authority',\n",
       " 'to',\n",
       " 'do',\n",
       " 'something']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = torch.load(path + \"pwngc_id.pt\")[\"0\"][0]\n",
    "li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, list_ids, labels, train_val_syn):\n",
    "        self.labels = labels\n",
    "        self.list_ids = list_ids\n",
    "        self.train_val_syn = train_val_syn\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Total Number of samples.\"\n",
    "\n",
    "        return len(self.list_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Extracts one Example of data.\"\n",
    "\n",
    "        id = self.list_ids[index]\n",
    "\n",
    "        # data\n",
    "        X = torch.load(path + \"pwngc_id.pt\")[id][0]\n",
    "        y = self.labels[id]\n",
    "        tag_y = self.train_val_syn[id]\n",
    "\n",
    "        return X, y, tag_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Downloading GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I downloaded it from torchtext\n",
    "glove_path = \"./.vector_cache\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#import bcolz\n",
    "# # initial code from: https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n",
    "## list of words in GloVe\n",
    "#words = []\n",
    "## index of words in GloVe\n",
    "#idx = 0\n",
    "## assign an index to each word in GloVe to ease embedding while training\n",
    "## e.g. {\"Hi\":0, \"the\":1, ...}\n",
    "##word2idx = {}\n",
    "\n",
    "# #I downloaded it from torchtext\n",
    "#glove_path = \"./.vector_cache\"\n",
    "\n",
    "#vectors = bcolz.carray(np.zeros((1,300), dtype=np.float32), rootdir=f'{glove_path}/840B.300d.dat', mode='w')\n",
    "\n",
    "#glove = {}\n",
    "\n",
    "# with open(f'{glove_path}/glove.840B.300d.txt', 'rb') as f:\n",
    "#     for l in f:\n",
    "#         try:\n",
    "#             line = l.decode().split()\n",
    "#             word = line[0]\n",
    "            \n",
    "#             #word2idx[word] = idx\n",
    "#             #idx += 1\n",
    "#             vec = np.array(line[1:], dtype=np.float32)\n",
    "#             # print(vec)\n",
    "#             vectors.append(vec)\n",
    "#             words.append(word)\n",
    "#             #print(word)\n",
    "#             idx += 1\n",
    "#             print(idx)\n",
    "#             glove[word] = vec\n",
    "#             #idx += 1\n",
    "#         except:\n",
    "#             f.__next__()\n",
    "#             #words = [\". . . . .\", \". . .\", \"at name@domain.com\"]\n",
    "#             #splits = [len(word.split()) for word in words]\n",
    "#             #for i, j in enumerate(splits):\n",
    "#             #    if line[0: j] == words[i].split() and isinstance(line[j], np.float32):\n",
    "#             #        words.append(words[i])\n",
    "#              #       word2idx[words[i]] = idx\n",
    "#               #      #idx += 1\n",
    "#                #     vec = np.array(line[j:], dtype=np.float32)\n",
    "#                 #    vectors.append(vec)\n",
    "#                  #   idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(vectors.shape)\n",
    "# wordy = [\". . . . .\", \". . .\", \"at name@domain.com\"]\n",
    "# w = wordy[2].split()\n",
    "# print(w)\n",
    "# #word2idx['.\\xa0.\\xa0.']\n",
    "#\n",
    "# print(\". . .\" in words)\n",
    "# print(\". . . . .\" in words) # False, because it transforms '.' to float, then checks if it is a float!\n",
    "# print(\"at name@domain.com\" in words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(len(words))\n",
    "# print(len(vectors))\n",
    "# print(len(glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# From their website: https://nlp.stanford.edu/projects/glove/\n",
    "# Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download)\n",
    "# vectors = bcolz.carray(vectors[1:], rootdir=f'{glove_path}/840B.300.dat', mode='w')\n",
    "# vectors.flush()\n",
    "# pickle.dump(words, open(f'{glove_path}/840B.300_words.pkl', 'wb'))\n",
    "# #pickle.dump(word2idx, open(f'{glove_path}/840B.300_idx.pkl', 'wb'))\n",
    "# pickle.dump(glove, open(f'{glove_path}/840B.300_glove.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vectors = bcolz.open(f'{glove_path}/840B.300.dat')[:]\n",
    "# words = pickle.load(open(f'{glove_path}/840B.300_words.pkl', 'rb'))\n",
    "# #word2idx = pickle.load(open(f'{glove_path}/840B.300_idx.pkl', 'rb'))\n",
    "# #glove = {w: vectors[word2idx[w]] for w in words}\n",
    "glove = pickle.load(open(f'{glove_path}/840B.300_glove.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2195846\n"
     ]
    }
   ],
   "source": [
    "print(len(glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_vocab(data, embed_size=300):\n",
    "    \n",
    "    # insert all dataset vocabulary\n",
    "    dataset_vocab = []\n",
    "    for instance in data:\n",
    "        dataset_vocab += instance[0]\n",
    "        \n",
    "    #print(len(dataset_vocab))\n",
    "    \n",
    "    # remove duplicates\n",
    "    target_vocab = set(dataset_vocab)\n",
    "    \n",
    "    # generate weights matrix using glove\n",
    "    matrix_len = len(target_vocab)\n",
    "    \n",
    "    weights_matrix = np.zeros((matrix_len, embed_size))\n",
    "    \n",
    "    words_found = 0\n",
    "\n",
    "    for i, word in enumerate(target_vocab):\n",
    "        #print(i, word)\n",
    "        try:\n",
    "            weights_matrix[i] = glove[word]\n",
    "            #print(weights_matrix[i])\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            weights_matrix[i] = np.random.normal(scale=0.6, size=(embed_size, ))\n",
    "            #print(weights_matrix[i])\n",
    "    #print(words_found)\n",
    "    \n",
    "    return target_vocab, weights_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate the Dataset in dataloader\n",
    "# Define the model\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 5, #64,\n",
    "          'shuffle': True,\n",
    "          'collate_fn': lambda x: x,\n",
    "          'num_workers': 0}#6}\n",
    "max_epochs = 10 #100\n",
    "\n",
    "# Datasets\n",
    "# partition = # IDs\n",
    "# labels = # Labels\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(splittings['train'], labels, train_val_syn)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(splittings['validate'], labels, train_val_syn)\n",
    "validation_generator = torch.utils.data.DataLoader(validation_set, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for batch in training_generator:\n",
    "    print(len(batch))\n",
    "    print(type(batch))\n",
    "    for local_batch, local_label, syn in batch:\n",
    "        print(local_batch,local_label, syn)\n",
    "        break\n",
    "        #print(local_batch, local_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def numericalize(tokens_list, vocab):\n",
    "    \n",
    "    str2num = {word: index for index, word in enumerate(vocab)}\n",
    "    num_list = []\n",
    "    for token in tokens_list:\n",
    "        num_list.append(str2num[token])\n",
    "        \n",
    "    return torch.tensor(num_list, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the target vocab dataframe containing senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spatial_wordnet = pd.read_pickle(\"../data/wordnet_dataframes/SPATIAL_WORDNET.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spatial_wordnet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add a column denoting the word-sense coordinates based on [l0, alpha, l_i, beta_i, radius]\n",
    "\n",
    "def decode_params(spatial_params):\n",
    "    l0 = spatial_params[0]\n",
    "    alpha = spatial_params[1]\n",
    "    alpha_rad = alpha * np.pi / 180\n",
    "    l_i = spatial_params[2]\n",
    "    beta_i = spatial_params[3]\n",
    "    beta_i_rad = beta_i * np.pi / 180\n",
    "    r = spatial_params[4]\n",
    "    return l0, alpha, alpha_rad, l_i, beta_i, beta_i_rad, r\n",
    "\n",
    "\n",
    "def point_in_space(spatial_params):\n",
    "    l0, alpha, alpha_rad, l_i, beta_i, beta_i_rad, r = decode_params(spatial_params)\n",
    "    # np.cos() and np.sin() take angles in radian as params\n",
    "    center_pt = np.array([l0*np.cos(alpha_rad), l0 * np.sin(alpha_rad)])\n",
    "    sense_pt = center_pt + np.array([l_i * np.cos(alpha_rad + beta_i_rad),\n",
    "                                     l_i * np.sin(alpha_rad + beta_i_rad)])\n",
    "    return sense_pt, center_pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spatial_wordnet[\"sense_coo\"] = spatial_wordnet.apply(lambda row: point_in_space([row.l0, row.alpha, row.l_i, row.beta_i, row.radius])[0], axis=1)\n",
    "spatial_wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop the rows of verb_root, adjective_root, and adverb_root\n",
    "spatial_wordnet  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arr = list(spatial_wordnet.sense_coo)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ex = np.random.rand(300) \n",
    "ex[0] += arr[0]\n",
    "ex[1] += arr[1]\n",
    "print(ex)\n",
    "len(ex)\n",
    "#normalize\n",
    "no = ex/np.linalg.norm(ex)\n",
    "no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng()\n",
    "numbers = rng.choice(20, size=10, replace=False)\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cpp = list(zip(spatial_wordnet.l0, spatial_wordnet.alpha, spatial_wordnet.l_i, spatial_wordnet.beta_i, spatial_wordnet.radius))\n",
    "type(cpp[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now I need to transform this dataset to a target vocab\n",
    "# How is a target vocab defined?\n",
    "# each word/synset? has a number (it can be the index of in the dataframe)\n",
    "# each of word has a Glove vector\n",
    "# add the point to it\n",
    "# add the sense coordinate\n",
    "# normalize\n",
    "def embed_target_vocab(df, embed_size=300):\n",
    "    \n",
    "    # store (word, synset)-tuples in a list\n",
    "    word_synset = list(zip(df.word, df.synset))\n",
    "    \n",
    "    # store indices in a list to numericalize the vocabulary of wordnet\n",
    "    indices = list(df.index)\n",
    "    \n",
    "    senses_coo = list(df.sense_coo)\n",
    "    \n",
    "    spatial_tags = list(zip(df.l0, df.alpha, df.l_i, df.beta_i, df.radius))\n",
    "    \n",
    "    \n",
    "    # generate weights matrix using glove\n",
    "    matrix_len = len(word_synset)\n",
    "    \n",
    "    weights_matrix = np.zeros((matrix_len, embed_size))\n",
    "    \n",
    "    words_found = 0\n",
    "\n",
    "    for i, word in enumerate(word_synset):\n",
    "        #print(i, word)\n",
    "        try:\n",
    "            weights_matrix[i] = glove[word[0]]\n",
    "            print(\"found GLOVE :) \")\n",
    "            #print(weights_matrix[i])\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            weights_matrix[i] = np.random.normal(scale=0.6, size=(embed_size, ))\n",
    "            print(\"Random Vector\")\n",
    "            #print(weights_matrix[i])\n",
    "    #print(words_found)\n",
    "    \n",
    "    # add the parameters to the sense vector\n",
    "    # add them in a way, that no all of them are in the same part of the 300-dim Glove vector\n",
    "    # Why?\n",
    "    # because this will help learn some patterns in the backpropagation, instead of adding all vectors to the first two columns\n",
    "    # only\n",
    "    # the randomness for choosing the indices to add the coordinates could result in better diversity, and thus better learning\n",
    "    # choose radom integers without replacement\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    x_y_indices = rng.choice(embed_size, size=matrix_len*2, replace=True)\n",
    "    x_col = x_y_indices[:matrix_len]\n",
    "    y_col = x_y_indices[matrix_len:]\n",
    "    \n",
    "    \n",
    "    sense_matrix = np.copy(weights_matrix)\n",
    "    for i, coo in enumerate(senses_coo):\n",
    "        sense_matrix[i][x_col[i]] += coo[0]\n",
    "        sense_matrix[i][y_col[i]] += coo[1]\n",
    "        # normalize\n",
    "        sense_matrix[i] = sense_matrix[i] / np.linalg.norm(sense_matrix[i])\n",
    "    \n",
    "\n",
    "    return weights_matrix, sense_matrix, word_synset, spatial_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# glove_wordnet, sense_matrix, word_synset_VOCAB, spatial_tags = embed_target_vocab(spatial_wordnet, embed_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.save(\"target_sense_matrix_exp_01.npy\", sense_matrix)\n",
    "\n",
    "# # to load:\n",
    "# np.load(\"target_sense_matrix_exp_01.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.save(\"glove_wordnet_matrix_exp_01.npy\", glove_wordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.save(\"WORDNET_VOCAB_exp_01.npy\", word_synset_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.save(\"WORDNET_SPATIAL_TAGS_exp_01.npy\", spatial_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the argmax sense from softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tsense_matrix = np.load(\"target_sense_matrix_exp_01.npy\")\n",
    "tsense_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227733, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['.22-caliber', '.22_caliber.a.01'],\n",
       "       ['.22-calibre', '.22_caliber.a.01']], dtype='<U76')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_VOCAB = np.load(\"WORDNET_VOCAB_exp_01.npy\")\n",
    "print(target_VOCAB.shape)\n",
    "target_VOCAB[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227733, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPATIAL_TAGS = np.load(\"WORDNET_SPATIAL_TAGS_exp_01.npy\")\n",
    "SPATIAL_TAGS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def inside_sphere(point, sphere_coo):\n",
    "\n",
    "    pt = point_in_space(point)\n",
    "    sphere_sense, sphere_center = point_in_space(sphere_coo)\n",
    "\n",
    "    sphere_rad = sphere_coo[-1] # in angles\n",
    "\n",
    "    contained = (pt[0] - sphere_sense[0])**2 + (pt[1] - sphere_sense[1])**2 <= sphere_rad**2\n",
    "\n",
    "    if contained:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def distance_loss(pred_pt, original_pt, include_r=False):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two sense points, including radii.\n",
    "    :param pred_pt:\n",
    "    :param original_pt:\n",
    "    :param include_r: if set to true, include radius in the distance. \n",
    "                      It gives more freedom/tolerance degrees to the loss function. \n",
    "                      Loss is satisfied once the predicted point is part of original point.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    r1 = pred_pt[-1]\n",
    "    r2 = original_pt[-1]\n",
    "    pred_sense, pred_center = point_in_space(pred_pt)\n",
    "    orig_sense, orig_center = point_in_space(original_pt)\n",
    "    \n",
    "    loss = np.linalg.norm(pred_sense - orig_sense)\n",
    "    tolerant_loss = r1 + loss - r2\n",
    "    if loss < 0:\n",
    "        tolerant_loss = 0\n",
    "    \n",
    "    if include_r:\n",
    "        return tolerant_loss\n",
    "    else:\n",
    "        return loss \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sphere_dist(pred_pt, original_pt):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two 2D spheres.\n",
    "    :param pred_pt:\n",
    "    :param original_pt:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pred_sense, pred_center = point_in_space(pred_pt)\n",
    "    pred_radius = pred_pt[-1]\n",
    "    orig_sense, orig_center = point_in_space(original_pt)\n",
    "    orig_radius = original_pt[-1]\n",
    "\n",
    "    return (pred_radius + orig_radius -\n",
    "            np.linalg.norm(pred_sense - orig_sense))\n",
    "\n",
    "def decode_prediction(spatial_params, df=\"SPATIAL_WORDNET.pickle\") -> [str]:\n",
    "    \"\"\"\n",
    "    Projects the predicted spatial parameters into the embedding space.\n",
    "    Returns the synsets in the vacinity of the projected point.\n",
    "    :param spatial_params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    synsets = [] # sort from specific to most general\n",
    "\n",
    "    sense_pt, center_pt = point_in_space(spatial_params)\n",
    "\n",
    "    spatial_df = pd.read_pickle(df)\n",
    "    # get the spheres, where the point/point+radius is contained/overlaping/near\n",
    "\n",
    "    # 1. check if the predicted point is contained in some sense\n",
    "    spatial_df[\"contained\"] = spatial_df.apply(lambda row:\n",
    "                                               inside_sphere(spatial_params,\n",
    "                                                             row[['l0', 'alpha', 'l_i', 'beta_i', 'radius']]))\n",
    "\n",
    "    # 2. For those synsets, which is the nearest synset point\n",
    "    #use distance() to calculate distance between centers\n",
    "\n",
    "    # 3. If None of the synsets apply to that word sense\n",
    "    # use sphere_dist to find the nearest sphere (most general synset), and assign it to that synset\n",
    "    # (this maybe good for rare senses)\n",
    "\n",
    "\n",
    "    return synsets\n",
    "\n",
    "def train_loss(tmp_pred, synset_params):\n",
    "    # Loss is the distance between the two spheres/containment of the word within that sphere\n",
    "    # radius acts as tolerance!\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    weights_matrix = torch.from_numpy(weights_matrix)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# emb, n, d = create_emb_layer(weights_matrix)\n",
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokens = torch.tensor([0,5,9], dtype=torch.long)\n",
    "# len(emb(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: np.ndarray, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self, weights_matrix:np.ndarray, \n",
    "                 ntoken: int, out_features:int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.weights_matrix = weights_matrix\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(self.weights_matrix, True)\n",
    "        \n",
    "        # Multi-head attention mechanism is included in TransformerEncoderLayer\n",
    "        # d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=<function relu>, \n",
    "        # layer_norm_eps=1e-05, batch_first=False, norm_first=False, device=None, dtype=None\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout) # activation\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers, norm=None)\n",
    "        \n",
    "        \n",
    "#         padding_idx (int, optional) – If specified, the entries at padding_idx do not contribute to the gradient;\n",
    "#         therefore, the embedding vector at padding_idx is not updated during training,\n",
    "#         i.e. it remains as a fixed “pad”. For a newly constructed Embedding, the embedding vector at\n",
    "#         padding_idx will default to all zeros, but can be updated to another value to be used as the padding vector.\n",
    "        self.emb = nn.Embedding(ntoken, d_model) \n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Linear layer: returns the last hidden state of the encoder \n",
    "        self.fc = nn.Linear(d_model, embedding_dim)\n",
    "        \n",
    "        # No! Here I am just redoing fully connected connections\n",
    "        # Linear Layer: affine transformation of last hidden layer into shape (1, embedding_dim)\n",
    "        #self.context_vec = nn.Linear(d_model, embedding_dim)\n",
    "        \n",
    "        #self.decoder = nn.Linear(d_model, ntoken)\n",
    "        \n",
    "        # Now, I need to have a Linear space that takes the whole/subset dataframe as input, extracts its spatial_context_vec,\n",
    "        # based on Glove-word-vector + spatial_point,\n",
    "        # then calculates softmax on this distribution\n",
    "        # choose the argmax\n",
    "        # get its spatial tags\n",
    "        # calculate distance loss between them\n",
    "        # do backprop! \n",
    "        # Nx300 into Nx227733: matmul product of two matrices Nx300 and 300x227733 --> Nx227733\n",
    "        # apply softmax to get the probabilities\n",
    "        # apply argmax to get the maximum indices\n",
    "        # use the indices to get the synset names as well as the mapping to coordinates\n",
    "        # into Nx5: mapping to the coordinates\n",
    "        \n",
    "        self.output = nn.Linear(embedding_dim, 5)\n",
    "        #self.wn_embeddings = nn.Linear(1, target_matrix.shape[0])\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "#         weights_matrix = weights_matrix, \n",
    "#                                     ntoken= # false: 300,\n",
    "#                                     out_features=5,\n",
    "#                                     d_model=300,\n",
    "#                                     d_hid=200,\n",
    "#                                     nlayers=2,\n",
    "#                                     nhead=2,\n",
    "#                                     dropout=0.2\n",
    "        \n",
    "        \n",
    "        # -------------------------------------\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        \"initialize weights using uniform distribution\"\n",
    "        initrange = 0.1\n",
    "        self.emb.weight.data.uniform_(-initrange, initrange)\n",
    "        # self.decoder.bias.data.zero_()\n",
    "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        #self.output.bias.data.zero_()\n",
    "        #self.output.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        \n",
    "        #src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = torch.mul(self.emb(src), math.sqrt(self.d_model)) #? 1/sqrt!\n",
    "#         print(\"Embedding\", src.shape)\n",
    "#         print('-' * 80)\n",
    "        \n",
    "        \n",
    "        src = self.pos_encoder(src)\n",
    "#         print(\"Positional Encoding\", src.shape)\n",
    "#         print('-' * 80)\n",
    "        \n",
    "        \n",
    "        encoder_output = self.transformer_encoder(src) #, src_mask)\n",
    "#         print(\"Encoder\", encoder_output.shape)\n",
    "#         # print(encoder_output)\n",
    "#         print('-' * 80)\n",
    "        \n",
    "        \n",
    "        linear_layer = self.fc(encoder_output)\n",
    "#         print(\"Linear Layer\", linear_layer.shape)\n",
    "#         # print(linear_layer)\n",
    "#         print('-' * 80)\n",
    "\n",
    "        # calculate the sum/weighted sum/ ?? on the linear layer to get the context vector of size (1, embd_dim)\n",
    "        context_vec = torch.sum(linear_layer, dim=1)\n",
    "#         print(\"Final Context Vector\", context_vec.shape)\n",
    "#         # print(context_vec)\n",
    "#         print('-' * 80)\n",
    "        \n",
    "        # regression output\n",
    "        coordinates = self.output(context_vec)\n",
    "#         print(\"Coordinates from Context Vector\", coordinates.size())\n",
    "#         # print(coordinates)\n",
    "#         print('-'*80)\n",
    "        return coordinates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometric Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coo2point(coo):\n",
    "    # print(coo)\n",
    "    l0 = coo[0]\n",
    "    alpha = coo[1]\n",
    "    alpha_rad = alpha * math.pi / 180\n",
    "    l_i = coo[2]\n",
    "    beta_i = coo[3]\n",
    "    beta_i_rad = beta_i * math.pi / 180\n",
    "    r = coo[4]\n",
    "    \n",
    "    # np.cos() and np.sin() take angles in radian as params\n",
    "    center_pt = torch.tensor([l0 * math.cos(alpha_rad), l0 * math.sin(alpha_rad)], dtype=torch.float64, requires_grad=True)\n",
    "    sense_pt = center_pt + torch.tensor([l_i * math.cos(alpha_rad + beta_i_rad),\n",
    "                                     l_i * math.sin(alpha_rad + beta_i_rad)], dtype=torch.float64, requires_grad=True)\n",
    "    return sense_pt, center_pt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def distance_loss(pred_pt, original_pt, include_r=False, pt_sphere=False):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two sense points, including radii.\n",
    "    :param pred_pt:\n",
    "    :param original_pt:\n",
    "    :param include_r: if set to true, include radius in the distance. \n",
    "                      It gives more freedom/tolerance degrees to the loss function. \n",
    "                      Loss is satisfied once the predicted point is part of original point.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "        \n",
    "    # original_pt = torch.from_numpy(original_pt)\n",
    "    # print(\"original point\", type(original_pt), original_pt)\n",
    "    \n",
    "    r1 = pred_pt[-1]\n",
    "    r2 = original_pt[-1]\n",
    "\n",
    "    pred_sense, pred_center = coo2point(pred_pt)\n",
    "    orig_sense, orig_center = coo2point(original_pt)\n",
    "    \n",
    "    \n",
    "    loss = torch.linalg.norm(torch.sub(pred_sense, orig_sense)) - r2\n",
    "    \n",
    "    # very strong assumption for the words that are not sense-tagged\n",
    "    # If I want more tolerance, I could neglect those tokens from the beginning\n",
    "    if torch.all(torch.eq(original_pt, torch.zeros(original_pt.size(0))), dim=0):\n",
    "        return loss\n",
    "    \n",
    "    if pt_sphere:\n",
    "        dist = torch.linalg.norm(torch.sub(pred_sense, orig_sense)) + r2\n",
    "        return dist\n",
    "\n",
    "    \n",
    "    if include_r:\n",
    "        \n",
    "        tolerant_loss = r1 + loss - r2\n",
    "    \n",
    "        if tolerant_loss < 0:\n",
    "            tolerant_loss = 0.0\n",
    "        \n",
    "#         if r1 > r2: #case the predicted radius is bigger than actual one\n",
    "#             tolerant_loss = torch.abs(torch.sub(r1, r2))\n",
    "           \n",
    "        return tolerant_loss\n",
    "    \n",
    "    else:\n",
    "        return loss \n",
    "   \n",
    "\n",
    "\n",
    "def geometric_loss(pred_list, label_list, include_r=False):\n",
    "    \n",
    "    # assert that the two lists must be of equal size\n",
    "    pred_size = pred_list.size()[0]\n",
    "    lab_size = label_list.size()[0]\n",
    "    assert pred_size == lab_size\n",
    "    \n",
    "    sentence_loss = 0.0\n",
    "    \n",
    "    # sum over all the tokens in the sentence\n",
    "    for i in range(pred_size):\n",
    "        sentence_loss += distance_loss(pred_list[i], label_list[i], include_r)\n",
    "        \n",
    "    return sentence_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sense Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_contained(pred, sphere_coo, compare_spheres=False):\n",
    "\n",
    "    pt, word = coo2point(pred)\n",
    "    sphere_sense, sphere_center = coo2point(sphere_coo)\n",
    "\n",
    "    pt_rad = pred[-1]\n",
    "    sphere_rad = sphere_coo[-1] # in angles\n",
    "    \n",
    "    \n",
    "    \n",
    "    if compare_spheres == False:\n",
    "        contained = (pt[0] - sphere_sense[0])**2 + (pt[1] - sphere_sense[1])**2 <= sphere_rad**2\n",
    "    else:\n",
    "        contained = pt_rad + torch.linalg.norm(pt - sphere_sense) - sphere_rad <= 0\n",
    "\n",
    "    if contained:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "def vicinity_matrix(spatial_params, target_vocab: np.ndarray, spatial_tags: np.ndarray, k=5):#, include_sphere=True, include_r=True) -> [str]:\n",
    "    \"\"\"\n",
    "    Projects the predicted spatial parameters into the embedding space.\n",
    "    Returns the synsets in the vicinity of the projected point.\n",
    "    :param spatial_params:\n",
    "    :return: Vicinity matrix, synsets dict\n",
    "    \"\"\"\n",
    "    N = len(spatial_tags)\n",
    "    \n",
    "    #convert spatial_tags to tensor\n",
    "    spatial_tags = torch.from_numpy(spatial_tags)\n",
    "    \n",
    "    synsets = {} # sort from most specific to most general\n",
    "    \n",
    "    indices = {}\n",
    "\n",
    "    sense_pt, center_pt = coo2point(spatial_params)\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------------------------\n",
    "    # Prepare distance and containment calculations\n",
    "    # ----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # distance calculations\n",
    "    dist_spheres = torch.empty(N) \n",
    "    dist_pt_sphere = torch.empty(N) \n",
    "    dist_pts = torch.empty(N)\n",
    "    \n",
    "    for i, tag in enumerate(spatial_tags):\n",
    "        dist_spheres[i] = distance_loss(spatial_params, tag, include_r=True)\n",
    "        dist_pt_sphere[i] = distance_loss(spatial_params, tag, pt_sphere=True)\n",
    "        dist_pts[i] = distance_loss(spatial_params, tag, include_r=False)\n",
    "    \n",
    "    # containment calculations\n",
    "    full_contained = torch.empty(N) \n",
    "    part_contained = torch.empty(N)\n",
    "    disconnected = torch.empty(N) # handles points only\n",
    "    \n",
    "    for j, tag in enumerate(spatial_tags):\n",
    "        full_contained[j] = is_contained(spatial_params, tag, compare_spheres=True)\n",
    "        part_contained[j] = distance_loss(spatial_params, tag, include_r=True) > 0\n",
    "        disconnected[j] = ~ is_contained(spatial_params, tag, compare_spheres=True) # reverse the True <----> False\n",
    "    \n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Initialize the Vicinity Matrix\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    # row=3, col=3, topk=2, 2 indicates the column of indices and the distances\n",
    "    vicinity_matrix = torch.zeros((3,3, k, 2))\n",
    "    \n",
    "    ####################################################################################################################\n",
    "    # # Full contained + min dist between sense points\n",
    "    ####################################################################################################################\n",
    "    \n",
    "#     print(\"True elements\")\n",
    "    true_indices1 = (full_contained == True).nonzero(as_tuple=True)[0]\n",
    "#     print(true_indices1)\n",
    "    \n",
    "    if true_indices1.size(0) != 0:\n",
    "        dist1 = torch.index_select(dist_pts, 0, true_indices1)\n",
    "#         print(\"dist1\", dist1)\n",
    "#         print(\"k = \", k)\n",
    "        # sort in ascending order\n",
    "        # select top k \n",
    "        sort_dist1, sort_indices = torch.topk(dist1, k, largest=False)  \n",
    "#         print(\"SORTING\", sort_dist1, sort_indices)\n",
    "        synsets1 = np.take(target_vocab, sort_indices, 0)\n",
    "        synsets[\"A\"] = synsets1\n",
    "        indices[\"A\"] = sort_indices\n",
    "        # index, distance (without synsets because this would result in conflicts for torch.tensor that do not support str)\n",
    "        vicinity_matrix[2][0] = torch.stack((sort_indices, sort_dist1), dim=1)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    ####################################################################################################################\n",
    "    # # Partially contained + min dist between sense points\n",
    "    ####################################################################################################################\n",
    "    true_indices2 = (part_contained == True).nonzero(as_tuple=True)[0]\n",
    "#     print(\"True Indices 2\", true_indices2)\n",
    "    \n",
    "    if true_indices2.size(0) != 0:\n",
    "        dist1 = torch.index_select(dist_pts, 0, true_indices2)\n",
    "        # sort in ascending order\n",
    "        # select top k \n",
    "        sort_dist2, sort_indices2 = torch.topk(dist1, k, largest=False)     \n",
    "        synsets2 = np.take(target_vocab, sort_indices2, 0)\n",
    "#         print(\"synset 2\", synsets2)\n",
    "        synsets[\"B\"] = synsets2\n",
    "        indices[\"B\"] = sort_indices2\n",
    "        # index, distance (without synsets because this would result in conflicts for torch.tensor that do not support str)\n",
    "        vicinity_matrix[2][1] = torch.stack((sort_indices2, sort_dist2), dim=1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # # Disconnected + min dist between spheres/point2sphere/sense points ---> acts as Nearest neighbor\n",
    "    ####################################################################################################################\n",
    "    # get indices, where disconnected is true\n",
    "    true_indices3 = (disconnected == True).nonzero(as_tuple=True)[0]\n",
    "#     print(\"True Indices 3\", true_indices3)\n",
    "\n",
    "    if true_indices3.size(0) != 0:\n",
    "        # get the distances at those indices\n",
    "        dist_spheres3 = torch.index_select(dist_spheres, 0, true_indices3)\n",
    "        dist_pt_sphere3 = torch.index_select(dist_pt_sphere, 0, true_indices3)\n",
    "        dist_pts3 = torch.index_select(dist_pts, 0, true_indices3)\n",
    "\n",
    "        # sort-select top k minimum distances\n",
    "        sort_dist_spheres3, sort_sph_indices3 = torch.topk(dist_spheres3, k, largest=False)\n",
    "        sort_dist_pt_sphere3, sort_pt_sph_indices3 = torch.topk(dist_pt_sphere3, k, largest=False)\n",
    "        sort_dist_pts3, sort_pts_indices3 = torch.topk(dist_pts3, k, largest=False)\n",
    "\n",
    "        # get their corresponding synsets\n",
    "        synsets30 = np.take(target_vocab, sort_sph_indices3, 0)\n",
    "        #print(\"synset30\", synsets30)\n",
    "        synsets[\"C\"] = synsets30\n",
    "        indices[\"C\"] = sort_sph_indices3\n",
    "        \n",
    "        synsets31 = np.take(target_vocab, sort_pt_sph_indices3, 0)\n",
    "        synsets[\"D\"] = synsets31\n",
    "        indices[\"D\"] = sort_pt_sph_indices3\n",
    "        \n",
    "        synsets32 = np.take(target_vocab, sort_pts_indices3, 0)\n",
    "        synsets[\"E\"] = synsets32\n",
    "        indices[\"E\"] = sort_pts_indices3\n",
    "        \n",
    "        # insert them into the vicinity matrix    \n",
    "        vicinity_matrix[0][3] = torch.stack((sort_sph_indices3, sort_dist_spheres3), dim=1)\n",
    "        vicinity_matrix[1][3] = torch.stack((sort_pt_sph_indices3, sort_dist_pt_sphere3), dim=1)\n",
    "        vicinity_matrix[2][3] = torch.stack((sort_pts_indices3, sort_dist_pts3), dim=1)  \n",
    "    \n",
    "\n",
    "\n",
    "#     # get the spheres, where the point/point+radius is contained/overlaping/near\n",
    "\n",
    "#     # 1. check if the predicted point is contained in some sense\n",
    "#     contained = torch.empty(N)\n",
    "    \n",
    "#     for i, tag in enumerate(spatial_tags):\n",
    "#         contained[i] = is_contained(spatial_params, tag, compare_spheres=include_sphere)\n",
    "    \n",
    "#     # 2. For those synsets, which is the nearest synset point\n",
    "#     #use distance() to calculate distance between centers\n",
    "#     distances = torch.empty(N)\n",
    "#     for i, tag in enumerate(spatial_tags):\n",
    "#         distances[i] = distance_loss(spatial_params, tag, include_r=include_r)\n",
    "    \n",
    "#     # sort dist--> indices\n",
    "#     # check if for those distances the containment is true\n",
    "#     # if true: choose the one having min_dist as sense\n",
    "#     # top k senses must be stored in a dict \n",
    "    \n",
    "#     # check if for those distances the containment is false, then, only the radius is falsly predicted (not priority now)\n",
    "#     # if false and min_dist: choose it as potential sense\n",
    "    \n",
    "    \n",
    "\n",
    "#     # 3. If None of the synsets apply to that word sense\n",
    "#     # use sphere_dist to find the nearest sphere (most general synset), and assign it to that synset\n",
    "#     # (this maybe good for rare senses)\n",
    "#     # acts as a second chance\n",
    "#     rare_contained = torch.empty(N)\n",
    "#     rare_distances = torch.empty(N)\n",
    "#     for i, tag in enumerate(spatial_tags):\n",
    "#         rare_contained[i] = is_contained(spatial_params, tag, compare_spheres=False) #only consider sense point\n",
    "#         rare_distances[i] = distance_loss(spatial_params, tag, include_r=False)\n",
    "\n",
    "\n",
    "    return indices, vicinity_matrix, synsets\n",
    "\n",
    "def decode_key(key, mtx):\n",
    "    if key == \"A\":\n",
    "        return mtx[2, 0]\n",
    "    if key == \"B\":\n",
    "        return mtx[2, 1]\n",
    "    if key == \"C\":\n",
    "        return mtx[0, 2]\n",
    "    if key == \"D\":\n",
    "        return mtx[1, 2]\n",
    "    if key == \"E\":\n",
    "        return mtx[2, 2]\n",
    "    \n",
    "\n",
    "def label_in_vicinity(vicinity_matrix, vicinity_synsets, target_vocab, spatial_tags, true_label):\n",
    "    \n",
    "    checked_synsets = []\n",
    "    contained = []\n",
    "    checks = 0\n",
    "    predicted = []\n",
    "    distances = []\n",
    "    \n",
    "    in_vicinity = False\n",
    "    associated_syn = []\n",
    "    \n",
    "    # true label is either one of the possibilities [word, synset] or a randomly chosen one\n",
    "    \n",
    "    # induce subset of word-synset name \n",
    "    \n",
    "    #spatial_tags = torch.from_numpy(spatial_tags)\n",
    "    #idx_label = (spatial_tags == true_label).nonzero(as_tuple=True)[0]\n",
    "    # transform to numpy to \n",
    "    true_label = np.array(true_label, dtype=np.float64)\n",
    "    # keep spatial tag an np.ndarray\n",
    "    rounded_l = np.round(true_label, decimals=2)\n",
    "    \n",
    "    if np.all(rounded_l == np.zeros(5)): #true_label): #torch.all(torch.eq(rounded_l, true_label)):\n",
    "        in_vicinity = True\n",
    "        associated_syn.append('no-synset')\n",
    "        return in_vicinity, associated_syn\n",
    "    \n",
    "    try:\n",
    "        # detecting the true label from the spatial_tags\n",
    "        idx = [[np.array_equal(rounded_l, tag) for tag in spatial_tags].index(True)]\n",
    "#         print(\"Found {} matching word-synset tags.\".format(len(idx)))\n",
    "        word_synset = target_vocab[idx] #list of list \n",
    "#         print(\"Matching word-synset\", word_synset)\n",
    "        # check if word_synset is within the vicinity matrix\n",
    "        if len(word_synset) != 0:\n",
    "            for e in word_synset:\n",
    "                for key, val in vicinity_synsets.items():\n",
    "#                     print(\"Searching in vicinity ... \")\n",
    "\n",
    "#                     print(\"Checking if true label is in vicinity ...\")\n",
    "                    checked_synsets.append(e)\n",
    "                    is_there = e[1] in val[:, 1]\n",
    "                    checks += 1\n",
    "                    contained.append(is_there)\n",
    "                    \n",
    "#                     print(\"1\")\n",
    "#                     print(checked_synsets)\n",
    "#                     print(checks)\n",
    "#                     print(contained)\n",
    "                    \n",
    "                    if is_there:\n",
    "#                         print(\"The main true label <{}> is in the vacinity of the predicted tag.\".format(e))\n",
    "                        idx_e = np.where(val[:, 1] == e[1])\n",
    "                        predicted.append(val[idx_e])\n",
    "#                         print(\"Predicted 1: \", predicted)\n",
    "                        distances.append(decode_key(key, vicinity_matrix)[idx_e][1])\n",
    "#                         print(\"Distances 1: \", distances)\n",
    "                    else:\n",
    "#                         print(\"The main true label is not in vicinity ... \")\n",
    "                        distances.append('no-distance')\n",
    "#                         print(\"Searching if alternative true label synsets are in vicinity ... \")\n",
    "                    # induce all the word-synset tuples that have same synset as true label.\n",
    "                    # This double check is necessary since I choose the spatial tags in the training data randomly sometimes.\n",
    "                    # get indices of all word-synsets sharing same synset (not same word)\n",
    "                    ix = np.where(target_vocab == [_, e[1]])[0] # add [0] to indicate only the row index, not the column\n",
    "#                     print(\"Indices \", ix)\n",
    "                    if len(ix) != 0:\n",
    "                        pos_syn = target_vocab[ix]\n",
    "                        \n",
    "#                         print(\"Possible synsets: \", pos_syn)\n",
    "#                         print(target_vocab[:10])\n",
    "                        for t in pos_syn:\n",
    "                            checks += 1\n",
    "                            checked_synsets.append(t)\n",
    "                            is_near = t[1] in val[:, -1]\n",
    "                            contained.append(is_near)\n",
    "#                             print(\"2\")\n",
    "#                             print(checked_synsets)\n",
    "#                             print(checks)\n",
    "#                             print(contained)\n",
    "                            if is_near == True:                                    \n",
    "#                                 print(\"... The word-synset <{}> is in the vicinity of the predicted tag.\".format(t))\n",
    "                                idx_t = np.where(val[:, -1] == t[1])\n",
    "                                predicted.append(val[idx_t])\n",
    "#                                 print(\"Predicted 2: \", predicted)\n",
    "                                distances.append(decode_key(key, vicinity_matrix)[idx_t][1])\n",
    "#                                 print(\"Distances 2: \", distances)\n",
    "                            else:\n",
    "                                distances.append('no-distance')\n",
    "                    else: \n",
    "                        print(\"... There are no other possibilites for word-synset <{}>\".format(e))\n",
    "                            \n",
    "        else:\n",
    "            print(\"Cannot find the suitable synset of this spatial tag!\")\n",
    "\n",
    "        \n",
    "    except ValueError as ve:\n",
    "        print(ve)\n",
    "#         print(\"Found no index for the true label. Something went wrong ...\")\n",
    "#         print(\"Comparing <true label = {}> with <rounded label = {}>\".format(true_label, rounded_l))\n",
    "    \n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Statistics\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "#     print(\"~\" * 80)\n",
    "#     print(\"Statistics\")\n",
    "#     print(\"~\" * 80)\n",
    "    \n",
    "#     print(\"Predicted Spatial Tag = \", spatial_params)\n",
    "#     print(\"Checked Spatial Tag(s) ; contained? ; Predicted ; distances = ({}):\".format(len(checked_synsets)))\n",
    "    for s, c, p, d in zip(checked_synsets, contained, predicted, distances):\n",
    "        print(s, \";\", c, \";\", \"\\n\", p, \";\", d)\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "#     print(\"True Spatial Tag(s) is in vicinity of predicted tag: \", contained)\n",
    "    contained_idx = np.where(np.array(contained) == True)\n",
    "    \n",
    "#     print(\"contained_idx\", contained_idx)\n",
    "#     print(\"checked_idx\", np.array(checked_synsets)[contained_idx])\n",
    "#     print(\"slice\", np.array(checked_synsets)[:, 1])\n",
    "#     print(\"check_slice\", np.array(checked_synsets)[:, 1][contained_idx])\n",
    "\n",
    "    if len(contained_idx[0]) > 0:\n",
    "#         print()\n",
    "#         print(contained_idx)\n",
    "        only_syn = set(np.array(checked_synsets)[contained_idx])#[:, 1])\n",
    "        associated_syn.append(only_syn)\n",
    "#         print(\"True Sense Tag(s) = ({}) --> \".format(len(only_syn)), only_syn)\n",
    "#         print(\"Prediction is correct!\")\n",
    "        in_vicinity = True\n",
    "#         print(\"Distance(predicted_sense, nearest_true_sense) = ({}): \".format(len(np.array(predicted)[contained_idx])))\n",
    "#         for p, d in zip(np.array(predicted), distances):\n",
    "#               print(p, d)\n",
    "              \n",
    "    else:\n",
    "#         print(\"Prediction is false ..\")\n",
    "#         print(\"All synsets in the vicinity of the predicted tag are not true senses ..\")\n",
    "#         print(\"Please check manually if the synsets in the vicinity are generalizations of the true labels.\")\n",
    "        in_vicinity = False\n",
    "        associated_syn.append(\"no-synset\")\n",
    "    \n",
    "    \n",
    "    return in_vicinity, associated_syn\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=np.array([1,4,2,8,3,2])\n",
    "contained_idx = np.where(s == 9)\n",
    "print(type(contained_idx), contained_idx[0])\n",
    "print(len(contained_idx[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    'Counts the parameters of the model to allow comparision between different models.'\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def spatial_stats(pred, original, zeros_tags=True):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 6. Training/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I need to split all training data beforehand\n",
    "\n",
    "class RegTagger:\n",
    "    \n",
    "    def __init__(self, use_cuda, device):\n",
    "        self.use_cuda = use_cuda\n",
    "        self.device = device\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "        \n",
    "    def train(self, batch_size: int, num_workers: int, max_epochs: int, \n",
    "              splittings: dict, labels: list, train_val_syn: list, data: list, embed_size: int,\n",
    "              target_vocab: list, spatial_tags: list,\n",
    "              k=5,\n",
    "              d_model=300, d_hid=200, nlayers=2, nhead=2, dropout=0.2,\n",
    "              lr=5.0, gamma=0.95,\n",
    "              shuffle=True):\n",
    "        \n",
    "        # create batches\n",
    "        \n",
    "        # parameters\n",
    "        params = {'batch_size': batch_size, #64,\n",
    "                  'shuffle': shuffle,\n",
    "                  'collate_fn': lambda x: x,\n",
    "                  'num_workers': num_workers} #6} #set 0 if training on Windows machine\n",
    "\n",
    "        # Training and validation data generators\n",
    "        training_set = Dataset(splittings['train'], labels, train_val_syn)\n",
    "        training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "\n",
    "        validation_set = Dataset(splittings['validate'], labels, train_val_syn)\n",
    "        validation_generator = torch.utils.data.DataLoader(validation_set, **params)\n",
    "\n",
    "        # -------------------------------------------------\n",
    "    \n",
    "        # history to store the losses\n",
    "        history = defaultdict(list)\n",
    "\n",
    "        VOCAB, weights_matrix = load_vocab(data, embed_size=embed_size)\n",
    "\n",
    "        # target_VOCAB\n",
    "        # SPATIAL_TAGS\n",
    "\n",
    "    \n",
    "\n",
    "        #######################################################################################################################\n",
    "        #        Count sentences and number of words in training and validation datasets to normalize the loss\n",
    "        #######################################################################################################################\n",
    "        nb_words_training = 0\n",
    "        nb_train_sentences = 0\n",
    "        nb_words_validation = 0\n",
    "\n",
    "        for batch in training_generator:\n",
    "            for sentence, label, syn in batch:\n",
    "                nb_train_sentences += 1\n",
    "                nb_words_training += len(sentence)\n",
    "\n",
    "        for batch in validation_generator:\n",
    "            for sentence, label, syn in batch:\n",
    "                nb_words_validation += len(sentence)\n",
    "\n",
    "#         print(\"Count results:\")\n",
    "#         print(\"nb_words_training = {}\".format(nb_words_training))\n",
    "#         print(\"nb_train_sentences = {}\".format(nb_train_sentences))\n",
    "#         print(\"nb_words_validation = {}\".format(nb_words_validation))\n",
    "\n",
    "#         print(params[\"batch_size\"])\n",
    "        n_batches = np.ceil(nb_train_sentences / batch_size)\n",
    "#         print(\"ceiling\", n_batches)\n",
    "\n",
    "        mean_words = nb_words_training / n_batches\n",
    "#         print(\"mean_words\", mean_words)\n",
    "\n",
    "\n",
    "        # Loop over epochs\n",
    "        for epoch in range(max_epochs):\n",
    "\n",
    "\n",
    "            self.model = TransformerEncoderRegressor(weights_matrix = weights_matrix, \n",
    "                                            ntoken= len(VOCAB), #300,\n",
    "                                            out_features=5,\n",
    "                                            d_model=d_model,\n",
    "                                            d_hid=d_hid,\n",
    "                                            nlayers=nlayers,\n",
    "                                            nhead=nhead,\n",
    "                                            dropout=dropout)\n",
    "            self.model.to(self.device)\n",
    "\n",
    "            # ---------------------------------------------------------------------\n",
    "            #                       Optimizer\n",
    "            # ---------------------------------------------------------------------\n",
    "            # criterion = nn.CrossEntropyLoss()\n",
    "            criterion = nn.MSELoss()\n",
    "#             lr = 5.0  # learning rate\n",
    "            optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=gamma)\n",
    "            # -------\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            loss_sum = 0\n",
    "\n",
    "            self.model.train()\n",
    "\n",
    "            # for transformer\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "            print(\"Training ...\")\n",
    "            # Training\n",
    "            for batch in training_generator:\n",
    "#                 print(\"New Batch for Training\")\n",
    "#                 print(\"#\" * 100)\n",
    "\n",
    "                for local_batch, local_labels, local_synsets in batch:\n",
    "\n",
    "                    # Transform list(<string>) to Tensor(<Tensor>)\n",
    "#                     print(\"Input Sentence:\")\n",
    "#                     print(local_batch)\n",
    "                    input_words = local_batch\n",
    "                    local_batch = numericalize(local_batch, VOCAB)\n",
    "#                     print(type(local_batch), local_batch)\n",
    "\n",
    "\n",
    "                    # Transform List(<Tensor>) to Tensor(<Tensor>)\n",
    "                    # I have labels of same length --> this should be no problem for Tensor\n",
    "                    local_labels = torch.stack(local_labels)\n",
    "#                     print(\"Labels:\")\n",
    "#                     print(local_synsets)\n",
    "#                     print(type(local_labels), len(local_labels), type(local_labels[0]))\n",
    "#                     print(local_labels)\n",
    "\n",
    "                    # Transfer to GPU\n",
    "                    local_batch, local_labels = local_batch.to(self.device), local_labels.to(self.device)\n",
    "\n",
    "                    # Model computations\n",
    "                    # out outputs the indices of wordnet database\n",
    "                    out = self.model(local_batch)\n",
    "#                     print(\"Model's Output\")\n",
    "#                     print(type(out), out.shape)\n",
    "                    # print(out)\n",
    "                    # predicted synsets\n",
    "#                     print(\"Current Predictions based on vacinity of prediction\")\n",
    "#                     print(\"*\" * 100)\n",
    "#                     print(\"*\" * 100)\n",
    "\n",
    "\n",
    "                    # ntokens = len(VOCAB)#300\n",
    "                    loss = geometric_loss(out, local_labels) / mean_words\n",
    "                    # criterion(out.view(-1), local_labels.view(-1))\n",
    "#                     print(\"Loss\")\n",
    "#                     print(type(loss), loss.size())\n",
    "#                     print(loss)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    # I added this\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "                    # ---\n",
    "                    optimizer.step()\n",
    "                    loss_sum += loss.item()\n",
    "#                     print(\"Loss Sum\", loss_sum)\n",
    "\n",
    "\n",
    "                    train_loss = loss_sum / len(local_batch)\n",
    "                    history['train_loss'].append(train_loss)\n",
    "#                     print(history)\n",
    "#                     print(len(history['train_loss']))\n",
    "\n",
    "\n",
    "            # Evaluate on the validation set.\n",
    "            # evaluate every 1 step:\n",
    "\n",
    "            print(\"Validation ...\")\n",
    "            vloss_sum = 0\n",
    "            if epoch % 1 == 0:\n",
    "\n",
    "                correct_sense = 0\n",
    "                sense_accuracy = 0\n",
    "\n",
    "                # set model to eval mode to ignore updating the weights of the model\n",
    "                self.model.eval()\n",
    "\n",
    "                # do not calculate gradients while evaluating\n",
    "                with torch.set_grad_enabled(False):\n",
    "\n",
    "                    for batch in validation_generator:\n",
    "                        print(\"New Batch for Validation\")\n",
    "                        print(\"#\" * 100)\n",
    "\n",
    "                        for local_batch, local_labels, local_synsets in batch:\n",
    "\n",
    "                            # Transform list(<string>) to Tensor(<Tensor>)\n",
    "                            print(\"Input Sentence\")\n",
    "                            print(local_batch)\n",
    "                            input_words = local_batch\n",
    "                            local_batch = numericalize(local_batch, VOCAB)\n",
    "        #                     print(type(local_batch), local_batch)\n",
    "\n",
    "\n",
    "                            # Transform List(<Tensor>) to Tensor(<Tensor>)\n",
    "                            # I have labels of same length --> this should be no problem for Tensor\n",
    "                            local_labels = torch.stack(local_labels)\n",
    "                            print(\"Labels:\")\n",
    "                            print(local_synsets)\n",
    "        #                     print(\"Labels\")\n",
    "        #                     print(type(local_labels), len(local_labels), type(local_labels[0]))\n",
    "        #                     print(local_labels)\n",
    "\n",
    "                            # Transfer to GPU\n",
    "                            local_batch, local_labels = local_batch.to(self.device), local_labels.to(self.device)\n",
    "\n",
    "                            # Model computations\n",
    "                            # out outputs the indices of wordnet database\n",
    "                            out = self.model(local_batch)\n",
    "\n",
    "                            # During validation and testing, I want to be less strict.\n",
    "                            # So, if a point resides within the label sphere, the sense is correctly identified.\n",
    "                            loss = geometric_loss(out, local_labels, include_r=True)\n",
    "\n",
    "                            vloss_sum += loss.item()                  \n",
    "\n",
    "                            validation_loss = vloss_sum / len(local_batch)\n",
    "                            history['validation_loss'].append(validation_loss)\n",
    "#                             print(history)\n",
    "#                             print(len(history['validation_loss']))\n",
    "\n",
    "                            correct_sense_batch = 0\n",
    "#                             print(\"Initializing the corrext sense batch = {}\".format(correct_sense_batch))\n",
    "\n",
    "                            true_pred = []\n",
    "                            predicted_synsets = []\n",
    "\n",
    "                            for i, word_tag in enumerate(out):\n",
    "#                                 print(\"i = \", i)\n",
    "#                                 print(\"+\"*150)\n",
    "#                                 print(\"word_tag = \", word_tag.size())\n",
    "#                                 print(word_tag)\n",
    "#                                 print(\"+\"*150)\n",
    "\n",
    "                                vindices, vmat, vsyn = vicinity_matrix(spatial_params=word_tag,\n",
    "                                                               target_vocab=target_vocab,\n",
    "                                                               spatial_tags=spatial_tags, k=k)\n",
    "#                                 print(\"Vicinity Matrix-Synsets: {}\".format(vsyn))\n",
    "\n",
    "\n",
    "                                in_vic, pred_syn = label_in_vicinity(vicinity_matrix=vmat, vicinity_synsets=vsyn,\n",
    "                                                           target_vocab=target_vocab, \n",
    "                                                           spatial_tags=spatial_tags, true_label=local_labels[i])\n",
    "                                \n",
    "                                true_pred.append(in_vic)\n",
    "                                predicted_synsets.append(pred_syn)\n",
    "                                \n",
    "#                                 print(\"In Vicinity? --> {}\".format(in_vic))\n",
    "#                                 print(\"Predicted synsets --> {}\".format(pred_syn))\n",
    "\n",
    "                                if in_vic==True:\n",
    "                                    correct_sense += 1\n",
    "                                    correct_sense_batch += 1\n",
    "\n",
    "                            print(true_pred)\n",
    "                            print(predicted_synsets)\n",
    "                        \n",
    "                            batch_acc = correct_sense_batch / len(local_batch)\n",
    "                            history[\"sense_accuracy\"].append(batch_acc)\n",
    "#                             print(\"correct sense batch ({}) / local_batch ({}) = {}\".format(correct_sense_batch, len(local_batch), batch_acc))\n",
    "\n",
    "\n",
    "                        t1 = time.time()\n",
    "                        print(f'Epoch {epoch}: train loss = {train_loss:.4f}, batch accuracy: {batch_acc:.4f}, time = {t1-t0:.4f}')\n",
    "\n",
    "\n",
    "                sense_accuracy = correct_sense / nb_words_validation\n",
    "\n",
    "                print(\"The sense accuracy on the validation set is {} %\".format(sense_accuracy))\n",
    "                \n",
    "        # **************************************************************************************************************\n",
    "        # Plot Histogram \n",
    "        # **************************************************************************************************************\n",
    "        data1 = history[\"train_loss\"] \n",
    "        data2 = history[\"sense_accuracy\"]\n",
    "\n",
    "        fig, ax1 = plt.subplots()\n",
    "\n",
    "        color = 'tab:red'\n",
    "        ax1.set_xlabel('time (s)')\n",
    "        ax1.set_ylabel('loss', color=color)\n",
    "        ax1.plot(data1, color=color)\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "        color = 'tab:blue'\n",
    "        ax2.set_ylabel('accuracy', color=color)  # we already handled the x-label with ax1\n",
    "        ax2.plot(data2, color=color)\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        plt.show()\n",
    "\n",
    "        return history\n",
    "    \n",
    "    \n",
    "    # assuming the sentence is already splitted into tokens, e.g. ['fall', 'in', 'catastrophes']\n",
    "    def test(self, testing_data, labels, test_syn, data, batch_size, num_workers, target_vocab, spatial_tags, k=5, shuffle=True):\n",
    "        \n",
    "        # parameters\n",
    "        params = {'batch_size': batch_size, #64,\n",
    "                  'shuffle': shuffle,\n",
    "                  'collate_fn': lambda x: x,\n",
    "                  'num_workers': num_workers} #6} #set 0 if training on Windows machine\n",
    "\n",
    "        # Training and validation data generators\n",
    "        testing_set = Dataset(testing_data, labels, test_syn)\n",
    "        testing_generator = torch.utils.data.DataLoader(testing_set, **params)\n",
    "        \n",
    "        # ------\n",
    "        # Count words in sentence to calculate accuracy\n",
    "        # ------\n",
    "        nb_words_testing = 0\n",
    "\n",
    "        for batch in testing_generator:\n",
    "            for sentence, label, syn in batch:\n",
    "                nb_words_testing += len(sentence)\n",
    "                \n",
    "        # --------------------------\n",
    "        VOCAB, weights_matrix = load_vocab(data, embed_size=embed_size)\n",
    "\n",
    "\n",
    "        # ---------------------------  \n",
    "        # testing\n",
    "        # ---------------------------\n",
    "        correct_sense = 0\n",
    "        sense_accuracy = 0\n",
    "        \n",
    "        t0 = time.time()\n",
    "\n",
    "        # set model to eval mode to ignore updating the weights of the model\n",
    "        self.model.eval()\n",
    "\n",
    "        # do not calculate gradients while evaluating\n",
    "        with torch.set_grad_enabled(False):\n",
    "\n",
    "            for batch in testing_generator:\n",
    "                print(\"Batches for testing\")\n",
    "                print(\"#\" * 100)\n",
    "\n",
    "                for local_batch, local_labels, local_synsets in batch:\n",
    "\n",
    "                    # Transform list(<string>) to Tensor(<Tensor>)\n",
    "                    print(\"Input Sentence\")\n",
    "                    print(local_batch)\n",
    "                    input_words = local_batch\n",
    "                    local_batch = numericalize(local_batch, VOCAB)\n",
    "#                     print(type(local_batch), local_batch)\n",
    "\n",
    "\n",
    "                    # Transform List(<Tensor>) to Tensor(<Tensor>)\n",
    "                    # I have labels of same length --> this should be no problem for Tensor\n",
    "                    local_labels = torch.stack(local_labels)\n",
    "                    print(\"Labels:\")\n",
    "                    print(local_synsets)\n",
    "#                     print(\"Labels\")\n",
    "#                     print(type(local_labels), len(local_labels), type(local_labels[0]))\n",
    "#                     print(local_labels)\n",
    "\n",
    "                    # Transfer to GPU\n",
    "                    local_batch, local_labels = local_batch.to(self.device), local_labels.to(self.device)\n",
    "\n",
    "                    # Model computations\n",
    "                    # out outputs the indices of wordnet database\n",
    "                    out = self.model(local_batch)\n",
    "\n",
    "                    # During validation and testing, I want to be less strict.\n",
    "                    # So, if a point resides within the label sphere, the sense is correctly identified.\n",
    "                    loss = geometric_loss(out, local_labels, include_r=True)\n",
    "\n",
    "                    vloss_sum += loss.item()                  \n",
    "\n",
    "                    validation_loss = vloss_sum / len(local_batch)\n",
    "                    history['testing_loss'].append(validation_loss)\n",
    "#                             print(history)\n",
    "#                             print(len(history['validation_loss']))\n",
    "\n",
    "                    correct_sense_batch = 0\n",
    "#                             print(\"Initializing the corrext sense batch = {}\".format(correct_sense_batch))\n",
    "\n",
    "                    true_pred = []\n",
    "                    predicted_synsets = []\n",
    "\n",
    "                    for i, word_tag in enumerate(out):\n",
    "\n",
    "                        vindices, vmat, vsyn = vicinity_matrix(spatial_params=word_tag,\n",
    "                                                       target_vocab=target_vocab,\n",
    "                                                       spatial_tags=spatial_tags, k=k)\n",
    "#                                 print(\"Vicinity Matrix-Synsets: {}\".format(vsyn))\n",
    "\n",
    "        \n",
    "                        in_vic, pred_syn = label_in_vicinity(vicinity_matrix=vmat, vicinity_synsets=vsyn,\n",
    "                                                   target_vocab=target_vocab, \n",
    "                                                   spatial_tags=spatial_tags, true_label=local_labels[i])\n",
    "        \n",
    "                        print(\"In Vicinity? --> {}\".format(in_vic))\n",
    "                        print(\"Predicted synsets --> {}\".format(pred_syn))\n",
    "            \n",
    "                        true_pred.append(in_vic)\n",
    "                        predicted_synsets.append(pred_syn)\n",
    "                        \n",
    "\n",
    "                        if in_vic:\n",
    "                            correct_sense += 1\n",
    "                            correct_sense_batch += 1\n",
    "\n",
    "                    print(true_pred)\n",
    "                    print(predicted_synsets)\n",
    "                    \n",
    "                    batch_acc = correct_sense_batch / len(local_batch)\n",
    "                    history[\"sense_accuracy\"].append(batch_acc)\n",
    "#                             print(\"correct sense batch ({}) / local_batch ({}) = {}\".format(correct_sense_batch, len(local_batch), batch_acc))\n",
    "\n",
    "                    \n",
    "                t1 = time.time()\n",
    "                print(f'batch accuracy: {batch_acc:.4f}, time = {t1-t0:.4f}')\n",
    "\n",
    "\n",
    "        sense_accuracy = correct_sense / nb_words_validation\n",
    "\n",
    "        print(\"The sense accuracy on the testing set is {} %\".format(sense_accuracy))\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    def tag(self, sentence):\n",
    "        #         if is_instance(sentence, str):\n",
    "        #             some preprocessing\n",
    "        \n",
    "        N = len(sentence)\n",
    "        tags = ['?' * N]\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Validation ...\n",
      "New Batch for Validation\n",
      "####################################################################################################\n",
      "Input Sentence\n",
      "['urgently', 'need']\n",
      "Labels:\n",
      "['urgently.r.01', 'need.v.03']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_11632/2656667895.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      7\u001B[0m t.train(batch_size=3, num_workers=0, max_epochs=5,\n\u001B[0;32m      8\u001B[0m         \u001B[0msplittings\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msplittings\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_val_syn\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtrain_val_syn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0membed_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m300\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 9\u001B[1;33m         target_vocab=target_VOCAB, spatial_tags=SPATIAL_TAGS)\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_11632/2237975160.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(self, batch_size, num_workers, max_epochs, splittings, labels, train_val_syn, data, embed_size, target_vocab, spatial_tags, k, d_model, d_hid, nlayers, nhead, dropout, lr, gamma, shuffle)\u001B[0m\n\u001B[0;32m    241\u001B[0m                                 vindices, vmat, vsyn = vicinity_matrix(spatial_params=word_tag,\n\u001B[0;32m    242\u001B[0m                                                                \u001B[0mtarget_vocab\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtarget_vocab\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 243\u001B[1;33m                                                                spatial_tags=spatial_tags, k=k)\n\u001B[0m\u001B[0;32m    244\u001B[0m \u001B[1;31m#                                 print(\"Vicinity Matrix-Synsets: {}\".format(vsyn))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    245\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_11632/2636481502.py\u001B[0m in \u001B[0;36mvicinity_matrix\u001B[1;34m(spatial_params, target_vocab, spatial_tags, k)\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     50\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtag\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mspatial_tags\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 51\u001B[1;33m         \u001B[0mdist_spheres\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdistance_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mspatial_params\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtag\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minclude_r\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     52\u001B[0m         \u001B[0mdist_pt_sphere\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdistance_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mspatial_params\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtag\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpt_sphere\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     53\u001B[0m         \u001B[0mdist_pts\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdistance_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mspatial_params\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtag\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minclude_r\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_11632/545762660.py\u001B[0m in \u001B[0;36mdistance_loss\u001B[1;34m(pred_pt, original_pt, include_r, pt_sphere)\u001B[0m\n\u001B[0;32m     35\u001B[0m     \u001B[0mr2\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moriginal_pt\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 37\u001B[1;33m     \u001B[0mpred_sense\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpred_center\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcoo2point\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpred_pt\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     38\u001B[0m     \u001B[0morig_sense\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0morig_center\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcoo2point\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moriginal_pt\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_11632/545762660.py\u001B[0m in \u001B[0;36mcoo2point\u001B[1;34m(coo)\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[0ml_i\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcoo\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[0mbeta_i\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcoo\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m     \u001B[0mbeta_i_rad\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbeta_i\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mmath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpi\u001B[0m \u001B[1;33m/\u001B[0m \u001B[1;36m180\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m     \u001B[0mr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcoo\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "#torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "t = RegTagger(use_cuda=use_cuda, device=device)\n",
    "t.train(batch_size=3, num_workers=0, max_epochs=5,\n",
    "        splittings=splittings, labels=labels, train_val_syn=train_val_syn, data=data, embed_size=300,\n",
    "        target_vocab=target_VOCAB, spatial_tags=SPATIAL_TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t.test(testing_data=splittings[], test_syn=, batch_size=5, num_workers=0, target_vocab=target_VOCAB, spatial_tag=SPATIAL_TAGS, k=5, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['train_loss'])\n",
    "# plt.plot(history['sense_accuracy'])\n",
    "# plt.legend(['training loss', 'validation accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['train_loss'])\n",
    "# plt.plot(history['sense_accuracy'])\n",
    "# plt.legend(['training loss', 'validation accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['train_loss'])\n",
    "# plt.plot(history['sense_accuracy'])\n",
    "# plt.legend(['training loss', 'validation accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['validation_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['sense_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = history[\"train_loss\"] \n",
    "data2 = history[\"sense_accuracy\"]\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('time (s)')\n",
    "ax1.set_ylabel('loss', color=color)\n",
    "ax1.plot(data1, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(data2, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = np.array([7,7,3])\n",
    "a[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sense Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def is_contained(pred, sphere_coo, compare_spheres=False):\n",
    "\n",
    "    pt, word = coo2point(pred)\n",
    "    sphere_sense, sphere_center = coo2point(sphere_coo)\n",
    "\n",
    "    pt_rad = pred[-1]\n",
    "    sphere_rad = sphere_coo[-1] # in angles\n",
    "    \n",
    "    \n",
    "    \n",
    "    if compare_spheres == False:\n",
    "        contained = (pt[0] - sphere_sense[0])**2 + (pt[1] - sphere_sense[1])**2 <= sphere_rad**2\n",
    "    else:\n",
    "        contained = pt_rad + torch.linalg.norm(pt - sphere_sense) - sphere_rad <= 0\n",
    "\n",
    "    if contained:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "def vicinity_matrix(spatial_params, target_vocab: np.ndarray, spatial_tags: np.ndarray, k=5):#, include_sphere=True, include_r=True) -> [str]:\n",
    "    \"\"\"\n",
    "    Projects the predicted spatial parameters into the embedding space.\n",
    "    Returns the synsets in the vicinity of the projected point.\n",
    "    :param spatial_params:\n",
    "    :return: Vicinity matrix, synsets dict\n",
    "    \"\"\"\n",
    "    N = len(spatial_tags)\n",
    "    \n",
    "    #convert spatial_tags to tensor\n",
    "    spatial_tags = torch.from_numpy(spatial_tags)\n",
    "    \n",
    "    synsets = {} # sort from most specific to most general\n",
    "    \n",
    "    indices = {}\n",
    "\n",
    "    sense_pt, center_pt = coo2point(spatial_params)\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------------------------\n",
    "    # Prepare distance and containment calculations\n",
    "    # ----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # distance calculations\n",
    "    dist_spheres = torch.empty(N) \n",
    "    dist_pt_sphere = torch.empty(N) \n",
    "    dist_pts = torch.empty(N)\n",
    "    \n",
    "    for i, tag in enumerate(spatial_tags):\n",
    "        dist_spheres[i] = distance_loss(spatial_params, tag, include_r=True)\n",
    "        dist_pt_sphere[i] = distance_loss(spatial_params, tag, pt_sphere=True)\n",
    "        dist_pts[i] = distance_loss(spatial_params, tag, include_r=False)\n",
    "    \n",
    "    # containment calculations\n",
    "    full_contained = torch.empty(N) \n",
    "    part_contained = torch.empty(N)\n",
    "    disconnected = torch.empty(N) # handles points only\n",
    "    \n",
    "    for j, tag in enumerate(spatial_tags):\n",
    "        full_contained[j] = is_contained(spatial_params, tag, compare_spheres=True)\n",
    "        part_contained[j] = distance_loss(spatial_params, tag, include_r=True) > 0\n",
    "        disconnected[j] = ~ is_contained(spatial_params, tag, compare_spheres=True) # reverse the True <----> False\n",
    "    \n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Initialize the Vicinity Matrix\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    # row=3, col=3, topk=2, 2 indicates the column of indices and the distances\n",
    "    vicinity_matrix = torch.zeros((3,3, k, 2))\n",
    "    \n",
    "    ####################################################################################################################\n",
    "    # # Full contained + min dist between sense points\n",
    "    ####################################################################################################################\n",
    "    \n",
    "    print(\"True elements\")\n",
    "    true_indices1 = (full_contained == True).nonzero(as_tuple=True)[0]\n",
    "    print(true_indices1)\n",
    "    \n",
    "    if true_indices1.size(0) != 0:\n",
    "        dist1 = torch.index_select(dist_pts, 0, true_indices1)\n",
    "        print(\"dist1\", dist1)\n",
    "        print(\"k = \", k)\n",
    "        # sort in ascending order\n",
    "        # select top k \n",
    "        sort_dist1, sort_indices = torch.topk(dist1, k, largest=False)  \n",
    "        print(\"SORTING\", sort_dist1, sort_indices)\n",
    "        synsets1 = np.take(target_vocab, sort_indices, 0)\n",
    "        synsets[\"A\"] = synsets1\n",
    "        indices[\"A\"] = sort_indices\n",
    "        # index, distance (without synsets because this would result in conflicts for torch.tensor that do not support str)\n",
    "        vicinity_matrix[2][0] = torch.stack((sort_indices, sort_dist1), dim=1)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    ####################################################################################################################\n",
    "    # # Partially contained + min dist between sense points\n",
    "    ####################################################################################################################\n",
    "    true_indices2 = (part_contained == True).nonzero(as_tuple=True)[0]\n",
    "    print(\"True Indices 2\", true_indices2)\n",
    "    \n",
    "    if true_indices2.size(0) != 0:\n",
    "        dist1 = torch.index_select(dist_pts, 0, true_indices2)\n",
    "        # sort in ascending order\n",
    "        # select top k \n",
    "        sort_dist2, sort_indices2 = torch.topk(dist1, k, largest=False)     \n",
    "        synsets2 = np.take(target_vocab, sort_indices2, 0)\n",
    "        print(\"synset 2\", synsets2)\n",
    "        synsets[\"B\"] = synsets2\n",
    "        indices[\"B\"] = sort_indices2\n",
    "        # index, distance (without synsets because this would result in conflicts for torch.tensor that do not support str)\n",
    "        vicinity_matrix[2][1] = torch.stack((sort_indices2, sort_dist2), dim=1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # # Disconnected + min dist between spheres/point2sphere/sense points ---> acts as Nearest neighbor\n",
    "    ####################################################################################################################\n",
    "    # get indices, where disconnected is true\n",
    "    true_indices3 = (disconnected == True).nonzero(as_tuple=True)[0]\n",
    "    print(\"True Indices 3\", true_indices3)\n",
    "\n",
    "    if true_indices3.size(0) != 0:\n",
    "        # get the distances at those indices\n",
    "        dist_spheres3 = torch.index_select(dist_spheres, 0, true_indices3)\n",
    "        dist_pt_sphere3 = torch.index_select(dist_pt_sphere, 0, true_indices3)\n",
    "        dist_pts3 = torch.index_select(dist_pts, 0, true_indices3)\n",
    "\n",
    "        # sort-select top k minimum distances\n",
    "        sort_dist_spheres3, sort_sph_indices3 = torch.topk(dist_spheres3, k, largest=False)\n",
    "        sort_dist_pt_sphere3, sort_pt_sph_indices3 = torch.topk(dist_pt_sphere3, k, largest=False)\n",
    "        sort_dist_pts3, sort_pts_indices3 = torch.topk(dist_pts3, k, largest=False)\n",
    "\n",
    "        # get their corresponding synsets\n",
    "        synsets30 = np.take(target_vocab, sort_sph_indices3, 0)\n",
    "        #print(\"synset30\", synsets30)\n",
    "        synsets[\"C\"] = synsets30\n",
    "        indices[\"C\"] = sort_sph_indices3\n",
    "        \n",
    "        synsets31 = np.take(target_vocab, sort_pt_sph_indices3, 0)\n",
    "        synsets[\"D\"] = synsets31\n",
    "        indices[\"D\"] = sort_pt_sph_indices3\n",
    "        \n",
    "        synsets32 = np.take(target_vocab, sort_pts_indices3, 0)\n",
    "        synsets[\"E\"] = synsets32\n",
    "        indices[\"E\"] = sort_pts_indices3\n",
    "        \n",
    "        # insert them into the vicinity matrix    \n",
    "        vicinity_matrix[0][3] = torch.stack((sort_sph_indices3, sort_dist_spheres3), dim=1)\n",
    "        vicinity_matrix[1][3] = torch.stack((sort_pt_sph_indices3, sort_dist_pt_sphere3), dim=1)\n",
    "        vicinity_matrix[2][3] = torch.stack((sort_pts_indices3, sort_dist_pts3), dim=1)  \n",
    "    \n",
    "\n",
    "\n",
    "#     # get the spheres, where the point/point+radius is contained/overlaping/near\n",
    "\n",
    "#     # 1. check if the predicted point is contained in some sense\n",
    "#     contained = torch.empty(N)\n",
    "    \n",
    "#     for i, tag in enumerate(spatial_tags):\n",
    "#         contained[i] = is_contained(spatial_params, tag, compare_spheres=include_sphere)\n",
    "    \n",
    "#     # 2. For those synsets, which is the nearest synset point\n",
    "#     #use distance() to calculate distance between centers\n",
    "#     distances = torch.empty(N)\n",
    "#     for i, tag in enumerate(spatial_tags):\n",
    "#         distances[i] = distance_loss(spatial_params, tag, include_r=include_r)\n",
    "    \n",
    "#     # sort dist--> indices\n",
    "#     # check if for those distances the containment is true\n",
    "#     # if true: choose the one having min_dist as sense\n",
    "#     # top k senses must be stored in a dict \n",
    "    \n",
    "#     # check if for those distances the containment is false, then, only the radius is falsly predicted (not priority now)\n",
    "#     # if false and min_dist: choose it as potential sense\n",
    "    \n",
    "    \n",
    "\n",
    "#     # 3. If None of the synsets apply to that word sense\n",
    "#     # use sphere_dist to find the nearest sphere (most general synset), and assign it to that synset\n",
    "#     # (this maybe good for rare senses)\n",
    "#     # acts as a second chance\n",
    "#     rare_contained = torch.empty(N)\n",
    "#     rare_distances = torch.empty(N)\n",
    "#     for i, tag in enumerate(spatial_tags):\n",
    "#         rare_contained[i] = is_contained(spatial_params, tag, compare_spheres=False) #only consider sense point\n",
    "#         rare_distances[i] = distance_loss(spatial_params, tag, include_r=False)\n",
    "\n",
    "\n",
    "    return indices, vicinity_matrix, synsets\n",
    "\n",
    "def decode_key(key, mtx):\n",
    "    if key == \"A\":\n",
    "        return mtx[2, 0]\n",
    "    if key == \"B\":\n",
    "        return mtx[2, 1]\n",
    "    if key == \"C\":\n",
    "        return mtx[0, 2]\n",
    "    if key == \"D\":\n",
    "        return mtx[1, 2]\n",
    "    if key == \"E\":\n",
    "        return mtx[2, 2]\n",
    "    \n",
    "\n",
    "def label_in_vicinity(vicinity_matrix, vicinity_synsets, target_vocab, spatial_tags, true_label):\n",
    "    \n",
    "    checked_synsets = []\n",
    "    contained = []\n",
    "    checks = 0\n",
    "    predicted = []\n",
    "    distances = []\n",
    "    \n",
    "    in_vicinity = False\n",
    "    \n",
    "    # true label is either one of the possibilities [word, synset] or a randomly chosen one\n",
    "    \n",
    "    # induce subset of word-synset name \n",
    "    \n",
    "    #spatial_tags = torch.from_numpy(spatial_tags)\n",
    "    #idx_label = (spatial_tags == true_label).nonzero(as_tuple=True)[0]\n",
    "    # transform to numpy to \n",
    "    true_label = np.array(true_label, dtype=np.float64)\n",
    "    # keep spatial tag an np.ndarray\n",
    "    rounded_l = np.round(true_label, decimals=2)\n",
    "    try:\n",
    "        # detecting the true label from the spatial_tags\n",
    "        idx = [[np.array_equal(rounded_l, tag) for tag in spatial_tags].index(True)]\n",
    "        print(\"Found {} matching word-synset tags.\".format(len(idx)))\n",
    "        word_synset = target_vocab[idx] #list of list \n",
    "        print(\"Matching word-synset\", word_synset)\n",
    "        # check if word_synset is within the vicinity matrix\n",
    "        if len(word_synset) != 0:\n",
    "            for e in word_synset:\n",
    "                for key, val in vicinity_synsets.items():\n",
    "                    print(\"Searching in vicinity ... \")\n",
    "\n",
    "                    print(\"Checking if true label is in vicinity ...\")\n",
    "                    checked_synsets.append(e)\n",
    "                    is_there = e[1] in val[:, 1]\n",
    "                    checks += 1\n",
    "                    contained.append(is_there)\n",
    "                    \n",
    "#                     print(\"1\")\n",
    "#                     print(checked_synsets)\n",
    "#                     print(checks)\n",
    "#                     print(contained)\n",
    "                    \n",
    "                    if is_there:\n",
    "                        print(\"The main true label <{}> is in the vacinity of the predicted tag.\".format(e))\n",
    "                        idx_e = np.where(val[:, 1] == e[1])\n",
    "                        predicted.append(val[idx_e])\n",
    "#                         print(\"Predicted 1: \", predicted)\n",
    "                        distances.append(decode_key(key, vicinity_matrix)[idx_e][1])\n",
    "#                         print(\"Distances 1: \", distances)\n",
    "                    else:\n",
    "                        print(\"The main true label is not in vicinity ... \")\n",
    "                        distances.append(0.0)\n",
    "                        print(\"Searching if alternative true label synsets are in vicinity ... \")\n",
    "                    # induce all the word-synset tuples that have same synset as true label.\n",
    "                    # This double check is necessary since I choose the spatial tags in the training data randomly sometimes.\n",
    "                    # get indices of all word-synsets sharing same synset (not same word)\n",
    "                    ix = np.where(target_vocab == [_, e[1]])[0] # add [0] to indicate only the row index, not the column\n",
    "#                     print(\"Indices \", ix)\n",
    "                    if len(ix) != 0:\n",
    "                        pos_syn = target_vocab[ix]\n",
    "                        \n",
    "#                         print(\"Possible synsets: \", pos_syn)\n",
    "#                         print(target_vocab[:10])\n",
    "                        for t in pos_syn:\n",
    "                            checks += 1\n",
    "                            checked_synsets.append(t)\n",
    "                            is_near = t[1] in val[:, -1]\n",
    "                            contained.append(is_near)\n",
    "#                             print(\"2\")\n",
    "#                             print(checked_synsets)\n",
    "#                             print(checks)\n",
    "#                             print(contained)\n",
    "                            if is_near == True:                                    \n",
    "                                print(\"... The word-synset <{}> is in the vicinity of the predicted tag.\".format(t))\n",
    "                                idx_t = np.where(val[:, -1] == t[1])\n",
    "                                predicted.append(val[idx_t])\n",
    "#                                 print(\"Predicted 2: \", predicted)\n",
    "                                distances.append(decode_key(key, vicinity_matrix)[idx_t][1])\n",
    "#                                 print(\"Distances 2: \", distances)\n",
    "                            else:\n",
    "                                distances.append(0.0)\n",
    "                    else: \n",
    "                        print(\"... There are no other possibilites for word-synset <{}>\".format(e))\n",
    "                            \n",
    "        else:\n",
    "            print(\"Cannot find the suitable synset of this spatial tag!\")\n",
    "\n",
    "        \n",
    "    except ValueError as ve:\n",
    "        print(ve)\n",
    "        print(\"Found no index for the true label. Something went wrong ...\")\n",
    "        print(\"Comparing <true label = {}> with <rounded label = {}>\".format(true_label, rounded_l))\n",
    "    \n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Statistics\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    print(\"~\" * 80)\n",
    "    print(\"Statistics\")\n",
    "    print(\"~\" * 80)\n",
    "    \n",
    "#     print(\"Predicted Spatial Tag = \", spatial_params)\n",
    "    print(\"Checked Spatial Tag(s) ; contained? ; Predicted ; distances = ({}):\".format(len(checked_synsets)))\n",
    "    for s, c, p, d in zip(checked_synsets, contained, predicted, distances):\n",
    "        print(s, \";\", c, \";\", \"\\n\", p, \";\", d)\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "#     print(\"True Spatial Tag(s) is in vicinity of predicted tag: \", contained)\n",
    "    contained_idx = np.where(np.array(contained) == True)\n",
    "    \n",
    "#     print(\"contained_idx\", contained_idx)\n",
    "#     print(\"checked_idx\", np.array(checked_synsets)[contained_idx])\n",
    "#     print(\"slice\", np.array(checked_synsets)[:, 1])\n",
    "#     print(\"check_slice\", np.array(checked_synsets)[:, 1][contained_idx])\n",
    "\n",
    "    if len(contained_idx) != 0:\n",
    "        print()\n",
    "        only_syn = set(np.array(checked_synsets)[contained_idx][:, 1])\n",
    "        print(\"True Sense Tag(s) = ({}) ..\".format(len(only_syn)), only_syn)\n",
    "        print(\"Prediction is correct!\")\n",
    "        in_vicinity = True\n",
    "#         print(\"Distance(predicted_sense, nearest_true_sense) = ({}): \".format(len(np.array(predicted)[contained_idx])))\n",
    "#         for p, d in zip(np.array(predicted), distances):\n",
    "#               print(p, d)\n",
    "              \n",
    "    else:\n",
    "        print(\"Prediction is false ..\")\n",
    "        print(\"All synsets in the vicinity of the predicted tag are not true senses ..\")\n",
    "        print(\"Please check manually if the synsets in the vicinity are generalizations of the true labels.\")\n",
    "        in_vicinity = False\n",
    "    \n",
    "    \n",
    "    return checked_synsets, contained, checks, predicted, distances\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.where(target_VOCAB==[_,\"boat.n.01\"])[0]\n",
    "SPATIAL_TAGS[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_VOCAB[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_VOCAB[60])\n",
    "print(SPATIAL_TAGS[60])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([34035, 78.06, 174780, 0.0, 64.5]) # synset boat\n",
    "\n",
    "# I created this synset for thousand.n.01, at index 60, word is 'thou'\n",
    "sample_tag = torch.tensor([36670, 100.07, 180000.00, 0.0, 1.6])\n",
    "true_lab = torch.tensor(SPATIAL_TAGS[60])\n",
    "\n",
    "idx, mat, syn = vicinity_matrix(spatial_params=sample_tag, target_vocab=target_VOCAB[:100], spatial_tags=SPATIAL_TAGS[:100], k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e = np.array(['thou', 'thousand.n.01'])\n",
    "e = np.array([\"thou\", \"thousand.n.01\"])\n",
    "val = np.array([['k', 'thousand.n.01'], ['c', 'hundred.n.01'], ['i', 'one.n.01'], ['ks', 'thousand.n.01'], ['cs', 'hundred.n.01']])\n",
    "e in val \n",
    "idx_e = np.where(val == e)\n",
    "idx_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s =np.where(val[:, 1] == e[1])\n",
    "val[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = np.array([True, False, True])\n",
    "\n",
    "co[np.where(co==True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csyn, cont, ch, pred, dist = label_in_vicinity(vicinity_matrix=mat, vicinity_synsets=syn, \n",
    "                      target_vocab=target_VOCAB[:100], spatial_tags=SPATIAL_TAGS[:100], true_label=true_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "poss = []\n",
    "dis = []\n",
    "for p, d in zip(pred, dist):\n",
    "    print(\"There are {} true values in the vicinity of predicted tag.\".format(len(p)))\n",
    "    if len(p) > 0:\n",
    "        for i in range(len(p)):\n",
    "            is_in = p[i] in poss\n",
    "            if is_in:\n",
    "                continue\n",
    "            else:\n",
    "                poss.append(p[i])\n",
    "                dis.append(d[i])\n",
    "    print(poss)\n",
    "    print(dis)\n",
    "    \n",
    "#     if p in poss:\n",
    "#         continue\n",
    "#     else:\n",
    "#         poss.append(p)\n",
    "#     if d in dis:\n",
    "#         continue\n",
    "#     else:\n",
    "#         dis.append(d)\n",
    "        \n",
    "        \n",
    "for p,d in zip(poss, dis):\n",
    "    print(p, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spatial_params = torch.tensor([111740.0, 98.3, 980130.0, 0.0, 18.5], dtype=torch.float64)\n",
    "# df=spatial_wordnet\n",
    "# st = list(zip(df.l0, df.alpha, df.l_i, df.beta_i, df.radius))\n",
    "# df.loc[df['l0'] == 111740.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The problem here was that there is transforming from torch to numpy leads to failures in rounding\n",
    "s = np.array(data[0][2][2], dtype=np.float64)\n",
    "print(s)\n",
    "spatial_params = np.array([1.351440e+05, 2.501000e+01, 6.417603e+04, 9.000000e+01, 5.000000e-01])\n",
    "print(\"spatial_params\", spatial_params)\n",
    "# index = np.where(SPATIAL_TAGS==spatial_params)\n",
    "# len(index[0])\n",
    "# len(SPATIAL_TAGS)\n",
    "sr = np.round(s, decimals=2)\n",
    "pr = np.round(spatial_params, decimals=2)\n",
    "print(\"rounded spatial_params\", pr)\n",
    "idx = [np.array_equal(pr,x) for x in SPATIAL_TAGS].index(True)\n",
    "idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [13, 304]\n",
    "target_VOCAB[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(target_VOCAB == [_, 'dog.n.01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target_VOCAB[111115]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tp = data[0][2][2]\n",
    "print(\"torch tensor:\", tp)\n",
    "tpr = torch.round(tp)\n",
    "print(\"rounded:\", tpr)\n",
    "ttags = torch.from_numpy(SPATIAL_TAGS) \n",
    "print(ttags[50801])\n",
    "# torch.nonzero((ttags == tp).sum(dim=1) == ttags.size(1))\n",
    "torch.all(ttags == tpr)#, x=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#spatial_params = torch.tensor([143500, 6.8500, 7574.6, 0.0, 0.5])\n",
    "torch.nonzero((torch.from_numpy(SPATIAL_TAGS) == spatial_params).sum(dim=1) == torch.from_numpy(SPATIAL_TAGS).size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#spatial_params = torch.tensor([143500, 6.8500, 7574.6, 0.0, 0.5])\n",
    "torch.nonzero((torch.from_numpy(SPATIAL_TAGS) == spatial_params).sum(dim=1) == torch.from_numpy(SPATIAL_TAGS).size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.all(torch.from_numpy(SPATIAL_TAGS) == spatial_params, x=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.from_numpy(SPATIAL_TAGS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spatial_params = torch.tensor([111740.0, 98.3, 980130.0, 0.0, 18.5])\n",
    "c0 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[0]).nonzero(as_tuple=True)\n",
    "print(c0)\n",
    "c1 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[1]).nonzero(as_tuple=True)\n",
    "print(c1)\n",
    "c2 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[2]).nonzero(as_tuple=True)\n",
    "print(c2)\n",
    "c3 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[3]).nonzero(as_tuple=True)\n",
    "print(c3)\n",
    "c5 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[4]).nonzero(as_tuple=True)\n",
    "print(c5)\n",
    "# for i in c[1]:\n",
    "#     if i != 3:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.where(torch.from_numpy(SPATIAL_TAGS)== spatial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.where(torch.from_numpy(SPATIAL_TAGS)== spatial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = torch.tensor([34035, 78.06, 174780, 0.0, 64.5]) # synset boat\n",
    "\n",
    "idx, mat, syn = vicinity_matrix(spatial_params=params, target_vocab=target_VOCAB[:20], spatial_tags=SPATIAL_TAGS[:20], k=5)\n",
    "syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tp = data[0][2][2]\n",
    "print(\"torch tensor:\", tp)\n",
    "tpr = torch.round(tp)\n",
    "print(\"rounded:\", tpr)\n",
    "ttags = torch.from_numpy(SPATIAL_TAGS) \n",
    "print(ttags[50801])\n",
    "# torch.nonzero((ttags == tp).sum(dim=1) == ttags.size(1))\n",
    "torch.all(ttags == tpr)#, x=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#spatial_params = torch.tensor([143500, 6.8500, 7574.6, 0.0, 0.5])\n",
    "torch.nonzero((torch.from_numpy(SPATIAL_TAGS) == spatial_params).sum(dim=1) == torch.from_numpy(SPATIAL_TAGS).size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#spatial_params = torch.tensor([143500, 6.8500, 7574.6, 0.0, 0.5])\n",
    "torch.nonzero((torch.from_numpy(SPATIAL_TAGS) == spatial_params).sum(dim=1) == torch.from_numpy(SPATIAL_TAGS).size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.all(torch.from_numpy(SPATIAL_TAGS) == spatial_params, x=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.from_numpy(SPATIAL_TAGS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spatial_params = torch.tensor([111740.0, 98.3, 980130.0, 0.0, 18.5])\n",
    "c0 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[0]).nonzero(as_tuple=True)\n",
    "print(c0)\n",
    "c1 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[1]).nonzero(as_tuple=True)\n",
    "print(c1)\n",
    "c2 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[2]).nonzero(as_tuple=True)\n",
    "print(c2)\n",
    "c3 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[3]).nonzero(as_tuple=True)\n",
    "print(c3)\n",
    "c5 = (torch.from_numpy(SPATIAL_TAGS)==spatial_params[4]).nonzero(as_tuple=True)\n",
    "print(c5)\n",
    "# for i in c[1]:\n",
    "#     if i != 3:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.where(torch.from_numpy(SPATIAL_TAGS)== spatial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.where(torch.from_numpy(SPATIAL_TAGS)== spatial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = torch.tensor([34035, 78.06, 174780, 0.0, 64.5]) # synset boat\n",
    "\n",
    "idx, mat, syn = vicinity_matrix(spatial_params=params, target_vocab=target_VOCAB[:20], spatial_tags=SPATIAL_TAGS[:20], k=5)\n",
    "syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat[2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in syn.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.tensor([]).size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not torch.all(torch.eq(spatial_params, torch.zeros(spatial_params.size(0))), dim=0):\n",
    "    print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = torch.tensor([True, False, False, True, True])\n",
    "(t == True).nonzero(as_tuple=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts = torch.tensor([12, 89, -2, 0.2])\n",
    "torch.sort(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topk = torch.topk(ts, 3, largest=False)\n",
    "topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sy = np.array([\"a\", \"b\", \"c\", \"d\"])\n",
    "\n",
    "np.take(sy, topk[1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row = 3\n",
    "col = 3\n",
    "k = 3\n",
    "t = 2\n",
    "vicinity_matrix = torch.empty((row, col, k, t))\n",
    "print(vicinity_matrix)\n",
    "print(vicinity_matrix[2][0])\n",
    "# newt = torch.tensor(topk[1], topk[0])\n",
    "# print(newt)\n",
    "# vicinity_matrix[2, 0] = torch.stack((torch.topk[1], torch.topk[0]), dim=-1)\n",
    "print(vicinity_matrix[2][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Geometric Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write a custom loss function in pytorch for \n",
    "# I need a fct to transform 5 coordinates into 2\n",
    "# and another function to calculate the loss between 2 points, then the total loss is the loss of all losses in a sentence \n",
    "\n",
    "\n",
    "def coo2point(coo):\n",
    "    print(coo)\n",
    "    l0 = coo[0]\n",
    "    alpha = coo[1]\n",
    "    alpha_rad = alpha * math.pi / 180\n",
    "    l_i = coo[2]\n",
    "    beta_i = coo[3]\n",
    "    beta_i_rad = beta_i * math.pi / 180\n",
    "    r = coo[4]\n",
    "    \n",
    "    # np.cos() and np.sin() take angles in radian as params\n",
    "    center_pt = torch.tensor([l0 * math.cos(alpha_rad), l0 * math.sin(alpha_rad)], dtype=torch.float64, requires_grad=True)\n",
    "    sense_pt = center_pt + torch.tensor([l_i * math.cos(alpha_rad + beta_i_rad),\n",
    "                                     l_i * math.sin(alpha_rad + beta_i_rad)], dtype=torch.float64, requires_grad=True)\n",
    "    return sense_pt, center_pt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def distance_loss(pred_pt, original_pt, include_r=False, pt_sphere=False):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two sense points, including radii.\n",
    "    :param pred_pt:\n",
    "    :param original_pt:\n",
    "    :param include_r: if set to true, include radius in the distance. \n",
    "                      It gives more freedom/tolerance degrees to the loss function. \n",
    "                      Loss is satisfied once the predicted point is part of original point.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "        \n",
    "    # original_pt = torch.from_numpy(original_pt)\n",
    "    # print(\"original point\", type(original_pt), original_pt)\n",
    "    \n",
    "    r1 = pred_pt[-1]\n",
    "    r2 = original_pt[-1]\n",
    "\n",
    "    pred_sense, pred_center = coo2point(pred_pt)\n",
    "    orig_sense, orig_center = coo2point(original_pt)\n",
    "    \n",
    "    \n",
    "    loss = torch.linalg.norm(torch.sub(pred_sense, orig_sense)) - r2\n",
    "    \n",
    "    # very strong assumption for the words that are not sense-tagged\n",
    "    # If I want more tolerance, I could neglect those tokens from the beginning\n",
    "    if torch.all(torch.eq(original_pt, torch.zeros(original_pt.size(0))), dim=0):\n",
    "        return loss\n",
    "    \n",
    "    if pt_sphere:\n",
    "        dist = torch.linalg.norm(torch.sub(pred_sense, orig_sense)) + r2\n",
    "        return dist\n",
    "\n",
    "    \n",
    "    if include_r:\n",
    "        \n",
    "        tolerant_loss = r1 + loss - r2\n",
    "    \n",
    "        if tolerant_loss < 0:\n",
    "            tolerant_loss = 0.0\n",
    "        \n",
    "#         if r1 > r2: #case the predicted radius is bigger than actual one\n",
    "#             tolerant_loss = torch.abs(torch.sub(r1, r2))\n",
    "           \n",
    "        return tolerant_loss\n",
    "    \n",
    "    else:\n",
    "        return loss \n",
    "   \n",
    "\n",
    "\n",
    "def geometric_loss(pred_list, label_list, include_r=False):\n",
    "    \n",
    "    # assert that the two lists must be of equal size\n",
    "    pred_size = pred_list.size()[0]\n",
    "    lab_size = label_list.size()[0]\n",
    "    assert pred_size == lab_size\n",
    "    \n",
    "    sentence_loss = 0.0\n",
    "    \n",
    "    # sum over all the tokens in the sentence\n",
    "    for i in range(pred_size):\n",
    "        sentence_loss += distance_loss(pred_list[i], label_list[i], include_r)\n",
    "        \n",
    "    return sentence_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example = torch.tensor([2.8, 45, 1.4, 0, 3.5])\n",
    "example2 = torch.tensor([0.0,0.0,0.0,0.0,0.0])\n",
    "geometric_loss(example, example2, include_r=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example = torch.tensor([2.8, 45, 1.4, 0, 3.5])\n",
    "example2 = torch.tensor([0.0,0.0,0.0,0.0,0.0])\n",
    "geometric_loss(example, example2, include_r=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sy = np.array([\"a\", \"b\", \"c\", \"d\"])\n",
    "\n",
    "np.take(sy, topk[1], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write a custom loss function in pytorch for \n",
    "# I need a fct to transform 5 coordinates into 2\n",
    "# and another function to calculate the loss between 2 points, then the total loss is the loss of all losses in a sentence \n",
    "\n",
    "\n",
    "def coo2point(coo):\n",
    "    print(coo)\n",
    "    l0 = coo[0]\n",
    "    alpha = coo[1]\n",
    "    alpha_rad = alpha * math.pi / 180\n",
    "    l_i = coo[2]\n",
    "    beta_i = coo[3]\n",
    "    beta_i_rad = beta_i * math.pi / 180\n",
    "    r = coo[4]\n",
    "    \n",
    "    # np.cos() and np.sin() take angles in radian as params\n",
    "    center_pt = torch.tensor([l0 * math.cos(alpha_rad), l0 * math.sin(alpha_rad)], dtype=torch.float64, requires_grad=True)\n",
    "    sense_pt = center_pt + torch.tensor([l_i * math.cos(alpha_rad + beta_i_rad),\n",
    "                                     l_i * math.sin(alpha_rad + beta_i_rad)], dtype=torch.float64, requires_grad=True)\n",
    "    return sense_pt, center_pt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def distance_loss(pred_pt, original_pt, include_r=False, pt_sphere=False):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two sense points, including radii.\n",
    "    :param pred_pt:\n",
    "    :param original_pt:\n",
    "    :param include_r: if set to true, include radius in the distance. \n",
    "                      It gives more freedom/tolerance degrees to the loss function. \n",
    "                      Loss is satisfied once the predicted point is part of original point.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    r1 = pred_pt[-1]\n",
    "    r2 = original_pt[-1]\n",
    "\n",
    "    pred_sense, pred_center = coo2point(pred_pt)\n",
    "    orig_sense, orig_center = coo2point(original_pt)\n",
    "    \n",
    "    \n",
    "    loss = torch.linalg.norm(torch.sub(pred_sense, orig_sense)) - r2\n",
    "    \n",
    "    # very strong assumption for the words that are not sense-tagged\n",
    "    # If I want more tolerance, I could neglect those tokens from the beginning\n",
    "    if torch.equal(original_pt, torch.zeros(original_pt.size()[0])):\n",
    "        return loss\n",
    "    \n",
    "    if pt_sphere:\n",
    "        dist = torch.linalg.norm(torch.sub(pred_sense, orig_sense)) + r2\n",
    "        return dist\n",
    "\n",
    "    \n",
    "    if include_r:\n",
    "        \n",
    "        tolerant_loss = r1 + loss - r2\n",
    "    \n",
    "        if tolerant_loss < 0:\n",
    "            tolerant_loss = 0.0\n",
    "        \n",
    "#         if r1 > r2: #case the predicted radius is bigger than actual one\n",
    "#             tolerant_loss = torch.abs(torch.sub(r1, r2))\n",
    "           \n",
    "        return tolerant_loss\n",
    "    \n",
    "    else:\n",
    "        return loss \n",
    "   \n",
    "\n",
    "\n",
    "def geometric_loss(pred_list, label_list, include_r=False):\n",
    "    \n",
    "    # assert that the two lists must be of equal size\n",
    "    pred_size = pred_list.size()[0]\n",
    "    lab_size = label_list.size()[0]\n",
    "    assert pred_size == lab_size\n",
    "    \n",
    "    sentence_loss = 0.0\n",
    "    \n",
    "    # sum over all the tokens in the sentence\n",
    "    for i in range(pred_size):\n",
    "        sentence_loss += distance_loss(pred_list[i], label_list[i], include_r)\n",
    "        \n",
    "    return sentence_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example = torch.tensor([2.8, 45, 1.4, 0, 3.5])\n",
    "example2 = torch.tensor([0.0,0.0,0.0,0.0,0.0])\n",
    "geometric_loss(example, example2, include_r=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l = torch.tensor([[2.8, 45, 1.4, 0, 3.5], [2124, 90, 1000, 14, 0.5]])\n",
    "l.view(-1)\n",
    "l.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(torch.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# <span style=\"color:red\"> WSD as Classification Task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> WSD as Classification Task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderModel(nn.Module):\n",
    "\n",
    "    def __init__(self, weights_matrix:np.ndarray, target_matrix: np.ndarray,\n",
    "                 ntoken: int, out_features:int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.weights_matrix = weights_matrix\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(self.weights_matrix, True)\n",
    "        \n",
    "        # Multi-head attention mechanism is included in TransformerEncoderLayer\n",
    "        # d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=<function relu>, \n",
    "        # layer_norm_eps=1e-05, batch_first=False, norm_first=False, device=None, dtype=None\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout) # activation\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers, norm=None)\n",
    "        \n",
    "        \n",
    "#         padding_idx (int, optional) – If specified, the entries at padding_idx do not contribute to the gradient;\n",
    "#         therefore, the embedding vector at padding_idx is not updated during training,\n",
    "#         i.e. it remains as a fixed “pad”. For a newly constructed Embedding, the embedding vector at\n",
    "#         padding_idx will default to all zeros, but can be updated to another value to be used as the padding vector.\n",
    "        self.emb = nn.Embedding(ntoken, d_model) \n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Linear layer: returns the last hidden state of the encoder \n",
    "        self.fc = nn.Linear(d_model, embedding_dim)\n",
    "        \n",
    "        # No! Here I am just redoing fully connected connections\n",
    "        # Linear Layer: affine transformation of last hidden layer into shape (1, embedding_dim)\n",
    "        #self.context_vec = nn.Linear(d_model, embedding_dim)\n",
    "        \n",
    "        #self.decoder = nn.Linear(d_model, ntoken)\n",
    "        \n",
    "        # Now, I need to have a Linear space that takes the whole/subset dataframe as input, extracts its spatial_context_vec,\n",
    "        # based on Glove-word-vector + spatial_point,\n",
    "        # then calculates softmax on this distribution\n",
    "        # choose the argmax\n",
    "        # get its spatial tags\n",
    "        # calculate distance loss between them\n",
    "        # do backprop! \n",
    "        # Nx300 into Nx227733: matmul product of two matrices Nx300 and 300x227733 --> Nx227733\n",
    "        # apply softmax to get the probabilities\n",
    "        # apply argmax to get the maximum indices\n",
    "        # use the indices to get the synset names as well as the mapping to coordinates\n",
    "        # into Nx5: mapping to the coordinates\n",
    "        \n",
    "        self.target_matrix = target_matrix\n",
    "        \n",
    "        #self.wn_embeddings = nn.Linear(1, target_matrix.shape[0])\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "#         weights_matrix = weights_matrix, \n",
    "#                                     ntoken= # false: 300,\n",
    "#                                     out_features=5,\n",
    "#                                     d_model=300,\n",
    "#                                     d_hid=200,\n",
    "#                                     nlayers=2,\n",
    "#                                     nhead=2,\n",
    "#                                     dropout=0.2\n",
    "        \n",
    "        \n",
    "        # -------------------------------------\n",
    "\n",
    "        # voc_size = len(text_field.vocab)\n",
    "        # print(\"voc_size: \", voc_size )\n",
    "        #\n",
    "        # # Embedding layer. If we're using pre-trained embeddings, copy them\n",
    "        # # into our embedding module.\n",
    "        # self.embedding = nn.Embedding(voc_size, 300)\n",
    "        # print(\"Embedding\", self.embedding)\n",
    "        # if text_field.vocab.vectors is not None:\n",
    "        #     self.embedding.weight = torch.nn.Parameter(TEXT.vocab.vectors)\n",
    "\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.emb.weight.data.uniform_(-initrange, initrange)\n",
    "        # self.decoder.bias.data.zero_()\n",
    "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        #self.output.bias.data.zero_()\n",
    "        #self.output.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        \n",
    "        #src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = torch.mul(self.emb(src), math.sqrt(self.d_model)) #? 1/sqrt!\n",
    "        print(\"Embedding\", src.shape)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        \n",
    "        src = self.pos_encoder(src)\n",
    "        print(\"Positional Encoding\", src.shape)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        \n",
    "        encoder_output = self.transformer_encoder(src) #, src_mask)\n",
    "        print(\"Encoder\", encoder_output.shape)\n",
    "        print(encoder_output)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        \n",
    "        linear_layer = self.fc(encoder_output)\n",
    "        print(\"Linear Layer\", linear_layer.shape)\n",
    "        print(linear_layer)\n",
    "        print('-' * 80)\n",
    "\n",
    "        # calculate the sum/weighted sum/ ?? on the linear layer to get the context vector of size (1, embd_dim)\n",
    "        context_vec = torch.sum(linear_layer, dim=1)\n",
    "        print(\"Final Context Vector\", context_vec.shape)\n",
    "        print(context_vec)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        # calculate the matrix that transforms a context vector into all wordnet embeddings\n",
    "        # expected: Nx223377x300\n",
    "#         wn_layer = self.wn_embeddings(context_vec)\n",
    "#         print(\"WordNet Embedding\", wn_layer.shape)\n",
    "#         print(wn_layer[0])\n",
    "#         print('-' * 80)\n",
    "        \n",
    "        # there must be a calculation here with the \n",
    "        # calculate dot product\n",
    "        # dim -1 is dim = 2 in 1[1[1[]],2[],..., N[]] \n",
    "        #trans_context_vec = torch.transpose(context_vec, 0, 1)\n",
    "        sense_matrix = torch.from_numpy(self.target_matrix).float()\n",
    "        print(\"sense mat\", sense_matrix.shape)\n",
    "        print(sense_matrix[0])\n",
    "        trans_sense_matrix = torch.transpose(sense_matrix, 0, 1)\n",
    "#         dot_prod = torch.sum(torch.matmul(context_vec, trans_sense_matrix), dim=-1)\n",
    "        dot_prod = torch.matmul(context_vec, trans_sense_matrix)\n",
    "\n",
    "        print(\"Dot Product\", dot_prod.shape)\n",
    "        print(dot_prod)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        \n",
    "        # calculate softmax\n",
    "        # expected Nx 227733\n",
    "        softmax = F.softmax(dot_prod)\n",
    "        print(\"Softmax\", softmax.shape)\n",
    "        print(softmax)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        # argmax\n",
    "        max_match = torch.argmax(softmax, dim=1) # dim 1 to indicate row\n",
    "        # expected Nx1\n",
    "        print(\"argmax\", max_match.shape)\n",
    "        print(max_match)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        # calculate softmax on all \n",
    "        \n",
    "        #dec = self.decoder(output)\n",
    "        #print(\"Decoder\")\n",
    "        #print(dec.shape)\n",
    "        #print(dec)\n",
    "        return max_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = defaultdict(list)\n",
    "\n",
    "VOCAB, weights_matrix = load_vocab(data, embed_size=300)\n",
    "\n",
    "# target_VOCAB\n",
    "# SPATIAL_TAGS\n",
    "\n",
    "n_epochs = 3\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    \n",
    "    model = TransformerEncoderModel(weights_matrix = weights_matrix, \n",
    "                                    target_matrix = tsense_matrix,\n",
    "                                    ntoken= len(VOCAB), #300,\n",
    "                                    out_features=5,\n",
    "                                    d_model=300,\n",
    "                                    d_hid=200,\n",
    "                                    nlayers=2,\n",
    "                                    nhead=2,\n",
    "                                    dropout=0.2)\n",
    "    model.to(device)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    #                       Optimizer\n",
    "    # ---------------------------------------------------------------------\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lr = 5.0  # learning rate\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "    # -------\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    loss_sum = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # for transformer\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "    # Training\n",
    "    for batch in training_generator:\n",
    "        \n",
    "        for local_batch, local_labels in batch:\n",
    "            \n",
    "            # Transform list(<string>) to Tensor(<Tensor>)\n",
    "            print(\"Input Sentence\")\n",
    "            print(local_batch)\n",
    "            input_words = local_batch\n",
    "            local_batch = numericalize(local_batch, VOCAB)\n",
    "            print(type(local_batch), local_batch)\n",
    "            \n",
    "            \n",
    "            # Transform List(<Tensor>) to Tensor(<Tensor>)\n",
    "            # I have labels of same length --> this should be no problem for Tensor\n",
    "            local_labels = torch.stack(local_labels)\n",
    "            print(\"Labels\")\n",
    "            print(type(local_labels), len(local_labels), type(local_labels[0]))\n",
    "            print(local_labels)\n",
    "            \n",
    "            # Transfer to GPU\n",
    "            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "            # Model computations\n",
    "            # out outputs the indices of wordnet database\n",
    "            out = model(local_batch)\n",
    "            print(type(out), out.shape)\n",
    "            # predicted synsets\n",
    "            word_synset_list = list(map(target_VOCAB.__getitem__, out))\n",
    "            print(\"Current Predictions\")\n",
    "            print(\"*\" * 100)\n",
    "            for i in range(len(input_words)):\n",
    "                print(\"<{}> predicted as {}\".format(input_words[i], word_synset_list[i]))\n",
    "            #print_pred = '\\n'.join('{} predicted as {}' for _, _ in zip(range(len(input_words)), range(len(word_synset_list)))).format(*input_words, *word_synset_list)\n",
    "            #print(print_pred)\n",
    "            #print(\"<{}> predicted as {}\".format(zip(input_words, word_synset_list)))\n",
    "            print(\"*\" * 100)\n",
    "            \n",
    "            tags = list(map(SPATIAL_TAGS.__getitem__, out))\n",
    "            # tags is a list of arrays of 5 parameters\n",
    "            print(\"Spatial Tags\", len(tags))\n",
    "            \n",
    "            ntokens = len(VOCAB)#300\n",
    "            #loss = criterion(out.view(-1, ntokens), local_labels)\n",
    "            # ignore prediction for the words that have no entry in the system? [0,0,0,0,0]\n",
    "            loss = distance_loss()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # I added this\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            # ---\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "\n",
    "            train_loss = loss_sum / len(local_batch)\n",
    "            history['train_loss'].append(train_loss)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "#         # Validation\n",
    "#         with torch.set_grad_enabled(False):\n",
    "#             for local_batch, local_labels in validation_generator:\n",
    "#                 # Transfer to GPU\n",
    "#                 local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "#                 # Model computations\n",
    "#                 [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X, y = training_set.__getitem__(0)\n",
    "for batch in training_generator:\n",
    "    print(len(batch))\n",
    "    for local_batch, local_label in batch:\n",
    "        print(local_batch, local_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# normalize each label\n",
    "sent_labels = torch.tensor([[8.5479e+04, 6.0380e+01, 1.2985e+05, 9.5740e+01, 5.0000e-01],\n",
    "                            [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
    "                            [1.2943e+05, 1.7871e+02, 2.0605e+04, 0.0000e+00, 5.0000e-01],\n",
    "                            [1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]])\n",
    "norm_sent_labels = torch.nn.functional.normalize(sent_labels, p=2.0, dim=1)\n",
    "norm_sent_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_labels\n",
    "# sent_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.transpose(sent_labels, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Output tensor\n",
    "out = torch.tensor([\n",
    "        [[-1.0549, -0.1433, -0.1394, -0.6256,  0.4749],\n",
    "         [-0.0305,  0.0031,  0.0382, -0.0017,  0.5000],\n",
    "         [ 0.5141, -0.0154, -0.3072,  0.4833,  0.3565],\n",
    "         [ 0.9379,  0.5004,  0.1174,  0.0816, -0.2704]],\n",
    "\n",
    "        [[-0.9528,  0.0359, -0.3091, -0.1796,  0.3259],\n",
    "         [ 0.2951,  0.0628, -0.1382, -0.0146,  0.2808],\n",
    "         [ 0.6864,  0.7282, -0.6748,  0.7032,  0.3163],\n",
    "         [ 1.1643,  0.3034,  0.2858,  0.0823, -0.5213]],\n",
    "\n",
    "        [[-0.3433,  0.7988, -0.9953, -0.2597,  0.6321],\n",
    "         [ 0.0388, -0.5481, -0.2017, -0.2700,  0.4406],\n",
    "         [ 0.9477,  0.4574, -0.2327,  0.0848,  0.3336],\n",
    "         [ 0.2500,  0.2523,  0.3734,  0.7667, -0.3530]],\n",
    "\n",
    "        [[-0.7934,  0.4223, -0.1750, -0.5049,  0.6555],\n",
    "         [ 0.0534, -0.6308, -0.7725, -0.3165,  0.3568],\n",
    "         [ 0.3805,  0.2004, -0.3675,  0.2600,  0.2836],\n",
    "         [-0.0449, -0.0765, -0.2250,  0.9507,  0.1856]]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nx300 [[1, ..., 300], ... , N[]] * [[1, ..., 300], ..., 227733[]]\n",
    "# from 4x5 to 4x20x5\n",
    "N = 1\n",
    "n = 5\n",
    "tensor = torch.rand(20, n) \n",
    "print(tensor.shape)\n",
    "print(tensor)\n",
    "\n",
    "trans = torch.transpose(sent_labels, 0, 1)\n",
    "print(trans.shape)\n",
    "print(trans)\n",
    "torch.matmul(tensor, trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.transpose(sent_labels, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Output tensor\n",
    "out = torch.tensor([\n",
    "        [[-1.0549, -0.1433, -0.1394, -0.6256,  0.4749],\n",
    "         [-0.0305,  0.0031,  0.0382, -0.0017,  0.5000],\n",
    "         [ 0.5141, -0.0154, -0.3072,  0.4833,  0.3565],\n",
    "         [ 0.9379,  0.5004,  0.1174,  0.0816, -0.2704]],\n",
    "\n",
    "        [[-0.9528,  0.0359, -0.3091, -0.1796,  0.3259],\n",
    "         [ 0.2951,  0.0628, -0.1382, -0.0146,  0.2808],\n",
    "         [ 0.6864,  0.7282, -0.6748,  0.7032,  0.3163],\n",
    "         [ 1.1643,  0.3034,  0.2858,  0.0823, -0.5213]],\n",
    "\n",
    "        [[-0.3433,  0.7988, -0.9953, -0.2597,  0.6321],\n",
    "         [ 0.0388, -0.5481, -0.2017, -0.2700,  0.4406],\n",
    "         [ 0.9477,  0.4574, -0.2327,  0.0848,  0.3336],\n",
    "         [ 0.2500,  0.2523,  0.3734,  0.7667, -0.3530]],\n",
    "\n",
    "        [[-0.7934,  0.4223, -0.1750, -0.5049,  0.6555],\n",
    "         [ 0.0534, -0.6308, -0.7725, -0.3165,  0.3568],\n",
    "         [ 0.3805,  0.2004, -0.3675,  0.2600,  0.2836],\n",
    "         [-0.0449, -0.0765, -0.2250,  0.9507,  0.1856]]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try normal sum over columns \n",
    "out_sum = torch.sum(out, dim=1)\n",
    "out_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "norm_sent_labels - out_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = defaultdict(list)\n",
    "\n",
    "VOCAB, weights_matrix = load_vocab(data, embed_size=300)\n",
    "\n",
    "n_epochs = 3\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    \n",
    "    model = TransformerEncoderModel(weights_matrix = weights_matrix, \n",
    "                                    ntoken=300,\n",
    "                                    out_features=5,\n",
    "                                    d_model=300,\n",
    "                                    d_hid=200,\n",
    "                                    nlayers=2,\n",
    "                                    nhead=2,\n",
    "                                    dropout=0.2)\n",
    "    model.to(device)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    #                       Optimizer\n",
    "    # ---------------------------------------------------------------------\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lr = 5.0  # learning rate\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "    # -------\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    loss_sum = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # for transformer\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "    # Training\n",
    "    for batch in training_generator:\n",
    "        \n",
    "        for local_batch, local_labels in batch:\n",
    "            \n",
    "            # Transform list(<string>) to Tensor(<Tensor>)\n",
    "            print(\"Input Sentence\")\n",
    "            print(local_batch)\n",
    "            local_batch = numericalize(local_batch, VOCAB)\n",
    "            print(type(local_batch), local_batch)\n",
    "            \n",
    "            \n",
    "            # Transform List(<Tensor>) to Tensor(<Tensor>)\n",
    "            # I have labels of same length --> this should be no problem for Tensor\n",
    "            local_labels = torch.stack(local_labels)\n",
    "            print(\"Labels\")\n",
    "            print(type(local_labels), len(local_labels), type(local_labels[0]))\n",
    "            print(local_labels)\n",
    "            \n",
    "            # Transfer to GPU\n",
    "            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "            # Model computations\n",
    "            out = model(local_batch)\n",
    "            ntokens = 300\n",
    "            loss = criterion(out.view(-1, ntokens), local_labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # I added this\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            # ---\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "\n",
    "            train_loss = loss_sum / len(local_batch)\n",
    "            history['train_loss'].append(train_loss)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "#         # Validation\n",
    "#         with torch.set_grad_enabled(False):\n",
    "#             for local_batch, local_labels in validation_generator:\n",
    "#                 # Transfer to GPU\n",
    "#                 local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "#                 # Model computations\n",
    "#                 [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Output tensor\n",
    "out = torch.tensor([\n",
    "        [[-1.0549, -0.1433, -0.1394, -0.6256,  0.4749],\n",
    "         [-0.0305,  0.0031,  0.0382, -0.0017,  0.5000],\n",
    "         [ 0.5141, -0.0154, -0.3072,  0.4833,  0.3565],\n",
    "         [ 0.9379,  0.5004,  0.1174,  0.0816, -0.2704]],\n",
    "\n",
    "        [[-0.9528,  0.0359, -0.3091, -0.1796,  0.3259],\n",
    "         [ 0.2951,  0.0628, -0.1382, -0.0146,  0.2808],\n",
    "         [ 0.6864,  0.7282, -0.6748,  0.7032,  0.3163],\n",
    "         [ 1.1643,  0.3034,  0.2858,  0.0823, -0.5213]],\n",
    "\n",
    "        [[-0.3433,  0.7988, -0.9953, -0.2597,  0.6321],\n",
    "         [ 0.0388, -0.5481, -0.2017, -0.2700,  0.4406],\n",
    "         [ 0.9477,  0.4574, -0.2327,  0.0848,  0.3336],\n",
    "         [ 0.2500,  0.2523,  0.3734,  0.7667, -0.3530]],\n",
    "\n",
    "        [[-0.7934,  0.4223, -0.1750, -0.5049,  0.6555],\n",
    "         [ 0.0534, -0.6308, -0.7725, -0.3165,  0.3568],\n",
    "         [ 0.3805,  0.2004, -0.3675,  0.2600,  0.2836],\n",
    "         [-0.0449, -0.0765, -0.2250,  0.9507,  0.1856]]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try normal sum over columns \n",
    "out_sum = torch.sum(out, dim=1)\n",
    "out_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "norm_sent_labels - out_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}